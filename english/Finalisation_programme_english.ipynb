{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base de classification en Anglais \n",
    "\n",
    "    - Transformers : Summarization : 2 modèles --> 2 Résumés / Puis score de similarités de ces 2 résumés\n",
    "    Noter que l'on peut faire aussi la similarité des textes (autre note ?) et non du résumé\n",
    "    - Text classification sur une base de catégories \"Presse\" : sport - actaulités - économie - etc\n",
    "    - Sentiment analysis : voir si le ton du texte est de même type \n",
    "    - Les 2 derniers classifier seronts utilisés en produit scalaire : Par Catégorie : texte1: note1 - texte2 : note2\n",
    "    et donc sum(notes_par_catégorie) = sum(note1*note2) * 100 au bout (note sur 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stg-sdu\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\stg-sdu\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.QVLO2T66WEPI7JZ63PS3HMOHFEY472BC.gfortran-win_amd64.dll\n",
      "C:\\Users\\stg-sdu\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pke\n",
    "import spacy\n",
    "import torch\n",
    "import stanza\n",
    "import spacy_stanza\n",
    "import warnings\n",
    "import string\n",
    "from gensim.models import KeyedVectors\n",
    "import enchant    # Pour correction orthographique de synonymes\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "tqdm.pandas()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selection des modèles NLP : ici FRANCAIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement pour l'utilisation de Spacy  - Français\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_spacy = {'en':nlp_en}   # 'en':nlp_en,'de':nlp_de,'es':nlp_es,'pl':nlp_pl  - POUR MEMOIRE\n",
    "langues = ['en','fr','es','de','pl','ar','tr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du modèle Word2Vec pour utilisation de synonymes\n",
    "import gensim.downloader\n",
    "model_gensim = gensim.downloader.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords Français NLTK + Spacy \n",
    "stopWords = list(nlp_en.Defaults.stop_words)\n",
    "stopwords_en = list(stopwords.words('english'))  \n",
    "stopwords_en = list(set(stopwords_en + stopWords))\n",
    "stopwds_lg = {'en':stopwords_en}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correcteur orthographique pour validation des synonymes OPTIONNEL CAR NON NECESSAIRE\n",
    "d = enchant.Dict(\"en\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sélection des modèles Transformers : Summary - Text Classification - Sentiment Analysis - Similarity**<br/>\n",
    "    Pour le modèle anglais, on prend 3 summarizer !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a47fc275e6437ba397b2360810b23b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c020d8f8911480fb978c7d603e9bd84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.14G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a500de38434ab28cfaef14c671b743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2181bfd60662405da67ceebbe1177b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6fef651b2cd402da401ca7500adb49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff195a8c4db4d55990de80531c970e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf2a643113ba4b09a6ea7cabfc18adb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09cffeba457465c8f2c1e273d635d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/87.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a9bf2943c947d1aabeb92581662272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2d4144e3284381a4f46e9411624e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4f08b0866a4cdeba5ee265cb042ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Modèles Transformers de Résumé (NB : Ne pas oublier d'ajouter la truncation pour tous les modèles, peut être source d'erreur)\n",
    "# Visiblement le modèle google/bigbird-pegasus-large-arxiv est trop gros et le pegasus multi news : les 2 très longs ! et résumé long\n",
    "# summarizer1 = pipeline(\"summarization\", model=\"google/bigbird-pegasus-large-arxiv\", truncation = \"only_first\")  # base trop large et longue\n",
    "# summarizer1 = pipeline(\"summarization\", model=\"google/pegasus-multi_news\", truncation = \"only_first\")\n",
    "summarizer1 = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", truncation = \"only_first\")   # ou sshleifer/distilbart-xsum-12-3\n",
    "summarizer2 = pipeline(\"summarization\", model=\"google/pegasus-xsum\", truncation = \"only_first\")  # voir cnn 12-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textes classification ou Zero shot classification (permet de chosir nos propres thèmes)\n",
    "text_clf1 = pipeline(\"zero-shot-classification\", model = \"joeddav/bart-large-mnli-yahoo-answers\", truncation = \"only_first\")   # 10 actégories, voir hugging face\n",
    "text_clf2 = pipeline('zero-shot-classification', model='cross-encoder/nli-MiniLM2-L6-H768',truncation = \"only_first\")\n",
    "# ces modèles sont zero shot classification : catégories possibles choisies par mes soins (dans la presse)\n",
    "candidate_labels = ['Science', 'Politics', 'Education', 'News', 'Health', 'Technology', 'Society', 'Sport', 'Economy', 'Culture', 'International', 'Environment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sentiment Analysis : base de 1 à 5 stars\n",
    "sentiment1 = pipeline(\"text-classification\", model = 'nlptown/bert-base-multilingual-uncased-sentiment',truncation = \"only_first\")\n",
    "\n",
    "# Sur la base des sentiments classiques : joy, anger, suprise, sadness, love, fear\n",
    "sentiment2 = pipeline(\"text-classification\", model = 'bhadresh-savani/distilbert-base-uncased-emotion',truncation = \"only_first\")\n",
    "\n",
    "# Sur la base des sentiments classiques : NEGATIVE / POSITIVE\n",
    "sentiment3 = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english',truncation = \"only_first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODAGE AVEC SENTENCE TRANSFORMER 2 modfèles et moyenne\n",
    "from sentence_transformers import SentenceTransformer,util\n",
    "encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "encoder2 = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "def score_similarite(sentence1,sentence2):\n",
    "    # attention, pour que torch fonctionne en dimension sentence1 (et 2) est une liste simple\n",
    "    embed1 = encoder.encode(sentence1, convert_to_tensor=True)\n",
    "    embed2 = encoder.encode(sentence2, convert_to_tensor=True)\n",
    "    embed3 = encoder2.encode(sentence1, convert_to_tensor=True)\n",
    "    embed4 = encoder2.encode(sentence2, convert_to_tensor=True)\n",
    "    return round(float(util.pytorch_cos_sim(embed1,embed2))+float(util.pytorch_cos_sim(embed3,embed4))*100/2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selection Data par langues**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train_data_prep_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id</th>\n",
       "      <th>pair_lang</th>\n",
       "      <th>source_url_1</th>\n",
       "      <th>publish_date_1</th>\n",
       "      <th>source_url_2</th>\n",
       "      <th>publish_date_2</th>\n",
       "      <th>title_1</th>\n",
       "      <th>text_1</th>\n",
       "      <th>meta_description_1</th>\n",
       "      <th>meta_keywords_1</th>\n",
       "      <th>...</th>\n",
       "      <th>text_2</th>\n",
       "      <th>meta_description_2</th>\n",
       "      <th>meta_keywords_2</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Entities</th>\n",
       "      <th>Time</th>\n",
       "      <th>Narrative</th>\n",
       "      <th>Overall</th>\n",
       "      <th>Style</th>\n",
       "      <th>Tone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1484084337_1484110209</td>\n",
       "      <td>en_en</td>\n",
       "      <td>https://www.washingtonpost.com</td>\n",
       "      <td>Wed Jan  1 00:00:00 2020</td>\n",
       "      <td>https://www.washingtonpost.com</td>\n",
       "      <td>Wed Jan  1 00:00:00 2020</td>\n",
       "      <td>Virginia man arrested in fatal DUI crash in We...</td>\n",
       "      <td>MARTINSBURG, W.Va. — A suspected drunken drive...</td>\n",
       "      <td>Police in West Virginia say a suspected drunke...</td>\n",
       "      <td>['Highway Fatal-DUI-West Virginia', 'Martinsbu...</td>\n",
       "      <td>...</td>\n",
       "      <td>PORT-AU-PRINCE, Haiti — Haitian President Jove...</td>\n",
       "      <td>Haitian President Jovenel Moïse has broken wit...</td>\n",
       "      <td>['CB-Haiti-Political Turmoil', 'Jean', 'Haiti'...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1484396422_1483924666</td>\n",
       "      <td>en_en</td>\n",
       "      <td>https://www.stlucianewsonline.com</td>\n",
       "      <td>Wed Jan  1 21:17:15 2020</td>\n",
       "      <td>https://www.thestar.com</td>\n",
       "      <td>Wed Jan  1 00:00:00 2020</td>\n",
       "      <td>Guyana: Three injured after car crashes into u...</td>\n",
       "      <td>Share This On:\\n\\nPin 11 Shares\\n\\n(NEWS ROOM ...</td>\n",
       "      <td>-</td>\n",
       "      <td>['']</td>\n",
       "      <td>...</td>\n",
       "      <td>BERLIN - A fire at a zoo in western Germany in...</td>\n",
       "      <td>BERLIN - A fire at a zoo in western Germany in...</td>\n",
       "      <td>['smg2_world', 'smg_europe', 'smg2_news']</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>1.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1484698254_1483758694</td>\n",
       "      <td>en_en</td>\n",
       "      <td>https://www.teaparty.org</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.timesofisrael.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Trump Brings In 2020 At Mar-a-Lago: ‘We’re Goi...</td>\n",
       "      <td>(Breitbart) – President Donald Trump welcomed ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>...</td>\n",
       "      <td>PALM BEACH, United States — US President Donal...</td>\n",
       "      <td>US president says response to rioting by pro-I...</td>\n",
       "      <td>['']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1576314516_1576455088</td>\n",
       "      <td>en_en</td>\n",
       "      <td>https://gadgets.ndtv.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://gadgets.ndtv.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zomato Buys Uber's Food Delivery Business in I...</td>\n",
       "      <td>Uber has sold its online food-ordering busines...</td>\n",
       "      <td>Zomato on Tuesday announced it has acquired Ub...</td>\n",
       "      <td>['zomato uber eats business acquisition india ...</td>\n",
       "      <td>...</td>\n",
       "      <td>Rapid digitisation and growth in both online b...</td>\n",
       "      <td>The report by Google and Boston Consulting Gro...</td>\n",
       "      <td>['indian online food delivery market to hit us...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1484036253_1483894099</td>\n",
       "      <td>en_en</td>\n",
       "      <td>https://news.yahoo.com</td>\n",
       "      <td>Wed Jan  1 08:57:59 2020</td>\n",
       "      <td>https://www.channelnewsasia.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>India approves third moon mission, months afte...</td>\n",
       "      <td>BENGALURU (Reuters) - India has approved its t...</td>\n",
       "      <td>India has approved its third lunar mission mon...</td>\n",
       "      <td>['']</td>\n",
       "      <td>...</td>\n",
       "      <td>BANGALORE: India plans to make a fresh attempt...</td>\n",
       "      <td>India plans to make a fresh attempt to land an...</td>\n",
       "      <td>['India', 'space']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4959</th>\n",
       "      <td>1586195445_1598778991</td>\n",
       "      <td>tr_tr</td>\n",
       "      <td>http://www.haberler.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.haberler.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BM, Aden'de 2 bini aşkın iç göçmenin selden za...</td>\n",
       "      <td>BM, Aden'de 2 bini aşkın iç göçmenin selden za...</td>\n",
       "      <td>Birleşmiş Milletler (BM), 2 bini aşkın iç göçm...</td>\n",
       "      <td>['Birleşmiş Milletler', 'Twitter', 'Yemen', 'G...</td>\n",
       "      <td>...</td>\n",
       "      <td>BM'den Yemen'de kadınların doğumda ölüm riski ...</td>\n",
       "      <td>BİRLEŞMİŞ Birleşmiş Milletler (BM) dünyanın en...</td>\n",
       "      <td>['Birleşmiş Milletler', 'Yemen', 'Güncel', 'Ha...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4960</th>\n",
       "      <td>1590915424_1590940388</td>\n",
       "      <td>tr_tr</td>\n",
       "      <td>https://www.haberler.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.aksam.com.tr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kovid-19'dan dolayı La Liga kulüplerinde hayat...</td>\n",
       "      <td>Kovid-19'dan dolayı La Liga kulüplerinde hayat...</td>\n",
       "      <td>Yeni tip koronavirüs (Kovid-19) salgınının eko...</td>\n",
       "      <td>['Real Madrid', 'İspanya', 'La Liga', 'Futbol'...</td>\n",
       "      <td>...</td>\n",
       "      <td>Yeni tip koronavirüs (Kovid-19) salgınının eko...</td>\n",
       "      <td>Yeni tip koronavirüs (Kovid-19) salgınının eko...</td>\n",
       "      <td>['İspanya 1. Futbol Ligi', 'la liga', 'koronav...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4961</th>\n",
       "      <td>1526157103_1492737005</td>\n",
       "      <td>tr_tr</td>\n",
       "      <td>http://www.samanyoluhaber.com</td>\n",
       "      <td>Thu Feb 20 10:47:10 2020</td>\n",
       "      <td>https://www.fotomac.com.tr</td>\n",
       "      <td>Sun Jan 12 00:00:00 2020</td>\n",
       "      <td>Saray da çare olmadı: 'Borca boğulan dev kulüp...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\nİflas noktasındaki kulüplerin ...</td>\n",
       "      <td>SARAY DA ÇARE OLMADI: 'BORCA BOĞULAN DEV KULÜP...</td>\n",
       "      <td>['Saray', 'da', 'çare', 'olmadı:', \"'Borca\", '...</td>\n",
       "      <td>...</td>\n",
       "      <td>TFF, resmi internet sitesinden Beşiktaş'ın fai...</td>\n",
       "      <td>Federasyon, Başkan Çebi’nin yaptığı açıklamala...</td>\n",
       "      <td>['']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4962</th>\n",
       "      <td>1603274500_1618292937</td>\n",
       "      <td>tr_tr</td>\n",
       "      <td>https://www.haberler.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.haberler.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ergene Belediyesi yol çalışmalarına aksatmadan...</td>\n",
       "      <td>Ergene Belediyesi yol çalışmalarına aksatmadan...</td>\n",
       "      <td>Tekirdağ'ın Ergene Belediyesi, Covid-19 salgın...</td>\n",
       "      <td>['Rasim Yüksel', 'Koronavirüs', 'Tekirdağ', 'E...</td>\n",
       "      <td>...</td>\n",
       "      <td>Ergene'de Ahimehmet ve Yeşiltepe mahallelerind...</td>\n",
       "      <td>Covid-19 salgınından vatandaşların korunması i...</td>\n",
       "      <td>['Koronavirüs', 'Yeşiltepe', 'Yaşam', 'Haber']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4963</th>\n",
       "      <td>1597247537_1604202164</td>\n",
       "      <td>tr_tr</td>\n",
       "      <td>https://www.takvim.com.tr</td>\n",
       "      <td>Tue May  5 00:00:00 2020</td>\n",
       "      <td>https://www.yeniasir.com.tr</td>\n",
       "      <td>Tue May 12 00:00:00 2020</td>\n",
       "      <td>Grup Yorum üyeleri zorla başlatmıştı... İbrahi...</td>\n",
       "      <td>DHKP-C Terör Örgütü üyeliğinden yargılanan ve ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>...</td>\n",
       "      <td>Ceza Mahkemesi'nde DHKP-C terör örgütü üyeliği...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4964 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    pair_id pair_lang                       source_url_1  \\\n",
       "0     1484084337_1484110209     en_en     https://www.washingtonpost.com   \n",
       "1     1484396422_1483924666     en_en  https://www.stlucianewsonline.com   \n",
       "2     1484698254_1483758694     en_en           https://www.teaparty.org   \n",
       "3     1576314516_1576455088     en_en           https://gadgets.ndtv.com   \n",
       "4     1484036253_1483894099     en_en             https://news.yahoo.com   \n",
       "...                     ...       ...                                ...   \n",
       "4959  1586195445_1598778991     tr_tr            http://www.haberler.com   \n",
       "4960  1590915424_1590940388     tr_tr           https://www.haberler.com   \n",
       "4961  1526157103_1492737005     tr_tr      http://www.samanyoluhaber.com   \n",
       "4962  1603274500_1618292937     tr_tr           https://www.haberler.com   \n",
       "4963  1597247537_1604202164     tr_tr          https://www.takvim.com.tr   \n",
       "\n",
       "                publish_date_1                     source_url_2  \\\n",
       "0     Wed Jan  1 00:00:00 2020   https://www.washingtonpost.com   \n",
       "1     Wed Jan  1 21:17:15 2020          https://www.thestar.com   \n",
       "2                          NaN    https://www.timesofisrael.com   \n",
       "3                          NaN         https://gadgets.ndtv.com   \n",
       "4     Wed Jan  1 08:57:59 2020  https://www.channelnewsasia.com   \n",
       "...                        ...                              ...   \n",
       "4959                       NaN         https://www.haberler.com   \n",
       "4960                       NaN         https://www.aksam.com.tr   \n",
       "4961  Thu Feb 20 10:47:10 2020       https://www.fotomac.com.tr   \n",
       "4962                       NaN         https://www.haberler.com   \n",
       "4963  Tue May  5 00:00:00 2020      https://www.yeniasir.com.tr   \n",
       "\n",
       "                publish_date_2  \\\n",
       "0     Wed Jan  1 00:00:00 2020   \n",
       "1     Wed Jan  1 00:00:00 2020   \n",
       "2                          NaN   \n",
       "3                          NaN   \n",
       "4                          NaN   \n",
       "...                        ...   \n",
       "4959                       NaN   \n",
       "4960                       NaN   \n",
       "4961  Sun Jan 12 00:00:00 2020   \n",
       "4962                       NaN   \n",
       "4963  Tue May 12 00:00:00 2020   \n",
       "\n",
       "                                                title_1  \\\n",
       "0     Virginia man arrested in fatal DUI crash in We...   \n",
       "1     Guyana: Three injured after car crashes into u...   \n",
       "2     Trump Brings In 2020 At Mar-a-Lago: ‘We’re Goi...   \n",
       "3     Zomato Buys Uber's Food Delivery Business in I...   \n",
       "4     India approves third moon mission, months afte...   \n",
       "...                                                 ...   \n",
       "4959  BM, Aden'de 2 bini aşkın iç göçmenin selden za...   \n",
       "4960  Kovid-19'dan dolayı La Liga kulüplerinde hayat...   \n",
       "4961  Saray da çare olmadı: 'Borca boğulan dev kulüp...   \n",
       "4962  Ergene Belediyesi yol çalışmalarına aksatmadan...   \n",
       "4963  Grup Yorum üyeleri zorla başlatmıştı... İbrahi...   \n",
       "\n",
       "                                                 text_1  \\\n",
       "0     MARTINSBURG, W.Va. — A suspected drunken drive...   \n",
       "1     Share This On:\\n\\nPin 11 Shares\\n\\n(NEWS ROOM ...   \n",
       "2     (Breitbart) – President Donald Trump welcomed ...   \n",
       "3     Uber has sold its online food-ordering busines...   \n",
       "4     BENGALURU (Reuters) - India has approved its t...   \n",
       "...                                                 ...   \n",
       "4959  BM, Aden'de 2 bini aşkın iç göçmenin selden za...   \n",
       "4960  Kovid-19'dan dolayı La Liga kulüplerinde hayat...   \n",
       "4961  \\n\\n\\n\\n\\n\\n\\n\\nİflas noktasındaki kulüplerin ...   \n",
       "4962  Ergene Belediyesi yol çalışmalarına aksatmadan...   \n",
       "4963  DHKP-C Terör Örgütü üyeliğinden yargılanan ve ...   \n",
       "\n",
       "                                     meta_description_1  \\\n",
       "0     Police in West Virginia say a suspected drunke...   \n",
       "1                                                     -   \n",
       "2                                                   NaN   \n",
       "3     Zomato on Tuesday announced it has acquired Ub...   \n",
       "4     India has approved its third lunar mission mon...   \n",
       "...                                                 ...   \n",
       "4959  Birleşmiş Milletler (BM), 2 bini aşkın iç göçm...   \n",
       "4960  Yeni tip koronavirüs (Kovid-19) salgınının eko...   \n",
       "4961  SARAY DA ÇARE OLMADI: 'BORCA BOĞULAN DEV KULÜP...   \n",
       "4962  Tekirdağ'ın Ergene Belediyesi, Covid-19 salgın...   \n",
       "4963                                                NaN   \n",
       "\n",
       "                                        meta_keywords_1  ...  \\\n",
       "0     ['Highway Fatal-DUI-West Virginia', 'Martinsbu...  ...   \n",
       "1                                                  ['']  ...   \n",
       "2                                                  ['']  ...   \n",
       "3     ['zomato uber eats business acquisition india ...  ...   \n",
       "4                                                  ['']  ...   \n",
       "...                                                 ...  ...   \n",
       "4959  ['Birleşmiş Milletler', 'Twitter', 'Yemen', 'G...  ...   \n",
       "4960  ['Real Madrid', 'İspanya', 'La Liga', 'Futbol'...  ...   \n",
       "4961  ['Saray', 'da', 'çare', 'olmadı:', \"'Borca\", '...  ...   \n",
       "4962  ['Rasim Yüksel', 'Koronavirüs', 'Tekirdağ', 'E...  ...   \n",
       "4963                                               ['']  ...   \n",
       "\n",
       "                                                 text_2  \\\n",
       "0     PORT-AU-PRINCE, Haiti — Haitian President Jove...   \n",
       "1     BERLIN - A fire at a zoo in western Germany in...   \n",
       "2     PALM BEACH, United States — US President Donal...   \n",
       "3     Rapid digitisation and growth in both online b...   \n",
       "4     BANGALORE: India plans to make a fresh attempt...   \n",
       "...                                                 ...   \n",
       "4959  BM'den Yemen'de kadınların doğumda ölüm riski ...   \n",
       "4960  Yeni tip koronavirüs (Kovid-19) salgınının eko...   \n",
       "4961  TFF, resmi internet sitesinden Beşiktaş'ın fai...   \n",
       "4962  Ergene'de Ahimehmet ve Yeşiltepe mahallelerind...   \n",
       "4963  Ceza Mahkemesi'nde DHKP-C terör örgütü üyeliği...   \n",
       "\n",
       "                                     meta_description_2  \\\n",
       "0     Haitian President Jovenel Moïse has broken wit...   \n",
       "1     BERLIN - A fire at a zoo in western Germany in...   \n",
       "2     US president says response to rioting by pro-I...   \n",
       "3     The report by Google and Boston Consulting Gro...   \n",
       "4     India plans to make a fresh attempt to land an...   \n",
       "...                                                 ...   \n",
       "4959  BİRLEŞMİŞ Birleşmiş Milletler (BM) dünyanın en...   \n",
       "4960  Yeni tip koronavirüs (Kovid-19) salgınının eko...   \n",
       "4961  Federasyon, Başkan Çebi’nin yaptığı açıklamala...   \n",
       "4962  Covid-19 salgınından vatandaşların korunması i...   \n",
       "4963                                                NaN   \n",
       "\n",
       "                                        meta_keywords_2 Geography  Entities  \\\n",
       "0     ['CB-Haiti-Political Turmoil', 'Jean', 'Haiti'...       4.0  4.000000   \n",
       "1             ['smg2_world', 'smg_europe', 'smg2_news']       4.0  4.000000   \n",
       "2                                                  ['']       1.0  2.000000   \n",
       "3     ['indian online food delivery market to hit us...       1.0  2.333333   \n",
       "4                                    ['India', 'space']       1.0  1.250000   \n",
       "...                                                 ...       ...       ...   \n",
       "4959  ['Birleşmiş Milletler', 'Yemen', 'Güncel', 'Ha...       1.0  2.000000   \n",
       "4960  ['İspanya 1. Futbol Ligi', 'la liga', 'koronav...       1.0  1.000000   \n",
       "4961                                               ['']       1.0  2.000000   \n",
       "4962     ['Koronavirüs', 'Yeşiltepe', 'Yaşam', 'Haber']       1.0  2.000000   \n",
       "4963                                               ['']       2.0  2.000000   \n",
       "\n",
       "          Time  Narrative   Overall     Style      Tone  \n",
       "0     1.000000   4.000000  4.000000  1.666667  2.000000  \n",
       "1     1.000000   4.000000  3.666667  1.666667  1.333333  \n",
       "2     1.000000   2.333333  2.333333  1.000000  1.333333  \n",
       "3     2.666667   1.666667  2.000000  1.666667  1.666667  \n",
       "4     1.000000   1.250000  1.250000  1.000000  1.000000  \n",
       "...        ...        ...       ...       ...       ...  \n",
       "4959  2.000000   4.000000  3.000000  1.000000  1.000000  \n",
       "4960  1.000000   1.000000  1.000000  1.000000  1.000000  \n",
       "4961  3.000000   4.000000  3.000000  1.000000  2.000000  \n",
       "4962  3.000000   3.000000  3.000000  1.000000  1.000000  \n",
       "4963  1.000000   2.000000  2.000000  3.000000  1.000000  \n",
       "\n",
       "[4964 rows x 21 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remémorer numéro de ligne - compléter les Nan\n",
    "data['ligne'] = data.index\n",
    "data = data.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# séparation des datasets, le dernier étant à traduire en plus\n",
    "anglais = data.loc[data.pair_lang == 'en_en',['ligne','title_1','title_2','text_1','text_2','Geography', 'Entities',\n",
    "       'Time', 'Narrative', 'Overall', 'Style', 'Tone']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter aussi le modèle anglais_all_traduit \n",
    "anglais_all_traduit = pd.read_csv('allemand_anglais_traduit.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tests divers et Fonctions nécessaires**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Uber has sold its online food-ordering business in India to local rival Zomato in exchange for a 9.99 percent stake in the startup backed by China's Ant Financial . Uber Eats in India accounted for 3 percent of the business' gross bookings globally, but more than a quarter of its adjusted EBITDA loss in first three quarters of 2019 .\n",
      " India's online food industry to become an $8 billion (roughly Rs. 56,000) market by 2022 -- growing at a CAGR of 25-30 percent . Zomato and Swiggy currently dominate the online food delivery market in India . Peer or network advocacy played a critical role in drawing people to try online food ordering for the first time .\n"
     ]
    }
   ],
   "source": [
    "# Résumés\n",
    "print(summarizer1(anglais.text_1[3])[0]['summary_text'])\n",
    "print(summarizer1(anglais.text_2[3])[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's food for thought for Uber.\n",
      "\"Ordering food online is now a habit.\"\n"
     ]
    }
   ],
   "source": [
    "print(summarizer2(anglais.text_1[3])[0]['summary_text'])\n",
    "print(summarizer2(anglais.text_2[3])[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de calcul du score (produit scalaire) pour résultats de classifaction\n",
    "def fonction_produit_dotcom(liste_categor, dico_scores1,dico_scores2):\n",
    "    \"\"\"\"dico scores sont les résultats obtenus pour chaque catégorie des textes 1 et 2\"\"\"\n",
    "    result = 0.0\n",
    "    for cat in liste_categor:\n",
    "        result += round(dico_scores1[cat] * dico_scores2[cat],4)\n",
    "    return result * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation des résultats du transformer type1\n",
    "def transform_text_clf1(liste_dico):\n",
    "    res = {}\n",
    "    for dic in liste_dico:\n",
    "        res[dic['label']] = dic['score']\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation des résultats du transformer type2\n",
    "def transform_text_clf2(liste_cat,liste_sc):\n",
    "    res = {}\n",
    "    for i in range(len(liste_cat)):\n",
    "        res[liste_cat[i]] = liste_sc[i]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.609999999999996\n"
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "classes = text_clf1(anglais.text_1[3], candidate_labels)\n",
    "scores1 = transform_text_clf2(classes['labels'],classes['scores'])\n",
    "classes2 = text_clf1(anglais.text_2[3], candidate_labels)\n",
    "scores2 = transform_text_clf2(classes2['labels'],classes2['scores'])\n",
    "print(fonction_produit_dotcom(candidate_labels, scores1,scores2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.33\n"
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "classes = text_clf2(anglais.text_1[3], candidate_labels)\n",
    "scores1 = transform_text_clf2(classes['labels'],classes['scores'])\n",
    "classes2 = text_clf2(anglais.text_2[3], candidate_labels)\n",
    "scores2 = transform_text_clf2(classes2['labels'],classes2['scores'])\n",
    "print(fonction_produit_dotcom(candidate_labels, scores1,scores2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1 star': 0.44524091482162476, '2 stars': 0.2690126299858093, '3 stars': 0.12020471692085266, '4 stars': 0.09190544486045837, '5 stars': 0.07363627851009369}\n",
      "{'1 star': 0.16283643245697021, '2 stars': 0.1999920904636383, '3 stars': 0.1352054625749588, '4 stars': 0.2618146240711212, '5 stars': 0.24015137553215027}\n",
      "18.44\n"
     ]
    }
   ],
   "source": [
    "# Tests sentiment analysis\n",
    "liste_labels = ['1 star','2 stars','3 stars','4 stars','5 stars']\n",
    "scores1 = transform_text_clf1(sentiment1(anglais.text_1[3],return_all_scores=True)[0])\n",
    "scores2 = transform_text_clf1(sentiment1(anglais.text_2[3],return_all_scores=True)[0])\n",
    "print(scores1)\n",
    "print(scores2)\n",
    "print(fonction_produit_dotcom(liste_labels, scores1,scores2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.90999999999999\n",
      "94.58\n"
     ]
    }
   ],
   "source": [
    "# ATTENTION CE MODELE SE DEFINIT SUR 4 CLASSES : mixed, positif, negatif, objectif\n",
    "liste_sentiments = ['joy','anger','sadness','love','surprise','fear']\n",
    "scores1 = transform_text_clf1(sentiment2(anglais.title_1[3],return_all_scores=True)[0])\n",
    "scores2 = transform_text_clf1(sentiment2(anglais.title_2[3],return_all_scores=True)[0])\n",
    "scores3 = transform_text_clf1(sentiment2(anglais.text_1[3],return_all_scores=True)[0])\n",
    "scores4 = transform_text_clf1(sentiment2(anglais.text_2[3],return_all_scores=True)[0])\n",
    "print(fonction_produit_dotcom(liste_sentiments, scores1,scores2))\n",
    "print(fonction_produit_dotcom(liste_sentiments, scores3,scores4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NEGATIVE': 0.9709159731864929, 'POSITIVE': 0.029084037989377975}\n",
      "{'NEGATIVE': 0.9774531126022339, 'POSITIVE': 0.022546928375959396}\n",
      "94.97\n"
     ]
    }
   ],
   "source": [
    "liste_sentiments2 = ['NEGATIVE','POSITIVE']\n",
    "scores1 = transform_text_clf1(sentiment3(anglais.text_1[3],return_all_scores=True)[0])\n",
    "scores2 = transform_text_clf1(sentiment3(anglais.text_2[3],return_all_scores=True)[0])\n",
    "print(scores1)\n",
    "print(scores2)\n",
    "print(fonction_produit_dotcom(liste_sentiments2, scores1,scores2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.97\n",
      "39.28\n",
      "3.79\n",
      "51.0\n"
     ]
    }
   ],
   "source": [
    "# TESTS SCORES DE SIMILARITES : Titres, Textes complets (attention Truncation ...)\n",
    "print(score_similarite(anglais.title_1[3],anglais.title_2[3]))\n",
    "print(score_similarite(anglais.text_1[3],anglais.text_2[3]))\n",
    "print(score_similarite(summarizer1(anglais.text_1[0])[0]['summary_text'],summarizer1(anglais.text_2[0])[0]['summary_text']))\n",
    "print(score_similarite(summarizer2(anglais.text_1[0])[0]['summary_text'],summarizer2(anglais.text_2[0])[0]['summary_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('child', 0.8979307413101196),\n",
       " ('murder', 0.8967654705047607),\n",
       " ('killing', 0.8901609182357788),\n",
       " ('kills', 0.8862658143043518),\n",
       " ('attack', 0.8817499876022339),\n",
       " ('victim', 0.8787229657173157),\n",
       " ('killed', 0.8760652542114258),\n",
       " ('suicide', 0.875905454158783),\n",
       " ('dies', 0.8747766613960266),\n",
       " ('died', 0.8738439083099365)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Essais de synonymies à utiliser pour sorties PKE, termes prinicpaux\n",
    "syns = model_gensim.most_similar(\"death\", topn=10)\n",
    "syns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions de summarization \n",
    "def summarization(texte):\n",
    "    return summarizer1(texte)[0]['summary_text'], summarizer2(texte)[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_classifiers = {'text_clf1': 'score_classif1','text_clf2':'score_classif2','sentiment1':'score_sentiment1',\n",
    "                    'sentiment2': 'score_sentiment2','sentiment3': 'score_sentiment3'}\n",
    "dico_categories = {'text_clf1': candidate_labels,'text_clf2':candidate_labels,'sentiment1':liste_labels,\n",
    "                    'sentiment2': liste_sentiments,'sentiment3': liste_sentiments2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions de classification et sentiment analysis\n",
    "def classification(texte,clf):\n",
    "    # assume nms des claasifiers et methode de transformation\n",
    "    if clf == \"text_clf1\":\n",
    "        try:\n",
    "            classes = text_clf1(texte,dico_categories['text_clf1'])\n",
    "        except:\n",
    "            return 'error'\n",
    "        else:\n",
    "            return transform_text_clf2(classes['labels'],classes['scores'])    \n",
    "    elif clf == \"text_clf2\":                                 \n",
    "        try:\n",
    "            classes = text_clf2(texte,dico_categories['text_clf2'])\n",
    "        except:\n",
    "            return 'error'\n",
    "        else:\n",
    "            return transform_text_clf2(classes['labels'],classes['scores'])                          \n",
    "    elif clf == \"sentiment1\":\n",
    "        try:\n",
    "            scores = transform_text_clf1(sentiment1(texte,return_all_scores=True)[0])\n",
    "        except:\n",
    "            return 'error'\n",
    "        else:\n",
    "            return scores\n",
    "    elif clf == \"sentiment2\":\n",
    "        try:\n",
    "            scores = transform_text_clf1(sentiment2(texte,return_all_scores=True)[0])\n",
    "        except:\n",
    "            return 'error'\n",
    "        else:\n",
    "            return scores\n",
    "    elif clf == \"sentiment3\":\n",
    "        try:\n",
    "            scores = transform_text_clf1(sentiment3(texte,return_all_scores=True)[0])\n",
    "        except:\n",
    "            return 'error'\n",
    "        else:\n",
    "            return scores\n",
    "    else:\n",
    "        return 'error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraitement NLP pour PKE : suppression des mots de moins de 2 lettres non numériques\n",
    "def supp_moins_2_lettres_stopwords(phrase,stopwd):\n",
    "    temp = phrase.split(' ')\n",
    "    res = ''\n",
    "    for mot in temp:\n",
    "        if mot not in stopwd and (len(mot)>2 or (len(mot)>0 and mot[0] in ['0','1','2','3','4','5','6','7','8','9'])):\n",
    "            res += mot + ' '\n",
    "    return res[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraitement NLP pour PKE : suppression des traits d'union(regroupe)/ des apostrophes / ponctuations\n",
    "def modif(texte,stopmots):\n",
    "    # modifications simples des textes : ponctuations, petits mots, stopwords (à faire pour entités et pke textes)\n",
    "    texte=re.sub('\\'',' ',texte)   # suppression apostrophe\n",
    "    texte=re.sub('-','',texte)    # suppression trait union\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation)) # suppression de toutes les ponctuations\n",
    "    texte=regex.sub(' ',texte)\n",
    "    texte = supp_moins_2_lettres_stopwords(texte,stopmots)\n",
    "    return texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout des synonymes (existants en orthographe) à la suite de l'analyse pke\n",
    "def ajout_synonymes(mot, correct_ortho = True):\n",
    "    # on ajoute les 10 premiers synonymes existants, on vérifie orthographe (optionnel)\n",
    "    syns = model_gensim.most_similar(mot,topn = 20)\n",
    "    if correct_ortho == True:\n",
    "        res = []\n",
    "        for m in syns:\n",
    "            if d.check(m[0]):   #  il y a le mot et son pourcentage d'importance\n",
    "                res.append(m)\n",
    "        syns = res\n",
    "    return syns[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelle est la bonne méthodologie à appliquer PKE ??? : Noms Ok - Verbes ? - Adjectifs ? - Noms propres ?\n",
    "Quelle quantité de mots prendre ?\n",
    "Ne pas noter les titres seuls ? : trop peu de mots !  Et du coup note peut être forte avec un seul mot !\n",
    "Supprimer en en-tête les stopwords, ponctuation, les apostrophes - AUTRES ?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choix des paramètres de la méthode : A revoir ?\n",
    "methode1 = {\"NOUN\", \"PROPN\", \"ADJ\",\"VERB\"}\n",
    "methode2 = {\"NOUN\", \"PROPN\", \"ADJ\"}\n",
    "nb_mots = {'meth1': 30, 'meth2':50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PKE : Analyse des termes principaux dans les textes et titres \n",
    "# Problème \n",
    "def transformation_pke_results(res1,res2, correct_ortho = True):\n",
    "    \"\"\"\n",
    "    Transformation des resultats de PKE : Pb bigramme peuvent ne pas être ds les 2 textes mais 1 mot seulement\n",
    "    liste de clés et dictionnaires de valeurs, bigrammes jouera ainsi de maniere coefficientée \n",
    "    Exemple : fuite eau:0.05 --> 3 mots au final : fuite, eau, fuite eau : 0.05\n",
    "    De plus on ajoute les synonymes issus de gensim en les coefficiant et vérifiant que cela \"\"\"\n",
    "    \n",
    "    liste1 = []; liste2 = [] ; dico1 = {}; dico2 = {}\n",
    "    for elt in res1:\n",
    "        liste1.append(elt[0])\n",
    "        dico1[elt[0]] = round(elt[1],3)\n",
    "        if ' ' in elt[0]:    # bigramme dans ce cas, ajout des 2 mots\n",
    "            liste = elt[0].split(' ')\n",
    "            for mot in liste:\n",
    "                liste1.append(mot)\n",
    "                dico1[mot] = round(elt[1],3)\n",
    "                try:\n",
    "                    synonyms = ajout_synonymes(mot,correct_ortho = correct_ortho)\n",
    "                except:\n",
    "                    pass\n",
    "                else:\n",
    "                    for syn in synonyms:\n",
    "                        liste1.append(syn[0])   # Ajout du mot \n",
    "                        dico1[syn[0]] = round(elt[1] * syn[1], 3)  # poids considéré\n",
    "                    \n",
    "    for elt in res2:\n",
    "        liste2.append(elt[0])\n",
    "        dico2[elt[0]] = round(elt[1],3)\n",
    "        if ' ' in elt[0]:\n",
    "            liste = elt[0].split(' ')\n",
    "            for mot in liste:\n",
    "                liste2.append(mot)\n",
    "                dico2[mot] = round(elt[1],3)\n",
    "                try:\n",
    "                    synonyms = ajout_synonymes(mot,correct_ortho = correct_ortho)\n",
    "                except:\n",
    "                    pass\n",
    "                else:\n",
    "                    for syn in synonyms:\n",
    "                        liste2.append(syn[0])   # Ajout du mot \n",
    "                        dico2[syn[0]] = round(elt[1] * syn[1], 3)  # poids considéré\n",
    "    \n",
    "    # similarites entre les 2 listes issus de pke avec poids\n",
    "    sim = 0\n",
    "    for elt in liste1:\n",
    "        if elt in liste2:\n",
    "            sim += (dico1[elt] + dico2[elt])/2\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entites_communes(nlp,text1,text2):\n",
    "    \"\"\"\"\n",
    "    Cette première fonction ne regarde que les entités communes : personnes, dates, groupe, localisations\n",
    "    Elle sera appliquée aux textes et aux titres et cumulé : si cumul en titre et texte : compte double !\"\"\"\n",
    "    \n",
    "    doc1 = nlp(text1)\n",
    "    doc2 = nlp(text2)\n",
    "    nb_commun_ent = 0; liste_commun_ent = []\n",
    "    nb_commun_geo = 0; liste_commun_geo = []\n",
    "    nb_commun_dat = 0; liste_commun_dat = []\n",
    "    \n",
    "    if len(doc1.ents)>0 and len(doc2.ents)>0:\n",
    "        liste1 = []; dico1 = {}\n",
    "        for elt in doc1.ents:\n",
    "            if elt.label_ in ['PERSON','PER'] and ' ' in elt.text:\n",
    "                mots = elt.text.split(' ')\n",
    "                for mot in mots:\n",
    "                    if mot not in liste1:\n",
    "                        liste1.append(mot)\n",
    "                        dico1[mot] = elt.label_\n",
    "            elif elt.label_ in ['LOC','ORG','GPE','DATE','TIME']:\n",
    "                if elt.text not in liste1:\n",
    "                    liste1.append(elt.text)\n",
    "                    dico1[elt.text] = elt.label_\n",
    "        liste2 = []\n",
    "        for elt in doc2.ents:\n",
    "            if elt.label_ in ['PERSON','PER'] and ' ' in elt.text:\n",
    "                mots = elt.text.split(' ')\n",
    "                for mot in mots:\n",
    "                    if mot not in liste2:\n",
    "                        liste2.append(mot)\n",
    "            elif elt.label_ in ['LOC','ORG','GPE','DATE','TIME']:\n",
    "                if elt.text not in liste2:\n",
    "                    liste2.append(elt.text)\n",
    "        \n",
    "        # points communs des listes        \n",
    "        for elt in liste1:\n",
    "            if elt in liste2:\n",
    "                if dico1[elt] == 'LOC':\n",
    "                    nb_commun_geo += 1\n",
    "                    liste_commun_geo.append(elt)\n",
    "                elif dico1[elt] in ['DATE','TIME']:\n",
    "                    nb_commun_dat += 1\n",
    "                    liste_commun_dat.append(elt)\n",
    "                else:\n",
    "                    nb_commun_ent += 1\n",
    "                    liste_commun_ent.append(elt)\n",
    "                    \n",
    "    return nb_commun_ent, liste_commun_ent,nb_commun_geo, liste_commun_geo,nb_commun_dat, liste_commun_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Creation_features_comparaison(df,langue, test_position = [methode1,methode2]):\n",
    "    \"\"\"Création des notes pour classification ensuite\"\"\"\n",
    "    \n",
    "    resultats = pd.DataFrame(columns = ['summary1_text1','summary2_text1','summary1_text2','summary2_text2',\n",
    "            'nb_entites_idem','nb_lieux_idem', 'nb_dates_idem','entites_idem','lieux_idem','dates_idem',\n",
    "            'score_similarite_titres','score_similarite_resume1','score_similarite_resume2','score_classif1','score_classif2',\n",
    "            'score_sentiment1','score_sentiment2','score_sentiment3','meth1_similarites','meth2_similarites'])\n",
    "    \n",
    "    # initialisation de la langue stanza\n",
    "    stanza.download(langue)\n",
    "    nlp_stanza = spacy_stanza.load_pipeline(langue)\n",
    "    stopmts = stopwds_lg[langue]\n",
    "    if langue in dico_spacy.keys():\n",
    "        nlp_spacy = dico_spacy[langue]\n",
    "    else:\n",
    "        nlp_spacy = None\n",
    "        \n",
    "    for i in tqdm(range(len(df))):\n",
    "        dico_res = {}\n",
    "        \n",
    "        # Summary et comparatifs \n",
    "        dico_res['summary1_text1'],dico_res['summary2_text1'] = summarization(df.text_1[i])\n",
    "        dico_res['summary1_text2'],dico_res['summary2_text2'] = summarization(df.text_2[i])\n",
    "        dico_res['score_similarite_titres'] = score_similarite(df.title_1[i],df.title_2[i])\n",
    "        dico_res['score_similarite_resume1'] = score_similarite(dico_res['summary1_text1'],dico_res['summary1_text2'])\n",
    "        dico_res['score_similarite_resume2'] = score_similarite(dico_res['summary2_text1'],dico_res['summary2_text2'])\n",
    "        \n",
    "        # analyse de textes classification et de sentiments\n",
    "        texte1 = df.title_1[i] + ' ' + df.text_1[i]\n",
    "        texte2 = df.title_2[i] + ' ' + df.text_2[i]\n",
    "        if len(texte1)>0 and len(texte2)>0:\n",
    "            for classifier in dico_classifiers.keys():\n",
    "                scores1 = classification(texte1,classifier)\n",
    "                scores2 = classification(texte2,classifier)\n",
    "                if scores1 != 'error' and scores2 != 'error':\n",
    "                    dico_res[dico_classifiers[classifier]] = fonction_produit_dotcom(dico_categories[classifier], scores1,scores2)\n",
    "                else:\n",
    "                    scores1 = classification(df.title_1[i],classifier)\n",
    "                    scores2 = classification(df.title_2[i],classifier)\n",
    "                    if scores1 != 'error' and scores2 != 'error':\n",
    "                        dico_res[dico_classifiers[classifier]] = fonction_produit_dotcom(dico_categories[classifier], scores1,scores2)\n",
    "                    else:\n",
    "                        dico_res[dico_classifiers[classifier]] = None\n",
    "                \n",
    "        # pré traitement des textes pour entités et PKE\n",
    "        texte1 = modif(texte1, stopmts)\n",
    "        texte2 = modif(texte2, stopmts)\n",
    "        \n",
    "        # ENTITES COMMUNES : on tient compte des bigrammes Noms qui posent erreurs ex: Joe Biden et Biden \n",
    "        # Ici, on considère mieux le CUMUl titres et Textes avec une pondération double pour le titre \n",
    "        # Il faut aussi enlever les petits mots donc pré-traitement en texte\n",
    "        \n",
    "        try:\n",
    "            nb_ent1,list_ent1,nb_geo1,list_geo1,nb_dat1,list_dat1 = entites_communes(nlp_stanza,df.title_1[i],df.title_2[i])\n",
    "        except:\n",
    "            nb_ent1,list_ent1,nb_geo1,list_geo1,nb_dat1,list_dat1 = (0,[],0,[],0,[])\n",
    "        try:\n",
    "            nb_ent2,list_ent2,nb_geo2,list_geo2,nb_dat2,list_dat2 = entites_communes(nlp_stanza,df.text_1[i],df.text_2[i])\n",
    "        except:\n",
    "            nb_ent2,list_ent2,nb_geo2,list_geo2,nb_dat2,list_dat2 = (0,[],0,[],0,[])                                                                       \n",
    "        if nlp_spacy != None:\n",
    "            nb_ent3,list_ent3,nb_geo3,list_geo3,nb_dat3,list_dat3 = entites_communes(nlp_spacy,df.title_1[i],df.title_2[i])\n",
    "            nb_ent4,list_ent4,nb_geo4,list_geo4,nb_dat4,list_dat4 = entites_communes(nlp_spacy,df.text_1[i],df.text_2[i])\n",
    "        else:\n",
    "            nb_ent3,list_ent3,nb_geo3,list_geo3,nb_dat3,list_dat3 = (0,[],0,[],0,[])\n",
    "            nb_ent4,list_ent4,nb_geo4,list_geo4,nb_dat4,list_dat4 = (0,[],0,[],0,[])\n",
    "        dico_res['nb_entites_idem'] = nb_ent1 * 2 + nb_ent2 + nb_ent3 * 2 + nb_ent4\n",
    "        dico_res['nb_lieux_idem'] = nb_geo1  * 2 + nb_geo2 + nb_geo3  * 2 + nb_geo4\n",
    "        dico_res['nb_dates_idem'] = nb_dat1 * 2 + nb_dat2 + nb_dat3 * 2 + nb_dat4\n",
    "        # fusion des listes en supprimant les doublons\n",
    "        dico_res['entites_idem'] = list(set(list_ent1+list_ent2+ list_ent3+list_ent4))\n",
    "        dico_res['lieux_idem'] = list(set(list_geo1+list_geo2+list_geo3+list_geo4))\n",
    "        dico_res['dates_idem'] = list(set(list_dat1+list_dat2+list_dat3+list_dat4))\n",
    "        \n",
    "        for j,meth in enumerate(test_position):\n",
    "            nom ='meth'+str(j+1)\n",
    "            nb_mots_meth = nb_mots[nom]\n",
    "            if len(texte1)>0 and len(texte2)>0:\n",
    "                extractor = pke.unsupervised.TopicRank()\n",
    "                extractor.load_document(input=texte1,language=langue,normalization=\"stemming\")\n",
    "                extractor.candidate_selection(pos=meth)\n",
    "                extractor.candidate_weighting()\n",
    "                keyphrases3 = extractor.get_n_best(n=nb_mots_meth)\n",
    "                extractor = pke.unsupervised.TopicRank()\n",
    "                extractor.load_document(input=texte2,language=langue,normalization=\"stemming\")\n",
    "                extractor.candidate_selection(pos=meth)\n",
    "                extractor.candidate_weighting()\n",
    "                keyphrases4 = extractor.get_n_best(n=nb_mots_meth)\n",
    "                dico_res[nom+'_similarites'] = round(100*transformation_pke_results(keyphrases3,keyphrases4),1)\n",
    "            else:\n",
    "                dico_res[nom+'_similarites'] = 'Error'\n",
    "        \n",
    "        resultats.loc[len(resultats)] = dico_res\n",
    "        \n",
    "    newdf = pd.concat([df,resultats],axis=1)\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b1ef5fc9dd4f2e80de1c14300af22a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-23 17:56:17 INFO: Downloading default packages for language: en (English)...\n",
      "2021-12-23 17:56:18 INFO: File exists: C:\\Users\\stg-sdu\\stanza_resources\\en\\default.zip.\n",
      "2021-12-23 17:56:21 INFO: Finished downloading models and saved to C:\\Users\\stg-sdu\\stanza_resources.\n",
      "2021-12-23 17:56:21 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2021-12-23 17:56:21 INFO: Use device: cpu\n",
      "2021-12-23 17:56:21 INFO: Loading: tokenize\n",
      "2021-12-23 17:56:21 INFO: Loading: pos\n",
      "2021-12-23 17:56:22 INFO: Loading: lemma\n",
      "2021-12-23 17:56:22 INFO: Loading: depparse\n",
      "2021-12-23 17:56:22 INFO: Loading: sentiment\n",
      "2021-12-23 17:56:22 INFO: Loading: constituency\n",
      "2021-12-23 17:56:23 INFO: Loading: ner\n",
      "2021-12-23 17:56:23 INFO: Done loading processors!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1423fcf1c5564ee8981c067100ebee11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 142, but you input_length is only 134. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your min_length is set to 56, but you input_length is only 3. You might consider decreasing min_length manually, e.g. summarizer('...', min_length=10)\n",
      "Your max_length is set to 142, but you input_length is only 3. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 64, but you input_length is only 1. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    }
   ],
   "source": [
    "# Attention, problème de mémoire : faire par steps de 200, voir pb (peut arriver sur certains textes pourris)\n",
    "# le n°50 n'a pas de sens en texte 2 - nbreux pbs tags : 'GW'\n",
    "similarites = Creation_features_comparaison(anglais[230:250].reset_index(drop=True),'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# similarites.to_csv('corpus_en_notes.csv')   # A Utiliser pour le premier\n",
    "precedent = pd.read_csv('corpus_en_notes.csv',index_col=0)\n",
    "similarites2 = pd.concat([precedent,similarites], axis=0)\n",
    "similarites2 = similarites2.reset_index(drop=True)\n",
    "similarites2.to_csv('corpus_en_notes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour vérification concat\n",
    "precedent = pd.read_csv('corpus_en_notes.csv',index_col=0)\n",
    "precedent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09113811a22e4169881d8ef4a6bff75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-23 13:56:37 INFO: Downloading default packages for language: en (English)...\n",
      "2021-12-23 13:56:38 INFO: File exists: C:\\Users\\stg-sdu\\stanza_resources\\en\\default.zip.\n",
      "2021-12-23 13:56:41 INFO: Finished downloading models and saved to C:\\Users\\stg-sdu\\stanza_resources.\n",
      "2021-12-23 13:56:41 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2021-12-23 13:56:41 INFO: Use device: cpu\n",
      "2021-12-23 13:56:41 INFO: Loading: tokenize\n",
      "2021-12-23 13:56:41 INFO: Loading: pos\n",
      "2021-12-23 13:56:41 INFO: Loading: lemma\n",
      "2021-12-23 13:56:41 INFO: Loading: depparse\n",
      "2021-12-23 13:56:41 INFO: Loading: sentiment\n",
      "2021-12-23 13:56:42 INFO: Loading: constituency\n",
      "2021-12-23 13:56:42 INFO: Loading: ner\n",
      "2021-12-23 13:56:43 INFO: Done loading processors!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcefdbfdff0b4c3483949d35679d58a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your min_length is set to 56, but you input_length is only 3. You might consider decreasing min_length manually, e.g. summarizer('...', min_length=10)\n",
      "Your max_length is set to 142, but you input_length is only 3. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 64, but you input_length is only 1. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 142, but you input_length is only 121. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your min_length is set to 56, but you input_length is only 3. You might consider decreasing min_length manually, e.g. summarizer('...', min_length=10)\n",
      "Your max_length is set to 142, but you input_length is only 3. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 64, but you input_length is only 1. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 142, but you input_length is only 41. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 64, but you input_length is only 36. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your min_length is set to 56, but you input_length is only 12. You might consider decreasing min_length manually, e.g. summarizer('...', min_length=10)\n",
      "Your max_length is set to 142, but you input_length is only 12. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 64, but you input_length is only 7. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your min_length is set to 56, but you input_length is only 3. You might consider decreasing min_length manually, e.g. summarizer('...', min_length=10)\n",
      "Your max_length is set to 142, but you input_length is only 3. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 64, but you input_length is only 1. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 142, but you input_length is only 112. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 142, but you input_length is only 116. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0beed309beb48e2ad9952dabb21419b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-23 15:24:58 INFO: Downloading default packages for language: en (English)...\n",
      "2021-12-23 15:24:58 INFO: File exists: C:\\Users\\stg-sdu\\stanza_resources\\en\\default.zip.\n",
      "2021-12-23 15:25:02 INFO: Finished downloading models and saved to C:\\Users\\stg-sdu\\stanza_resources.\n",
      "2021-12-23 15:25:02 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2021-12-23 15:25:02 INFO: Use device: cpu\n",
      "2021-12-23 15:25:02 INFO: Loading: tokenize\n",
      "2021-12-23 15:25:02 INFO: Loading: pos\n",
      "2021-12-23 15:25:02 INFO: Loading: lemma\n",
      "2021-12-23 15:25:02 INFO: Loading: depparse\n",
      "2021-12-23 15:25:02 INFO: Loading: sentiment\n",
      "2021-12-23 15:25:02 INFO: Loading: constituency\n",
      "2021-12-23 15:25:03 INFO: Loading: ner\n",
      "2021-12-23 15:25:03 INFO: Done loading processors!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45413d4725dd4dc5aff1cb0361437065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your min_length is set to 56, but you input_length is only 3. You might consider decreasing min_length manually, e.g. summarizer('...', min_length=10)\n",
      "Your max_length is set to 142, but you input_length is only 3. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 64, but you input_length is only 1. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 142, but you input_length is only 55. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 64, but you input_length is only 48. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 142, but you input_length is only 127. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 142, but you input_length is only 120. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your min_length is set to 56, but you input_length is only 22. You might consider decreasing min_length manually, e.g. summarizer('...', min_length=10)\n",
      "Your max_length is set to 142, but you input_length is only 22. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 64, but you input_length is only 20. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 142, but you input_length is only 44. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 64, but you input_length is only 38. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your min_length is set to 56, but you input_length is only 19. You might consider decreasing min_length manually, e.g. summarizer('...', min_length=10)\n",
      "Your max_length is set to 142, but you input_length is only 19. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 64, but you input_length is only 15. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 142, but you input_length is only 118. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your min_length is set to 56, but you input_length is only 3. You might consider decreasing min_length manually, e.g. summarizer('...', min_length=10)\n",
      "Your max_length is set to 142, but you input_length is only 3. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 64, but you input_length is only 1. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your min_length is set to 56, but you input_length is only 19. You might consider decreasing min_length manually, e.g. summarizer('...', min_length=10)\n",
      "Your max_length is set to 142, but you input_length is only 19. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 64, but you input_length is only 15. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 142, but you input_length is only 111. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 142, but you input_length is only 122. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 142, but you input_length is only 105. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your min_length is set to 56, but you input_length is only 3. You might consider decreasing min_length manually, e.g. summarizer('...', min_length=10)\n",
      "Your max_length is set to 142, but you input_length is only 3. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 64, but you input_length is only 1. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 142, but you input_length is only 47. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 64, but you input_length is only 44. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your min_length is set to 56, but you input_length is only 3. You might consider decreasing min_length manually, e.g. summarizer('...', min_length=10)\n",
      "Your max_length is set to 142, but you input_length is only 3. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 64, but you input_length is only 1. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    }
   ],
   "source": [
    "for j in range(2):\n",
    "    similarites = Creation_features_comparaison(anglais[100+50*j:100+50*(j+1)].reset_index(drop=True),'en')\n",
    "    precedent = pd.read_csv('corpus_en_notes.csv',index_col=0)\n",
    "    similarites2 = pd.concat([precedent,similarites], axis=0)\n",
    "    similarites2 = similarites2.reset_index(drop=True)\n",
    "    similarites2.to_csv('corpus_en_notes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention, problème de mémoire : faire par steps de 200, voir pb (peut arriver sur certains textes pourris)\n",
    "# similarites = Creation_features_comparaison(anglais_all_traduit[:200].reset_index(drop=True),'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarites.to_csv('corpus_en_notes.csv')   # A Utiliser pour le premier\n",
    "# precedent = pd.read_csv('corpus_en_de_notes.csv',index_col=0)\n",
    "# similarites2 = pd.concat([precedent,similarites], axis=0)\n",
    "# similarites2 = similarites2.reset_index(drop=True)\n",
    "# similarites2.to_csv('corpus_en_de_notes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File corpus_en_de_notes.csv does not exist: 'corpus_en_de_notes.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1520/2542600915.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# pour vérifivation concat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprecedent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'corpus_en_de_notes.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprecedent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File corpus_en_de_notes.csv does not exist: 'corpus_en_de_notes.csv'"
     ]
    }
   ],
   "source": [
    "# pour vérification concat\n",
    "#precedent = pd.read_csv('corpus_en_de_notes.csv',index_col=0)\n",
    "#precedent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Sklearn Pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "anglais = pd.read_csv('corpus_en_notes.csv',index_col=0)\n",
    "# anglais2 = pd.read_csv('corpus_en_de_notes.csv',index_col=0)\n",
    "# anglais = pd.concat([anglais,anglais2],axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention certains textes ne sont pas fournies et donc mis en \"Error\" : A supprimer donc\n",
    "# On pourrait éventuellement tester en ne prenant plus les meth similarités ds les predicteurs\n",
    "anglais = anglais[anglais.meth1_similarites!='Error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "anglais = anglais.round({'Geography':0, 'Entities':0,'Time':0, 'Narrative':0, 'Overall':0, 'Style':0, 'Tone':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "partiel = anglais[['Geography', 'Entities','Time', 'Narrative', 'Overall', 'Style', 'Tone']].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "anglais = pd.concat([anglais[['ligne', 'title_1', 'title_2', 'text_1', 'text_2','summary1_text1', 'summary2_text1', 'summary1_text2', 'summary2_text2']],\n",
    "        partiel,anglais[['nb_entites_idem', 'nb_lieux_idem', 'nb_dates_idem', 'entites_idem','dates_idem', 'score_similarite_titres',\n",
    "       'score_similarite_resume1', 'score_similarite_resume2','score_classif1', 'score_classif2', 'score_sentiment1',\n",
    "       'score_sentiment2', 'score_sentiment3', 'meth1_similarites', 'meth2_similarites']]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ligne</th>\n",
       "      <th>title_1</th>\n",
       "      <th>title_2</th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>summary1_text1</th>\n",
       "      <th>summary2_text1</th>\n",
       "      <th>summary1_text2</th>\n",
       "      <th>summary2_text2</th>\n",
       "      <th>Geography</th>\n",
       "      <th>...</th>\n",
       "      <th>score_similarite_titres</th>\n",
       "      <th>score_similarite_resume1</th>\n",
       "      <th>score_similarite_resume2</th>\n",
       "      <th>score_classif1</th>\n",
       "      <th>score_classif2</th>\n",
       "      <th>score_sentiment1</th>\n",
       "      <th>score_sentiment2</th>\n",
       "      <th>score_sentiment3</th>\n",
       "      <th>meth1_similarites</th>\n",
       "      <th>meth2_similarites</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Virginia man arrested in fatal DUI crash in We...</td>\n",
       "      <td>Haiti’s leader marks independence day amid sec...</td>\n",
       "      <td>MARTINSBURG, W.Va. — A suspected drunken drive...</td>\n",
       "      <td>PORT-AU-PRINCE, Haiti — Haitian President Jove...</td>\n",
       "      <td>Cody Wade Braithwaite, 32, of Winchester, Vir...</td>\n",
       "      <td>All images are copyrighted.</td>\n",
       "      <td>New Year's Day marked by protests over lack o...</td>\n",
       "      <td>All images are copyrighted.</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.80</td>\n",
       "      <td>3.79</td>\n",
       "      <td>51.00</td>\n",
       "      <td>10.42</td>\n",
       "      <td>8.33</td>\n",
       "      <td>32.09</td>\n",
       "      <td>13.47</td>\n",
       "      <td>99.20</td>\n",
       "      <td>818.0</td>\n",
       "      <td>231.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Guyana: Three injured after car crashes into u...</td>\n",
       "      <td>Fire kills more than 30 animals at zoo in west...</td>\n",
       "      <td>Share This On:\\n\\nPin 11 Shares\\n\\n(NEWS ROOM ...</td>\n",
       "      <td>BERLIN - A fire at a zoo in western Germany in...</td>\n",
       "      <td>Motorcar PNN 7976 driven by 22-year-old Seera...</td>\n",
       "      <td>All images are copyrighted.</td>\n",
       "      <td>Fire at a zoo in western Germany in the first...</td>\n",
       "      <td>A fire at a zoo in western Germany in the firs...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>6.44</td>\n",
       "      <td>9.10</td>\n",
       "      <td>2.91</td>\n",
       "      <td>10.57</td>\n",
       "      <td>8.34</td>\n",
       "      <td>28.15</td>\n",
       "      <td>92.87</td>\n",
       "      <td>99.04</td>\n",
       "      <td>129.5</td>\n",
       "      <td>174.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Trump Brings In 2020 At Mar-a-Lago: ‘We’re Goi...</td>\n",
       "      <td>Trump says he does not expect war with Iran, ‘...</td>\n",
       "      <td>(Breitbart) – President Donald Trump welcomed ...</td>\n",
       "      <td>PALM BEACH, United States — US President Donal...</td>\n",
       "      <td>President Trump welcomed guests to Mar-a-Lago...</td>\n",
       "      <td>It’s a new year, but it’s also a new president.</td>\n",
       "      <td>U.S. President Donald Trump says he does not ...</td>\n",
       "      <td>US President Donald Trump says he does not for...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>20.70</td>\n",
       "      <td>27.45</td>\n",
       "      <td>9.68</td>\n",
       "      <td>13.14</td>\n",
       "      <td>8.33</td>\n",
       "      <td>25.00</td>\n",
       "      <td>2.56</td>\n",
       "      <td>96.75</td>\n",
       "      <td>827.2</td>\n",
       "      <td>504.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Zomato Buys Uber's Food Delivery Business in I...</td>\n",
       "      <td>Indian Online Food Delivery Market to Hit $8 B...</td>\n",
       "      <td>Uber has sold its online food-ordering busines...</td>\n",
       "      <td>Rapid digitisation and growth in both online b...</td>\n",
       "      <td>Uber has sold its online food-ordering busine...</td>\n",
       "      <td>It's food for thought for Uber.</td>\n",
       "      <td>India's online food industry to become an $8 ...</td>\n",
       "      <td>\"Ordering food online is now a habit.\"</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>24.97</td>\n",
       "      <td>27.90</td>\n",
       "      <td>10.34</td>\n",
       "      <td>18.21</td>\n",
       "      <td>8.35</td>\n",
       "      <td>23.79</td>\n",
       "      <td>93.55</td>\n",
       "      <td>94.72</td>\n",
       "      <td>620.0</td>\n",
       "      <td>735.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>India approves third moon mission, months afte...</td>\n",
       "      <td>India targets new moon mission in 2020</td>\n",
       "      <td>BENGALURU (Reuters) - India has approved its t...</td>\n",
       "      <td>BANGALORE: India plans to make a fresh attempt...</td>\n",
       "      <td>India has approved its third lunar mission mo...</td>\n",
       "      <td>All images are copyrighted.</td>\n",
       "      <td>Work is going \"smoothly\" on the Chandrayaan-3...</td>\n",
       "      <td>\"We are targeting the launch for this year but...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>29.21</td>\n",
       "      <td>32.40</td>\n",
       "      <td>-1.95</td>\n",
       "      <td>15.76</td>\n",
       "      <td>8.35</td>\n",
       "      <td>26.28</td>\n",
       "      <td>74.48</td>\n",
       "      <td>93.05</td>\n",
       "      <td>473.0</td>\n",
       "      <td>559.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>194</td>\n",
       "      <td>Authorities call on athletes to elevate Cuba i...</td>\n",
       "      <td>New Year, New Resolution-Tips to Help Achieve ...</td>\n",
       "      <td>Authorities call on athletes to elevate Cuba i...</td>\n",
       "      <td>New Years-fresh starts, clean slates and new r...</td>\n",
       "      <td>Cuba has 26 qualified athletes for the Tokyo-...</td>\n",
       "      <td>The president of the Cuban Olympic Committee a...</td>\n",
       "      <td>People set New Year’s Resolutions in the hope...</td>\n",
       "      <td>It’s that time of year again. Every first of J...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.78</td>\n",
       "      <td>4.27</td>\n",
       "      <td>10.28</td>\n",
       "      <td>8.92</td>\n",
       "      <td>8.29</td>\n",
       "      <td>19.18</td>\n",
       "      <td>90.36</td>\n",
       "      <td>5.42</td>\n",
       "      <td>51.9</td>\n",
       "      <td>185.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>195</td>\n",
       "      <td>In Chess Game With Iran, Trump Has Only Bad Op...</td>\n",
       "      <td>Iran-Iraq Snowball Puts U.S. Mideast Policy to...</td>\n",
       "      <td>WASHINGTON The Trump administration is facing ...</td>\n",
       "      <td>As the new civil year begins, an isolated even...</td>\n",
       "      <td>A day after the U.S. Embassy in Baghdad was s...</td>\n",
       "      <td>President Donald Trump's national security adv...</td>\n",
       "      <td>An isolated event in northern Iraq threatens ...</td>\n",
       "      <td>As the new civil year begins, an isolated even...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>25.11</td>\n",
       "      <td>28.36</td>\n",
       "      <td>22.68</td>\n",
       "      <td>20.06</td>\n",
       "      <td>8.37</td>\n",
       "      <td>21.05</td>\n",
       "      <td>17.43</td>\n",
       "      <td>99.57</td>\n",
       "      <td>196.9</td>\n",
       "      <td>735.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>196</td>\n",
       "      <td>The boom no one saw coming in 2019</td>\n",
       "      <td>Treasury yields rise on final trading day of 2019</td>\n",
       "      <td>Bloomberg opinion:\\n\\nIt's pretty much never a...</td>\n",
       "      <td>U.S. government debt prices moved lower on Tue...</td>\n",
       "      <td>Every key US bond market posted positive retu...</td>\n",
       "      <td>As the year draws to a close, here's a look ba...</td>\n",
       "      <td>The yield on the benchmark 10-year Treasury n...</td>\n",
       "      <td>All images are copyrighted.</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>7.94</td>\n",
       "      <td>29.86</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>32.86</td>\n",
       "      <td>8.35</td>\n",
       "      <td>26.32</td>\n",
       "      <td>5.13</td>\n",
       "      <td>95.33</td>\n",
       "      <td>654.8</td>\n",
       "      <td>1324.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>197</td>\n",
       "      <td>No cheer: Spurt in tomato, onion &amp; potato pric...</td>\n",
       "      <td>New Year shocker: Indian Railways hikes passen...</td>\n",
       "      <td>By: Akshta MishraLUCKNOW: Prices of tomato, on...</td>\n",
       "      <td>New Delhi: In a shock to millions of passenger...</td>\n",
       "      <td>Prices of tomato, onion and potato – staple v...</td>\n",
       "      <td>Tomato, onion and potato prices have gone up b...</td>\n",
       "      <td>Passenger fares for sleeper class in mail and...</td>\n",
       "      <td>In a shock to millions of passengers across th...</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>13.93</td>\n",
       "      <td>13.98</td>\n",
       "      <td>11.86</td>\n",
       "      <td>13.83</td>\n",
       "      <td>8.32</td>\n",
       "      <td>21.20</td>\n",
       "      <td>14.38</td>\n",
       "      <td>99.43</td>\n",
       "      <td>179.4</td>\n",
       "      <td>129.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>198</td>\n",
       "      <td>CAA Protest Chennai: Cops probing kolam protes...</td>\n",
       "      <td>In the anti citizenship law protests at Chenna...</td>\n",
       "      <td>Kolam in front of house against CAA, NRC (File...</td>\n",
       "      <td>In the anti citizenship law protests at Chenna...</td>\n",
       "      <td>Gayatri Kandhadai's Facebook profile says she...</td>\n",
       "      <td>Police have booked eight people for drawing ko...</td>\n",
       "      <td>Gayatri Khandhadai has been a part of a slew ...</td>\n",
       "      <td>A probe would be carried out to ascertain whet...</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>35.48</td>\n",
       "      <td>31.25</td>\n",
       "      <td>7.64</td>\n",
       "      <td>16.50</td>\n",
       "      <td>8.35</td>\n",
       "      <td>21.61</td>\n",
       "      <td>89.30</td>\n",
       "      <td>99.00</td>\n",
       "      <td>78.8</td>\n",
       "      <td>559.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ligne                                            title_1  \\\n",
       "0        0  Virginia man arrested in fatal DUI crash in We...   \n",
       "1        1  Guyana: Three injured after car crashes into u...   \n",
       "2        2  Trump Brings In 2020 At Mar-a-Lago: ‘We’re Goi...   \n",
       "3        3  Zomato Buys Uber's Food Delivery Business in I...   \n",
       "4        4  India approves third moon mission, months afte...   \n",
       "..     ...                                                ...   \n",
       "193    194  Authorities call on athletes to elevate Cuba i...   \n",
       "194    195  In Chess Game With Iran, Trump Has Only Bad Op...   \n",
       "195    196                 The boom no one saw coming in 2019   \n",
       "196    197  No cheer: Spurt in tomato, onion & potato pric...   \n",
       "197    198  CAA Protest Chennai: Cops probing kolam protes...   \n",
       "\n",
       "                                               title_2  \\\n",
       "0    Haiti’s leader marks independence day amid sec...   \n",
       "1    Fire kills more than 30 animals at zoo in west...   \n",
       "2    Trump says he does not expect war with Iran, ‘...   \n",
       "3    Indian Online Food Delivery Market to Hit $8 B...   \n",
       "4               India targets new moon mission in 2020   \n",
       "..                                                 ...   \n",
       "193  New Year, New Resolution-Tips to Help Achieve ...   \n",
       "194  Iran-Iraq Snowball Puts U.S. Mideast Policy to...   \n",
       "195  Treasury yields rise on final trading day of 2019   \n",
       "196  New Year shocker: Indian Railways hikes passen...   \n",
       "197  In the anti citizenship law protests at Chenna...   \n",
       "\n",
       "                                                text_1  \\\n",
       "0    MARTINSBURG, W.Va. — A suspected drunken drive...   \n",
       "1    Share This On:\\n\\nPin 11 Shares\\n\\n(NEWS ROOM ...   \n",
       "2    (Breitbart) – President Donald Trump welcomed ...   \n",
       "3    Uber has sold its online food-ordering busines...   \n",
       "4    BENGALURU (Reuters) - India has approved its t...   \n",
       "..                                                 ...   \n",
       "193  Authorities call on athletes to elevate Cuba i...   \n",
       "194  WASHINGTON The Trump administration is facing ...   \n",
       "195  Bloomberg opinion:\\n\\nIt's pretty much never a...   \n",
       "196  By: Akshta MishraLUCKNOW: Prices of tomato, on...   \n",
       "197  Kolam in front of house against CAA, NRC (File...   \n",
       "\n",
       "                                                text_2  \\\n",
       "0    PORT-AU-PRINCE, Haiti — Haitian President Jove...   \n",
       "1    BERLIN - A fire at a zoo in western Germany in...   \n",
       "2    PALM BEACH, United States — US President Donal...   \n",
       "3    Rapid digitisation and growth in both online b...   \n",
       "4    BANGALORE: India plans to make a fresh attempt...   \n",
       "..                                                 ...   \n",
       "193  New Years-fresh starts, clean slates and new r...   \n",
       "194  As the new civil year begins, an isolated even...   \n",
       "195  U.S. government debt prices moved lower on Tue...   \n",
       "196  New Delhi: In a shock to millions of passenger...   \n",
       "197  In the anti citizenship law protests at Chenna...   \n",
       "\n",
       "                                        summary1_text1  \\\n",
       "0     Cody Wade Braithwaite, 32, of Winchester, Vir...   \n",
       "1     Motorcar PNN 7976 driven by 22-year-old Seera...   \n",
       "2     President Trump welcomed guests to Mar-a-Lago...   \n",
       "3     Uber has sold its online food-ordering busine...   \n",
       "4     India has approved its third lunar mission mo...   \n",
       "..                                                 ...   \n",
       "193   Cuba has 26 qualified athletes for the Tokyo-...   \n",
       "194   A day after the U.S. Embassy in Baghdad was s...   \n",
       "195   Every key US bond market posted positive retu...   \n",
       "196   Prices of tomato, onion and potato – staple v...   \n",
       "197   Gayatri Kandhadai's Facebook profile says she...   \n",
       "\n",
       "                                        summary2_text1  \\\n",
       "0                          All images are copyrighted.   \n",
       "1                          All images are copyrighted.   \n",
       "2      It’s a new year, but it’s also a new president.   \n",
       "3                      It's food for thought for Uber.   \n",
       "4                          All images are copyrighted.   \n",
       "..                                                 ...   \n",
       "193  The president of the Cuban Olympic Committee a...   \n",
       "194  President Donald Trump's national security adv...   \n",
       "195  As the year draws to a close, here's a look ba...   \n",
       "196  Tomato, onion and potato prices have gone up b...   \n",
       "197  Police have booked eight people for drawing ko...   \n",
       "\n",
       "                                        summary1_text2  \\\n",
       "0     New Year's Day marked by protests over lack o...   \n",
       "1     Fire at a zoo in western Germany in the first...   \n",
       "2     U.S. President Donald Trump says he does not ...   \n",
       "3     India's online food industry to become an $8 ...   \n",
       "4     Work is going \"smoothly\" on the Chandrayaan-3...   \n",
       "..                                                 ...   \n",
       "193   People set New Year’s Resolutions in the hope...   \n",
       "194   An isolated event in northern Iraq threatens ...   \n",
       "195   The yield on the benchmark 10-year Treasury n...   \n",
       "196   Passenger fares for sleeper class in mail and...   \n",
       "197   Gayatri Khandhadai has been a part of a slew ...   \n",
       "\n",
       "                                        summary2_text2  Geography  ...  \\\n",
       "0                          All images are copyrighted.          4  ...   \n",
       "1    A fire at a zoo in western Germany in the firs...          4  ...   \n",
       "2    US President Donald Trump says he does not for...          1  ...   \n",
       "3               \"Ordering food online is now a habit.\"          1  ...   \n",
       "4    \"We are targeting the launch for this year but...          1  ...   \n",
       "..                                                 ...        ...  ...   \n",
       "193  It’s that time of year again. Every first of J...          4  ...   \n",
       "194  As the new civil year begins, an isolated even...          1  ...   \n",
       "195                        All images are copyrighted.          1  ...   \n",
       "196  In a shock to millions of passengers across th...          2  ...   \n",
       "197  A probe would be carried out to ascertain whet...          2  ...   \n",
       "\n",
       "     score_similarite_titres  score_similarite_resume1  \\\n",
       "0                       0.80                      3.79   \n",
       "1                       6.44                      9.10   \n",
       "2                      20.70                     27.45   \n",
       "3                      24.97                     27.90   \n",
       "4                      29.21                     32.40   \n",
       "..                       ...                       ...   \n",
       "193                     1.78                      4.27   \n",
       "194                    25.11                     28.36   \n",
       "195                     7.94                     29.86   \n",
       "196                    13.93                     13.98   \n",
       "197                    35.48                     31.25   \n",
       "\n",
       "     score_similarite_resume2  score_classif1  score_classif2  \\\n",
       "0                       51.00           10.42            8.33   \n",
       "1                        2.91           10.57            8.34   \n",
       "2                        9.68           13.14            8.33   \n",
       "3                       10.34           18.21            8.35   \n",
       "4                       -1.95           15.76            8.35   \n",
       "..                        ...             ...             ...   \n",
       "193                     10.28            8.92            8.29   \n",
       "194                     22.68           20.06            8.37   \n",
       "195                     -1.50           32.86            8.35   \n",
       "196                     11.86           13.83            8.32   \n",
       "197                      7.64           16.50            8.35   \n",
       "\n",
       "     score_sentiment1  score_sentiment2  score_sentiment3  meth1_similarites  \\\n",
       "0               32.09             13.47             99.20              818.0   \n",
       "1               28.15             92.87             99.04              129.5   \n",
       "2               25.00              2.56             96.75              827.2   \n",
       "3               23.79             93.55             94.72              620.0   \n",
       "4               26.28             74.48             93.05              473.0   \n",
       "..                ...               ...               ...                ...   \n",
       "193             19.18             90.36              5.42               51.9   \n",
       "194             21.05             17.43             99.57              196.9   \n",
       "195             26.32              5.13             95.33              654.8   \n",
       "196             21.20             14.38             99.43              179.4   \n",
       "197             21.61             89.30             99.00               78.8   \n",
       "\n",
       "    meth2_similarites  \n",
       "0               231.9  \n",
       "1               174.9  \n",
       "2               504.8  \n",
       "3               735.1  \n",
       "4               559.4  \n",
       "..                ...  \n",
       "193             185.6  \n",
       "194             735.7  \n",
       "195            1324.8  \n",
       "196             129.4  \n",
       "197             559.6  \n",
       "\n",
       "[191 rows x 31 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anglais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "taille_train = 100\n",
    "dernier_test = 191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicteurs = ['nb_entites_idem', 'nb_lieux_idem', 'nb_dates_idem', 'score_similarite_titres', 'score_similarite_resume1',\n",
    "    'score_similarite_resume2', 'score_classif1', 'score_classif2','score_sentiment1', \n",
    "    'score_sentiment2', 'score_sentiment3', 'meth1_similarites','meth2_similarites']\n",
    "# 2e test sans les predicteurs entites et méthodes similarités\n",
    "# predicteurs1 = ['score_similarite_titres','score_similarite_resume1','score_similarite_resume2','score_classif1','score_classif2',\n",
    "#            'score_sentiment1','score_sentiment2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "english_classif = setup(data = anglais[predicteurs + ['Overall']],  target = 'Overall', html=False, silent=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7143</td>\n",
       "      <td>0.8159</td>\n",
       "      <td>0.6726</td>\n",
       "      <td>0.7024</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.5725</td>\n",
       "      <td>0.5769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.8380</td>\n",
       "      <td>0.3393</td>\n",
       "      <td>0.4226</td>\n",
       "      <td>0.4571</td>\n",
       "      <td>0.2222</td>\n",
       "      <td>0.2261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5714</td>\n",
       "      <td>0.8984</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.4214</td>\n",
       "      <td>0.4832</td>\n",
       "      <td>0.2759</td>\n",
       "      <td>0.2992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6154</td>\n",
       "      <td>0.8729</td>\n",
       "      <td>0.4583</td>\n",
       "      <td>0.5623</td>\n",
       "      <td>0.5799</td>\n",
       "      <td>0.4248</td>\n",
       "      <td>0.4329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.7932</td>\n",
       "      <td>0.2917</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.2174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5385</td>\n",
       "      <td>0.8749</td>\n",
       "      <td>0.4167</td>\n",
       "      <td>0.4808</td>\n",
       "      <td>0.4989</td>\n",
       "      <td>0.2844</td>\n",
       "      <td>0.2938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.5385</td>\n",
       "      <td>0.8266</td>\n",
       "      <td>0.4167</td>\n",
       "      <td>0.6282</td>\n",
       "      <td>0.5734</td>\n",
       "      <td>0.3554</td>\n",
       "      <td>0.3615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.5385</td>\n",
       "      <td>0.7526</td>\n",
       "      <td>0.4167</td>\n",
       "      <td>0.4835</td>\n",
       "      <td>0.5089</td>\n",
       "      <td>0.3036</td>\n",
       "      <td>0.3066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.6923</td>\n",
       "      <td>0.8174</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.5192</td>\n",
       "      <td>0.5934</td>\n",
       "      <td>0.5185</td>\n",
       "      <td>0.5591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.6923</td>\n",
       "      <td>0.8907</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.7399</td>\n",
       "      <td>0.6704</td>\n",
       "      <td>0.5439</td>\n",
       "      <td>0.5591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.5863</td>\n",
       "      <td>0.8381</td>\n",
       "      <td>0.4554</td>\n",
       "      <td>0.5422</td>\n",
       "      <td>0.5527</td>\n",
       "      <td>0.3717</td>\n",
       "      <td>0.3833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SD</th>\n",
       "      <td>0.0835</td>\n",
       "      <td>0.0440</td>\n",
       "      <td>0.1204</td>\n",
       "      <td>0.1076</td>\n",
       "      <td>0.0809</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.1325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
       "0       0.7143  0.8159  0.6726  0.7024  0.7000  0.5725  0.5769\n",
       "1       0.5000  0.8380  0.3393  0.4226  0.4571  0.2222  0.2261\n",
       "2       0.5714  0.8984  0.3750  0.4214  0.4832  0.2759  0.2992\n",
       "3       0.6154  0.8729  0.4583  0.5623  0.5799  0.4248  0.4329\n",
       "4       0.4615  0.7932  0.2917  0.4615  0.4615  0.2155  0.2174\n",
       "5       0.5385  0.8749  0.4167  0.4808  0.4989  0.2844  0.2938\n",
       "6       0.5385  0.8266  0.4167  0.6282  0.5734  0.3554  0.3615\n",
       "7       0.5385  0.7526  0.4167  0.4835  0.5089  0.3036  0.3066\n",
       "8       0.6923  0.8174  0.5000  0.5192  0.5934  0.5185  0.5591\n",
       "9       0.6923  0.8907  0.6667  0.7399  0.6704  0.5439  0.5591\n",
       "Mean    0.5863  0.8381  0.4554  0.5422  0.5527  0.3717  0.3833\n",
       "SD      0.0835  0.0440  0.1204  0.1076  0.0809  0.1276  0.1325"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6429</td>\n",
       "      <td>0.8402</td>\n",
       "      <td>0.5476</td>\n",
       "      <td>0.5952</td>\n",
       "      <td>0.6143</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.4651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6429</td>\n",
       "      <td>0.8340</td>\n",
       "      <td>0.5476</td>\n",
       "      <td>0.7083</td>\n",
       "      <td>0.6357</td>\n",
       "      <td>0.4488</td>\n",
       "      <td>0.4602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.7857</td>\n",
       "      <td>0.8661</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.7737</td>\n",
       "      <td>0.6379</td>\n",
       "      <td>0.6841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7692</td>\n",
       "      <td>0.9366</td>\n",
       "      <td>0.7083</td>\n",
       "      <td>0.7179</td>\n",
       "      <td>0.7385</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.6696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5385</td>\n",
       "      <td>0.8271</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.4879</td>\n",
       "      <td>0.2844</td>\n",
       "      <td>0.2938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.6154</td>\n",
       "      <td>0.8736</td>\n",
       "      <td>0.4167</td>\n",
       "      <td>0.4385</td>\n",
       "      <td>0.5110</td>\n",
       "      <td>0.3868</td>\n",
       "      <td>0.4256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.6923</td>\n",
       "      <td>0.9189</td>\n",
       "      <td>0.5833</td>\n",
       "      <td>0.6795</td>\n",
       "      <td>0.6341</td>\n",
       "      <td>0.5273</td>\n",
       "      <td>0.5554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.6154</td>\n",
       "      <td>0.8122</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.5769</td>\n",
       "      <td>0.5934</td>\n",
       "      <td>0.4348</td>\n",
       "      <td>0.4387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.6923</td>\n",
       "      <td>0.9321</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.4846</td>\n",
       "      <td>0.5687</td>\n",
       "      <td>0.5094</td>\n",
       "      <td>0.5655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.6923</td>\n",
       "      <td>0.8794</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.7399</td>\n",
       "      <td>0.6704</td>\n",
       "      <td>0.5439</td>\n",
       "      <td>0.5591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.6687</td>\n",
       "      <td>0.8720</td>\n",
       "      <td>0.5470</td>\n",
       "      <td>0.6252</td>\n",
       "      <td>0.6228</td>\n",
       "      <td>0.4899</td>\n",
       "      <td>0.5117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SD</th>\n",
       "      <td>0.0703</td>\n",
       "      <td>0.0426</td>\n",
       "      <td>0.1109</td>\n",
       "      <td>0.1292</td>\n",
       "      <td>0.0856</td>\n",
       "      <td>0.1073</td>\n",
       "      <td>0.1126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
       "0       0.6429  0.8402  0.5476  0.5952  0.6143  0.4615  0.4651\n",
       "1       0.6429  0.8340  0.5476  0.7083  0.6357  0.4488  0.4602\n",
       "2       0.7857  0.8661  0.6667  0.8500  0.7737  0.6379  0.6841\n",
       "3       0.7692  0.9366  0.7083  0.7179  0.7385  0.6638  0.6696\n",
       "4       0.5385  0.8271  0.3333  0.4615  0.4879  0.2844  0.2938\n",
       "5       0.6154  0.8736  0.4167  0.4385  0.5110  0.3868  0.4256\n",
       "6       0.6923  0.9189  0.5833  0.6795  0.6341  0.5273  0.5554\n",
       "7       0.6154  0.8122  0.5000  0.5769  0.5934  0.4348  0.4387\n",
       "8       0.6923  0.9321  0.5000  0.4846  0.5687  0.5094  0.5655\n",
       "9       0.6923  0.8794  0.6667  0.7399  0.6704  0.5439  0.5591\n",
       "Mean    0.6687  0.8720  0.5470  0.6252  0.6228  0.4899  0.5117\n",
       "SD      0.0703  0.0426  0.1109  0.1292  0.0856  0.1073  0.1126"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6429</td>\n",
       "      <td>0.7236</td>\n",
       "      <td>0.6845</td>\n",
       "      <td>0.7238</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.4928</td>\n",
       "      <td>0.5114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5714</td>\n",
       "      <td>0.8405</td>\n",
       "      <td>0.4702</td>\n",
       "      <td>0.5952</td>\n",
       "      <td>0.5751</td>\n",
       "      <td>0.3778</td>\n",
       "      <td>0.3864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5714</td>\n",
       "      <td>0.8143</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.4253</td>\n",
       "      <td>0.4746</td>\n",
       "      <td>0.2432</td>\n",
       "      <td>0.2830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6923</td>\n",
       "      <td>0.8681</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.6630</td>\n",
       "      <td>0.6627</td>\n",
       "      <td>0.5398</td>\n",
       "      <td>0.5501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.7914</td>\n",
       "      <td>0.2917</td>\n",
       "      <td>0.4451</td>\n",
       "      <td>0.4473</td>\n",
       "      <td>0.1947</td>\n",
       "      <td>0.1966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.6154</td>\n",
       "      <td>0.8597</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.5897</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.4397</td>\n",
       "      <td>0.4435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.6923</td>\n",
       "      <td>0.9063</td>\n",
       "      <td>0.5833</td>\n",
       "      <td>0.6136</td>\n",
       "      <td>0.6414</td>\n",
       "      <td>0.5398</td>\n",
       "      <td>0.5501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.5385</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>0.4583</td>\n",
       "      <td>0.5128</td>\n",
       "      <td>0.5231</td>\n",
       "      <td>0.3276</td>\n",
       "      <td>0.3304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.6154</td>\n",
       "      <td>0.8832</td>\n",
       "      <td>0.4167</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.5275</td>\n",
       "      <td>0.3981</td>\n",
       "      <td>0.4293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.6154</td>\n",
       "      <td>0.8930</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.6026</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.4444</td>\n",
       "      <td>0.4483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.6016</td>\n",
       "      <td>0.8298</td>\n",
       "      <td>0.4988</td>\n",
       "      <td>0.5633</td>\n",
       "      <td>0.5702</td>\n",
       "      <td>0.3998</td>\n",
       "      <td>0.4129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SD</th>\n",
       "      <td>0.0664</td>\n",
       "      <td>0.0638</td>\n",
       "      <td>0.1237</td>\n",
       "      <td>0.0935</td>\n",
       "      <td>0.0708</td>\n",
       "      <td>0.1112</td>\n",
       "      <td>0.1100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
       "0       0.6429  0.7236  0.6845  0.7238  0.6500  0.4928  0.5114\n",
       "1       0.5714  0.8405  0.4702  0.5952  0.5751  0.3778  0.3864\n",
       "2       0.5714  0.8143  0.3333  0.4253  0.4746  0.2432  0.2830\n",
       "3       0.6923  0.8681  0.6250  0.6630  0.6627  0.5398  0.5501\n",
       "4       0.4615  0.7914  0.2917  0.4451  0.4473  0.1947  0.1966\n",
       "5       0.6154  0.8597  0.5000  0.5897  0.6000  0.4397  0.4435\n",
       "6       0.6923  0.9063  0.5833  0.6136  0.6414  0.5398  0.5501\n",
       "7       0.5385  0.7178  0.4583  0.5128  0.5231  0.3276  0.3304\n",
       "8       0.6154  0.8832  0.4167  0.4615  0.5275  0.3981  0.4293\n",
       "9       0.6154  0.8930  0.6250  0.6026  0.6000  0.4444  0.4483\n",
       "Mean    0.6016  0.8298  0.4988  0.5633  0.5702  0.3998  0.4129\n",
       "SD      0.0664  0.0638  0.1237  0.0935  0.0708  0.1112  0.1100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.7809</td>\n",
       "      <td>0.4405</td>\n",
       "      <td>0.5357</td>\n",
       "      <td>0.4530</td>\n",
       "      <td>0.2222</td>\n",
       "      <td>0.2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.6803</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.4762</td>\n",
       "      <td>0.4857</td>\n",
       "      <td>0.2519</td>\n",
       "      <td>0.2538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5714</td>\n",
       "      <td>0.7391</td>\n",
       "      <td>0.4702</td>\n",
       "      <td>0.5595</td>\n",
       "      <td>0.5513</td>\n",
       "      <td>0.3636</td>\n",
       "      <td>0.3843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5385</td>\n",
       "      <td>0.7965</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.5615</td>\n",
       "      <td>0.5269</td>\n",
       "      <td>0.3607</td>\n",
       "      <td>0.3729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5385</td>\n",
       "      <td>0.5809</td>\n",
       "      <td>0.4167</td>\n",
       "      <td>0.4963</td>\n",
       "      <td>0.5089</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.3156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.7943</td>\n",
       "      <td>0.5417</td>\n",
       "      <td>0.7692</td>\n",
       "      <td>0.4872</td>\n",
       "      <td>0.3158</td>\n",
       "      <td>0.3980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.6923</td>\n",
       "      <td>0.6948</td>\n",
       "      <td>0.5833</td>\n",
       "      <td>0.5751</td>\n",
       "      <td>0.6260</td>\n",
       "      <td>0.5357</td>\n",
       "      <td>0.5516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.5385</td>\n",
       "      <td>0.7050</td>\n",
       "      <td>0.5417</td>\n",
       "      <td>0.5103</td>\n",
       "      <td>0.5100</td>\n",
       "      <td>0.3607</td>\n",
       "      <td>0.3761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.6923</td>\n",
       "      <td>0.8780</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.5385</td>\n",
       "      <td>0.6007</td>\n",
       "      <td>0.5273</td>\n",
       "      <td>0.5663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.6154</td>\n",
       "      <td>0.7981</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.6026</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.4444</td>\n",
       "      <td>0.4483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.5577</td>\n",
       "      <td>0.7448</td>\n",
       "      <td>0.5214</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.5350</td>\n",
       "      <td>0.3692</td>\n",
       "      <td>0.3901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SD</th>\n",
       "      <td>0.0837</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.0811</td>\n",
       "      <td>0.0777</td>\n",
       "      <td>0.0547</td>\n",
       "      <td>0.1002</td>\n",
       "      <td>0.1047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
       "0       0.4286  0.7809  0.4405  0.5357  0.4530  0.2222  0.2339\n",
       "1       0.5000  0.6803  0.4286  0.4762  0.4857  0.2519  0.2538\n",
       "2       0.5714  0.7391  0.4702  0.5595  0.5513  0.3636  0.3843\n",
       "3       0.5385  0.7965  0.5000  0.5615  0.5269  0.3607  0.3729\n",
       "4       0.5385  0.5809  0.4167  0.4963  0.5089  0.3097  0.3156\n",
       "5       0.4615  0.7943  0.5417  0.7692  0.4872  0.3158  0.3980\n",
       "6       0.6923  0.6948  0.5833  0.5751  0.6260  0.5357  0.5516\n",
       "7       0.5385  0.7050  0.5417  0.5103  0.5100  0.3607  0.3761\n",
       "8       0.6923  0.8780  0.6667  0.5385  0.6007  0.5273  0.5663\n",
       "9       0.6154  0.7981  0.6250  0.6026  0.6000  0.4444  0.4483\n",
       "Mean    0.5577  0.7448  0.5214  0.5625  0.5350  0.3692  0.3901\n",
       "SD      0.0837  0.0788  0.0811  0.0777  0.0547  0.1002  0.1047"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2143</td>\n",
       "      <td>0.6057</td>\n",
       "      <td>0.2024</td>\n",
       "      <td>0.2976</td>\n",
       "      <td>0.1825</td>\n",
       "      <td>-0.0336</td>\n",
       "      <td>-0.0426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.7278</td>\n",
       "      <td>0.3988</td>\n",
       "      <td>0.6088</td>\n",
       "      <td>0.4429</td>\n",
       "      <td>0.2329</td>\n",
       "      <td>0.2636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.8198</td>\n",
       "      <td>0.3929</td>\n",
       "      <td>0.4804</td>\n",
       "      <td>0.4502</td>\n",
       "      <td>0.2741</td>\n",
       "      <td>0.3152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6154</td>\n",
       "      <td>0.8519</td>\n",
       "      <td>0.4583</td>\n",
       "      <td>0.5128</td>\n",
       "      <td>0.5495</td>\n",
       "      <td>0.4037</td>\n",
       "      <td>0.4259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.3846</td>\n",
       "      <td>0.7732</td>\n",
       "      <td>0.4167</td>\n",
       "      <td>0.6410</td>\n",
       "      <td>0.4231</td>\n",
       "      <td>0.2121</td>\n",
       "      <td>0.2414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.8224</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.6077</td>\n",
       "      <td>0.4835</td>\n",
       "      <td>0.2835</td>\n",
       "      <td>0.3077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.3846</td>\n",
       "      <td>0.8665</td>\n",
       "      <td>0.4583</td>\n",
       "      <td>0.5769</td>\n",
       "      <td>0.3692</td>\n",
       "      <td>0.2239</td>\n",
       "      <td>0.2843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.3846</td>\n",
       "      <td>0.7252</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.4308</td>\n",
       "      <td>0.3645</td>\n",
       "      <td>0.1746</td>\n",
       "      <td>0.1948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.6154</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.8297</td>\n",
       "      <td>0.6051</td>\n",
       "      <td>0.4922</td>\n",
       "      <td>0.5678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.6154</td>\n",
       "      <td>0.8618</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.7846</td>\n",
       "      <td>0.6026</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.5509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.4604</td>\n",
       "      <td>0.7954</td>\n",
       "      <td>0.4702</td>\n",
       "      <td>0.5770</td>\n",
       "      <td>0.4473</td>\n",
       "      <td>0.2763</td>\n",
       "      <td>0.3109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SD</th>\n",
       "      <td>0.1235</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.1589</td>\n",
       "      <td>0.1503</td>\n",
       "      <td>0.1202</td>\n",
       "      <td>0.1510</td>\n",
       "      <td>0.1683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
       "0       0.2143  0.6057  0.2024  0.2976  0.1825 -0.0336 -0.0426\n",
       "1       0.4286  0.7278  0.3988  0.6088  0.4429  0.2329  0.2636\n",
       "2       0.5000  0.8198  0.3929  0.4804  0.4502  0.2741  0.3152\n",
       "3       0.6154  0.8519  0.4583  0.5128  0.5495  0.4037  0.4259\n",
       "4       0.3846  0.7732  0.4167  0.6410  0.4231  0.2121  0.2414\n",
       "5       0.4615  0.8224  0.5000  0.6077  0.4835  0.2835  0.3077\n",
       "6       0.3846  0.8665  0.4583  0.5769  0.3692  0.2239  0.2843\n",
       "7       0.3846  0.7252  0.3750  0.4308  0.3645  0.1746  0.1948\n",
       "8       0.6154  0.9000  0.7500  0.8297  0.6051  0.4922  0.5678\n",
       "9       0.6154  0.8618  0.7500  0.7846  0.6026  0.5000  0.5509\n",
       "Mean    0.4604  0.7954  0.4702  0.5770  0.4473  0.2763  0.3109\n",
       "SD      0.1235  0.0842  0.1589  0.1503  0.1202  0.1510  0.1683"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6429</td>\n",
       "      <td>0.8247</td>\n",
       "      <td>0.5952</td>\n",
       "      <td>0.5595</td>\n",
       "      <td>0.5939</td>\n",
       "      <td>0.4574</td>\n",
       "      <td>0.4685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.8123</td>\n",
       "      <td>0.3869</td>\n",
       "      <td>0.5238</td>\n",
       "      <td>0.5037</td>\n",
       "      <td>0.2741</td>\n",
       "      <td>0.2803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6429</td>\n",
       "      <td>0.8491</td>\n",
       "      <td>0.4167</td>\n",
       "      <td>0.4929</td>\n",
       "      <td>0.5546</td>\n",
       "      <td>0.3913</td>\n",
       "      <td>0.4256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6923</td>\n",
       "      <td>0.8123</td>\n",
       "      <td>0.5833</td>\n",
       "      <td>0.6410</td>\n",
       "      <td>0.6077</td>\n",
       "      <td>0.5094</td>\n",
       "      <td>0.5677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6154</td>\n",
       "      <td>0.9056</td>\n",
       "      <td>0.4167</td>\n",
       "      <td>0.5538</td>\n",
       "      <td>0.5769</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.4461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.6923</td>\n",
       "      <td>0.8493</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.7436</td>\n",
       "      <td>0.7026</td>\n",
       "      <td>0.5517</td>\n",
       "      <td>0.5565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.5385</td>\n",
       "      <td>0.8094</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.6410</td>\n",
       "      <td>0.5462</td>\n",
       "      <td>0.3390</td>\n",
       "      <td>0.3478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.3846</td>\n",
       "      <td>0.7064</td>\n",
       "      <td>0.2917</td>\n",
       "      <td>0.2821</td>\n",
       "      <td>0.3253</td>\n",
       "      <td>0.0545</td>\n",
       "      <td>0.0575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.6154</td>\n",
       "      <td>0.8012</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.7015</td>\n",
       "      <td>0.6012</td>\n",
       "      <td>0.4348</td>\n",
       "      <td>0.4548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.7692</td>\n",
       "      <td>0.8266</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.8462</td>\n",
       "      <td>0.7744</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.6843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.6093</td>\n",
       "      <td>0.8197</td>\n",
       "      <td>0.5190</td>\n",
       "      <td>0.5985</td>\n",
       "      <td>0.5786</td>\n",
       "      <td>0.4109</td>\n",
       "      <td>0.4289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SD</th>\n",
       "      <td>0.1045</td>\n",
       "      <td>0.0475</td>\n",
       "      <td>0.1327</td>\n",
       "      <td>0.1473</td>\n",
       "      <td>0.1126</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>0.1643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
       "0       0.6429  0.8247  0.5952  0.5595  0.5939  0.4574  0.4685\n",
       "1       0.5000  0.8123  0.3869  0.5238  0.5037  0.2741  0.2803\n",
       "2       0.6429  0.8491  0.4167  0.4929  0.5546  0.3913  0.4256\n",
       "3       0.6923  0.8123  0.5833  0.6410  0.6077  0.5094  0.5677\n",
       "4       0.6154  0.9056  0.4167  0.5538  0.5769  0.4298  0.4461\n",
       "5       0.6923  0.8493  0.6250  0.7436  0.7026  0.5517  0.5565\n",
       "6       0.5385  0.8094  0.5000  0.6410  0.5462  0.3390  0.3478\n",
       "7       0.3846  0.7064  0.2917  0.2821  0.3253  0.0545  0.0575\n",
       "8       0.6154  0.8012  0.6250  0.7015  0.6012  0.4348  0.4548\n",
       "9       0.7692  0.8266  0.7500  0.8462  0.7744  0.6667  0.6843\n",
       "Mean    0.6093  0.8197  0.5190  0.5985  0.5786  0.4109  0.4289\n",
       "SD      0.1045  0.0475  0.1327  0.1473  0.1126  0.1579  0.1643"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9286</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9643</td>\n",
       "      <td>0.9464</td>\n",
       "      <td>0.9309</td>\n",
       "      <td>0.8955</td>\n",
       "      <td>0.9025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6429</td>\n",
       "      <td>0.8121</td>\n",
       "      <td>0.5952</td>\n",
       "      <td>0.7262</td>\n",
       "      <td>0.6677</td>\n",
       "      <td>0.4964</td>\n",
       "      <td>0.5078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5714</td>\n",
       "      <td>0.9027</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.4214</td>\n",
       "      <td>0.4832</td>\n",
       "      <td>0.2696</td>\n",
       "      <td>0.2932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6154</td>\n",
       "      <td>0.9016</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.5513</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.4348</td>\n",
       "      <td>0.4467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.7922</td>\n",
       "      <td>0.2917</td>\n",
       "      <td>0.3654</td>\n",
       "      <td>0.4066</td>\n",
       "      <td>0.1574</td>\n",
       "      <td>0.1628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.6154</td>\n",
       "      <td>0.8646</td>\n",
       "      <td>0.5417</td>\n",
       "      <td>0.6502</td>\n",
       "      <td>0.6114</td>\n",
       "      <td>0.4248</td>\n",
       "      <td>0.4329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.6154</td>\n",
       "      <td>0.8657</td>\n",
       "      <td>0.4583</td>\n",
       "      <td>0.5879</td>\n",
       "      <td>0.5953</td>\n",
       "      <td>0.4248</td>\n",
       "      <td>0.4288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.6154</td>\n",
       "      <td>0.7740</td>\n",
       "      <td>0.5833</td>\n",
       "      <td>0.5092</td>\n",
       "      <td>0.5550</td>\n",
       "      <td>0.4298</td>\n",
       "      <td>0.4419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.6154</td>\n",
       "      <td>0.7460</td>\n",
       "      <td>0.4583</td>\n",
       "      <td>0.5989</td>\n",
       "      <td>0.6012</td>\n",
       "      <td>0.4144</td>\n",
       "      <td>0.4185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.8263</td>\n",
       "      <td>0.4167</td>\n",
       "      <td>0.4176</td>\n",
       "      <td>0.4379</td>\n",
       "      <td>0.1947</td>\n",
       "      <td>0.1966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.6143</td>\n",
       "      <td>0.8485</td>\n",
       "      <td>0.5143</td>\n",
       "      <td>0.5774</td>\n",
       "      <td>0.5867</td>\n",
       "      <td>0.4142</td>\n",
       "      <td>0.4232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SD</th>\n",
       "      <td>0.1217</td>\n",
       "      <td>0.0709</td>\n",
       "      <td>0.1769</td>\n",
       "      <td>0.1626</td>\n",
       "      <td>0.1387</td>\n",
       "      <td>0.1935</td>\n",
       "      <td>0.1934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
       "0       0.9286  1.0000  0.9643  0.9464  0.9309  0.8955  0.9025\n",
       "1       0.6429  0.8121  0.5952  0.7262  0.6677  0.4964  0.5078\n",
       "2       0.5714  0.9027  0.3333  0.4214  0.4832  0.2696  0.2932\n",
       "3       0.6154  0.9016  0.5000  0.5513  0.5780  0.4348  0.4467\n",
       "4       0.4615  0.7922  0.2917  0.3654  0.4066  0.1574  0.1628\n",
       "5       0.6154  0.8646  0.5417  0.6502  0.6114  0.4248  0.4329\n",
       "6       0.6154  0.8657  0.4583  0.5879  0.5953  0.4248  0.4288\n",
       "7       0.6154  0.7740  0.5833  0.5092  0.5550  0.4298  0.4419\n",
       "8       0.6154  0.7460  0.4583  0.5989  0.6012  0.4144  0.4185\n",
       "9       0.4615  0.8263  0.4167  0.4176  0.4379  0.1947  0.1966\n",
       "Mean    0.6143  0.8485  0.5143  0.5774  0.5867  0.4142  0.4232\n",
       "SD      0.1217  0.0709  0.1769  0.1626  0.1387  0.1935  0.1934"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.5643</td>\n",
       "      <td>0.5784</td>\n",
       "      <td>0.4017</td>\n",
       "      <td>0.4345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5536</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.5527</td>\n",
       "      <td>0.4126</td>\n",
       "      <td>0.4538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3036</td>\n",
       "      <td>0.4405</td>\n",
       "      <td>0.4203</td>\n",
       "      <td>0.1765</td>\n",
       "      <td>0.1922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.4077</td>\n",
       "      <td>0.4835</td>\n",
       "      <td>0.4144</td>\n",
       "      <td>0.4775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4583</td>\n",
       "      <td>0.5769</td>\n",
       "      <td>0.5897</td>\n",
       "      <td>0.4397</td>\n",
       "      <td>0.4556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.3846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.5192</td>\n",
       "      <td>0.4103</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.2276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.6154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.4571</td>\n",
       "      <td>0.5139</td>\n",
       "      <td>0.4348</td>\n",
       "      <td>0.4788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.3077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.2989</td>\n",
       "      <td>0.2859</td>\n",
       "      <td>0.0410</td>\n",
       "      <td>0.0479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.6154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4167</td>\n",
       "      <td>0.4825</td>\n",
       "      <td>0.5104</td>\n",
       "      <td>0.3299</td>\n",
       "      <td>0.4518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4167</td>\n",
       "      <td>0.5577</td>\n",
       "      <td>0.4634</td>\n",
       "      <td>0.2353</td>\n",
       "      <td>0.2771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.5258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4274</td>\n",
       "      <td>0.4905</td>\n",
       "      <td>0.4809</td>\n",
       "      <td>0.3067</td>\n",
       "      <td>0.3497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SD</th>\n",
       "      <td>0.1136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0908</td>\n",
       "      <td>0.0880</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.1322</td>\n",
       "      <td>0.1445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Accuracy  AUC  Recall   Prec.      F1   Kappa     MCC\n",
       "0       0.6429  0.0  0.5000  0.5643  0.5784  0.4017  0.4345\n",
       "1       0.5714  0.0  0.5536  0.6000  0.5527  0.4126  0.4538\n",
       "2       0.4286  0.0  0.3036  0.4405  0.4203  0.1765  0.1922\n",
       "3       0.6154  0.0  0.5000  0.4077  0.4835  0.4144  0.4775\n",
       "4       0.6154  0.0  0.4583  0.5769  0.5897  0.4397  0.4556\n",
       "5       0.3846  0.0  0.3750  0.5192  0.4103  0.1811  0.2276\n",
       "6       0.6154  0.0  0.5000  0.4571  0.5139  0.4348  0.4788\n",
       "7       0.3077  0.0  0.2500  0.2989  0.2859  0.0410  0.0479\n",
       "8       0.6154  0.0  0.4167  0.4825  0.5104  0.3299  0.4518\n",
       "9       0.4615  0.0  0.4167  0.5577  0.4634  0.2353  0.2771\n",
       "Mean    0.5258  0.0  0.4274  0.4905  0.4809  0.3067  0.3497\n",
       "SD      0.1136  0.0  0.0908  0.0880  0.0869  0.1322  0.1445"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.8295</td>\n",
       "      <td>0.2143</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.3158</td>\n",
       "      <td>-0.0467</td>\n",
       "      <td>-0.0620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6429</td>\n",
       "      <td>0.8566</td>\n",
       "      <td>0.4583</td>\n",
       "      <td>0.6039</td>\n",
       "      <td>0.5675</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.4403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5714</td>\n",
       "      <td>0.8827</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.4835</td>\n",
       "      <td>0.4571</td>\n",
       "      <td>0.1765</td>\n",
       "      <td>0.3096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3846</td>\n",
       "      <td>0.7422</td>\n",
       "      <td>0.2083</td>\n",
       "      <td>0.1923</td>\n",
       "      <td>0.2564</td>\n",
       "      <td>-0.1064</td>\n",
       "      <td>-0.1895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.7169</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.2130</td>\n",
       "      <td>0.2915</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.8267</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.2130</td>\n",
       "      <td>0.2915</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.7724</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.2308</td>\n",
       "      <td>0.3077</td>\n",
       "      <td>0.0319</td>\n",
       "      <td>0.0569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.7445</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.2130</td>\n",
       "      <td>0.2915</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.7280</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.2130</td>\n",
       "      <td>0.2915</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.5385</td>\n",
       "      <td>0.7589</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.4825</td>\n",
       "      <td>0.4412</td>\n",
       "      <td>0.2041</td>\n",
       "      <td>0.2738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.4874</td>\n",
       "      <td>0.7858</td>\n",
       "      <td>0.2798</td>\n",
       "      <td>0.3095</td>\n",
       "      <td>0.3512</td>\n",
       "      <td>0.0634</td>\n",
       "      <td>0.0829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SD</th>\n",
       "      <td>0.0716</td>\n",
       "      <td>0.0553</td>\n",
       "      <td>0.0717</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>0.0962</td>\n",
       "      <td>0.1370</td>\n",
       "      <td>0.1841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
       "0       0.4286  0.8295  0.2143  0.2500  0.3158 -0.0467 -0.0620\n",
       "1       0.6429  0.8566  0.4583  0.6039  0.5675  0.3750  0.4403\n",
       "2       0.5714  0.8827  0.3333  0.4835  0.4571  0.1765  0.3096\n",
       "3       0.3846  0.7422  0.2083  0.1923  0.2564 -0.1064 -0.1895\n",
       "4       0.4615  0.7169  0.2500  0.2130  0.2915  0.0000  0.0000\n",
       "5       0.4615  0.8267  0.2500  0.2130  0.2915  0.0000  0.0000\n",
       "6       0.4615  0.7724  0.2500  0.2308  0.3077  0.0319  0.0569\n",
       "7       0.4615  0.7445  0.2500  0.2130  0.2915  0.0000  0.0000\n",
       "8       0.4615  0.7280  0.2500  0.2130  0.2915  0.0000  0.0000\n",
       "9       0.5385  0.7589  0.3333  0.4825  0.4412  0.2041  0.2738\n",
       "Mean    0.4874  0.7858  0.2798  0.3095  0.3512  0.0634  0.0829\n",
       "SD      0.0716  0.0553  0.0717  0.1441  0.0962  0.1370  0.1841"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1429</td>\n",
       "      <td>0.5250</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.1558</td>\n",
       "      <td>0.1172</td>\n",
       "      <td>-0.0633</td>\n",
       "      <td>-0.1048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2143</td>\n",
       "      <td>0.6159</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.1706</td>\n",
       "      <td>0.1688</td>\n",
       "      <td>0.0723</td>\n",
       "      <td>0.1042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1429</td>\n",
       "      <td>0.6446</td>\n",
       "      <td>0.2083</td>\n",
       "      <td>0.0679</td>\n",
       "      <td>0.0850</td>\n",
       "      <td>-0.0244</td>\n",
       "      <td>-0.0392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0769</td>\n",
       "      <td>0.5603</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.0154</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>-0.0909</td>\n",
       "      <td>-0.1558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1538</td>\n",
       "      <td>0.6386</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0308</td>\n",
       "      <td>0.0513</td>\n",
       "      <td>-0.0288</td>\n",
       "      <td>-0.0464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0769</td>\n",
       "      <td>0.3640</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.0154</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>-0.0986</td>\n",
       "      <td>-0.1625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.2308</td>\n",
       "      <td>0.5490</td>\n",
       "      <td>0.2917</td>\n",
       "      <td>0.2615</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>0.0464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.2308</td>\n",
       "      <td>0.5309</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.3632</td>\n",
       "      <td>0.2357</td>\n",
       "      <td>0.0226</td>\n",
       "      <td>0.0311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.3077</td>\n",
       "      <td>0.6145</td>\n",
       "      <td>0.4583</td>\n",
       "      <td>0.7019</td>\n",
       "      <td>0.3336</td>\n",
       "      <td>0.1818</td>\n",
       "      <td>0.2485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0769</td>\n",
       "      <td>0.6264</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0237</td>\n",
       "      <td>-0.0909</td>\n",
       "      <td>-0.1820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.1654</td>\n",
       "      <td>0.5669</td>\n",
       "      <td>0.2417</td>\n",
       "      <td>0.1797</td>\n",
       "      <td>0.1233</td>\n",
       "      <td>-0.0090</td>\n",
       "      <td>-0.0260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SD</th>\n",
       "      <td>0.0747</td>\n",
       "      <td>0.0800</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.2073</td>\n",
       "      <td>0.0982</td>\n",
       "      <td>0.0840</td>\n",
       "      <td>0.1294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
       "0       0.1429  0.5250  0.2500  0.1558  0.1172 -0.0633 -0.1048\n",
       "1       0.2143  0.6159  0.3333  0.1706  0.1688  0.0723  0.1042\n",
       "2       0.1429  0.6446  0.2083  0.0679  0.0850 -0.0244 -0.0392\n",
       "3       0.0769  0.5603  0.1250  0.0154  0.0256 -0.0909 -0.1558\n",
       "4       0.1538  0.6386  0.2500  0.0308  0.0513 -0.0288 -0.0464\n",
       "5       0.0769  0.3640  0.1250  0.0154  0.0256 -0.0986 -0.1625\n",
       "6       0.2308  0.5490  0.2917  0.2615  0.1667  0.0299  0.0464\n",
       "7       0.2308  0.5309  0.2500  0.3632  0.2357  0.0226  0.0311\n",
       "8       0.3077  0.6145  0.4583  0.7019  0.3336  0.1818  0.2485\n",
       "9       0.0769  0.6264  0.1250  0.0140  0.0237 -0.0909 -0.1820\n",
       "Mean    0.1654  0.5669  0.2417  0.1797  0.1233 -0.0090 -0.0260\n",
       "SD      0.0747  0.0800  0.1000  0.2073  0.0982  0.0840  0.1294"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.5357</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.5812</td>\n",
       "      <td>0.2143</td>\n",
       "      <td>0.2308</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>-0.0980</td>\n",
       "      <td>-0.1720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.6147</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.2130</td>\n",
       "      <td>0.2915</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.2130</td>\n",
       "      <td>0.2915</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.2130</td>\n",
       "      <td>0.2915</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.5640</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.2130</td>\n",
       "      <td>0.2915</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.2130</td>\n",
       "      <td>0.2915</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.2130</td>\n",
       "      <td>0.2915</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.5640</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.2130</td>\n",
       "      <td>0.2915</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.4659</td>\n",
       "      <td>0.5781</td>\n",
       "      <td>0.2464</td>\n",
       "      <td>0.2222</td>\n",
       "      <td>0.3007</td>\n",
       "      <td>-0.0098</td>\n",
       "      <td>-0.0172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SD</th>\n",
       "      <td>0.0196</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
       "0       0.5000  0.5357  0.2500  0.2500  0.3333  0.0000  0.0000\n",
       "1       0.4286  0.5812  0.2143  0.2308  0.3000 -0.0980 -0.1720\n",
       "2       0.5000  0.6147  0.2500  0.2500  0.3333  0.0000  0.0000\n",
       "3       0.4615  0.6280  0.2500  0.2130  0.2915  0.0000  0.0000\n",
       "4       0.4615  0.5000  0.2500  0.2130  0.2915  0.0000  0.0000\n",
       "5       0.4615  0.6280  0.2500  0.2130  0.2915  0.0000  0.0000\n",
       "6       0.4615  0.5640  0.2500  0.2130  0.2915  0.0000  0.0000\n",
       "7       0.4615  0.5000  0.2500  0.2130  0.2915  0.0000  0.0000\n",
       "8       0.4615  0.6656  0.2500  0.2130  0.2915  0.0000  0.0000\n",
       "9       0.4615  0.5640  0.2500  0.2130  0.2915  0.0000  0.0000\n",
       "Mean    0.4659  0.5781  0.2464  0.2222  0.3007 -0.0098 -0.0172\n",
       "SD      0.0196  0.0533  0.0107  0.0149  0.0165  0.0294  0.0516"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = create_model('lr')\n",
    "rf = create_model('rf')\n",
    "xgb = create_model('xgboost')\n",
    "ada = create_model('ada')\n",
    "lda = create_model('lda')  # linear discriminant\n",
    "knn = create_model('knn')\n",
    "mlp = create_model('mlp')\n",
    "svm = create_model('svm')\n",
    "rbfsvm = create_model('rbfsvm')\n",
    "nb = create_model('nb')\n",
    "gpc = create_model('gpc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predicteurs : Les plus satisfaisants : ADA - LDA - KNN - LR : pluto moins bien en accuracy .... <br/>\n",
    "predicteurs 1 : Les plus satisfaisants : LR - LDA - RF - XGB - NB : environ 40% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "essai_classif = anglais[['Geography','Entities', 'Time', 'Narrative', 'Overall', 'Style', 'Tone','nb_entites_idem', \n",
    "    'nb_lieux_idem', 'nb_dates_idem', 'score_similarite_titres', 'score_similarite_resume1','score_similarite_resume2', \n",
    "    'score_classif1', 'score_classif2','score_sentiment1', 'score_sentiment2', 'score_sentiment3','meth1_similarites','meth2_similarites']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Geography                     int32\n",
       "Entities                      int32\n",
       "Time                          int32\n",
       "Narrative                     int32\n",
       "Overall                       int32\n",
       "Style                         int32\n",
       "Tone                          int32\n",
       "nb_entites_idem               int64\n",
       "nb_lieux_idem                 int64\n",
       "nb_dates_idem                 int64\n",
       "score_similarite_titres     float64\n",
       "score_similarite_resume1    float64\n",
       "score_similarite_resume2    float64\n",
       "score_classif1              float64\n",
       "score_classif2              float64\n",
       "score_sentiment1            float64\n",
       "score_sentiment2            float64\n",
       "score_sentiment3            float64\n",
       "meth1_similarites           float64\n",
       "meth2_similarites           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Si on veut utiliser faire un classemen,t supprimer ligne error puis changer les types pour meth1 meth2\n",
    "essai_classif = essai_classif[essai_classif.meth1_similarites != 'Error']\n",
    "essai_classif['meth1_similarites'] = essai_classif['meth1_similarites'].astype('float')\n",
    "essai_classif['meth2_similarites'] = essai_classif['meth2_similarites'].astype('float')\n",
    "essai_classif.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6429</td>\n",
       "      <td>0.8102</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.6039</td>\n",
       "      <td>0.5794</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.4341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.8267</td>\n",
       "      <td>0.3393</td>\n",
       "      <td>0.3690</td>\n",
       "      <td>0.4226</td>\n",
       "      <td>0.2033</td>\n",
       "      <td>0.2215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.7143</td>\n",
       "      <td>0.9096</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.8214</td>\n",
       "      <td>0.6769</td>\n",
       "      <td>0.5821</td>\n",
       "      <td>0.6219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3846</td>\n",
       "      <td>0.7439</td>\n",
       "      <td>0.2083</td>\n",
       "      <td>0.2885</td>\n",
       "      <td>0.3297</td>\n",
       "      <td>0.0459</td>\n",
       "      <td>0.0474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6923</td>\n",
       "      <td>0.8259</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.5192</td>\n",
       "      <td>0.5934</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.5444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.6154</td>\n",
       "      <td>0.7685</td>\n",
       "      <td>0.4583</td>\n",
       "      <td>0.6923</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>0.3868</td>\n",
       "      <td>0.4204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.5385</td>\n",
       "      <td>0.8685</td>\n",
       "      <td>0.4167</td>\n",
       "      <td>0.5220</td>\n",
       "      <td>0.5243</td>\n",
       "      <td>0.3097</td>\n",
       "      <td>0.3127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.6154</td>\n",
       "      <td>0.8639</td>\n",
       "      <td>0.5417</td>\n",
       "      <td>0.6346</td>\n",
       "      <td>0.6081</td>\n",
       "      <td>0.4348</td>\n",
       "      <td>0.4387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.5385</td>\n",
       "      <td>0.7465</td>\n",
       "      <td>0.4583</td>\n",
       "      <td>0.4322</td>\n",
       "      <td>0.4781</td>\n",
       "      <td>0.3158</td>\n",
       "      <td>0.3247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.5385</td>\n",
       "      <td>0.8395</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.3846</td>\n",
       "      <td>0.4462</td>\n",
       "      <td>0.2571</td>\n",
       "      <td>0.2803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.8203</td>\n",
       "      <td>0.4464</td>\n",
       "      <td>0.5268</td>\n",
       "      <td>0.5246</td>\n",
       "      <td>0.3424</td>\n",
       "      <td>0.3646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SD</th>\n",
       "      <td>0.0932</td>\n",
       "      <td>0.0517</td>\n",
       "      <td>0.1174</td>\n",
       "      <td>0.1558</td>\n",
       "      <td>0.0993</td>\n",
       "      <td>0.1466</td>\n",
       "      <td>0.1563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
       "0       0.6429  0.8102  0.5000  0.6039  0.5794  0.3750  0.4341\n",
       "1       0.5000  0.8267  0.3393  0.3690  0.4226  0.2033  0.2215\n",
       "2       0.7143  0.9096  0.6667  0.8214  0.6769  0.5821  0.6219\n",
       "3       0.3846  0.7439  0.2083  0.2885  0.3297  0.0459  0.0474\n",
       "4       0.6923  0.8259  0.5000  0.5192  0.5934  0.5140  0.5444\n",
       "5       0.6154  0.7685  0.4583  0.6923  0.5872  0.3868  0.4204\n",
       "6       0.5385  0.8685  0.4167  0.5220  0.5243  0.3097  0.3127\n",
       "7       0.6154  0.8639  0.5417  0.6346  0.6081  0.4348  0.4387\n",
       "8       0.5385  0.7465  0.4583  0.4322  0.4781  0.3158  0.3247\n",
       "9       0.5385  0.8395  0.3750  0.3846  0.4462  0.2571  0.2803\n",
       "Mean    0.5780  0.8203  0.4464  0.5268  0.5246  0.3424  0.3646\n",
       "SD      0.0932  0.0517  0.1174  0.1558  0.0993  0.1466  0.1563"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Xtrain = essai_classif[predicteurs + ['Overall']]\n",
    "eng_classif = setup(data = Xtrain,  target = 'Overall', html=False, silent=True, verbose=False)\n",
    "rf = create_model('rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = predict_model(rf)  # ne marche pas ????\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random Forest simple sur scikit learn\n",
    "Xtrain = essai_classif[predicteurs].reset_index(drop=True)\n",
    "ytrain = essai_classif['Overall'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(Xtrain[:taille_train],ytrain[:taille_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_entites_idem</th>\n",
       "      <th>nb_lieux_idem</th>\n",
       "      <th>nb_dates_idem</th>\n",
       "      <th>score_similarite_titres</th>\n",
       "      <th>score_similarite_resume1</th>\n",
       "      <th>score_similarite_resume2</th>\n",
       "      <th>score_classif1</th>\n",
       "      <th>score_classif2</th>\n",
       "      <th>score_sentiment1</th>\n",
       "      <th>score_sentiment2</th>\n",
       "      <th>score_sentiment3</th>\n",
       "      <th>meth1_similarites</th>\n",
       "      <th>meth2_similarites</th>\n",
       "      <th>Overall</th>\n",
       "      <th>RF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>16.92</td>\n",
       "      <td>13.18</td>\n",
       "      <td>12.12</td>\n",
       "      <td>55.78</td>\n",
       "      <td>8.34</td>\n",
       "      <td>20.98</td>\n",
       "      <td>6.82</td>\n",
       "      <td>97.85</td>\n",
       "      <td>526.2</td>\n",
       "      <td>408.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>15.11</td>\n",
       "      <td>30.88</td>\n",
       "      <td>9.29</td>\n",
       "      <td>32.23</td>\n",
       "      <td>8.32</td>\n",
       "      <td>24.34</td>\n",
       "      <td>98.09</td>\n",
       "      <td>96.14</td>\n",
       "      <td>419.6</td>\n",
       "      <td>421.1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.68</td>\n",
       "      <td>4.31</td>\n",
       "      <td>5.74</td>\n",
       "      <td>10.86</td>\n",
       "      <td>8.51</td>\n",
       "      <td>35.78</td>\n",
       "      <td>98.09</td>\n",
       "      <td>99.48</td>\n",
       "      <td>367.1</td>\n",
       "      <td>453.2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.08</td>\n",
       "      <td>2.45</td>\n",
       "      <td>3.82</td>\n",
       "      <td>10.04</td>\n",
       "      <td>8.33</td>\n",
       "      <td>20.46</td>\n",
       "      <td>61.80</td>\n",
       "      <td>79.66</td>\n",
       "      <td>253.3</td>\n",
       "      <td>327.2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7.41</td>\n",
       "      <td>14.68</td>\n",
       "      <td>22.20</td>\n",
       "      <td>13.71</td>\n",
       "      <td>8.35</td>\n",
       "      <td>18.03</td>\n",
       "      <td>2.12</td>\n",
       "      <td>63.97</td>\n",
       "      <td>239.6</td>\n",
       "      <td>140.1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.78</td>\n",
       "      <td>4.27</td>\n",
       "      <td>10.28</td>\n",
       "      <td>8.92</td>\n",
       "      <td>8.29</td>\n",
       "      <td>19.18</td>\n",
       "      <td>90.36</td>\n",
       "      <td>5.42</td>\n",
       "      <td>51.9</td>\n",
       "      <td>185.6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.11</td>\n",
       "      <td>28.36</td>\n",
       "      <td>22.68</td>\n",
       "      <td>20.06</td>\n",
       "      <td>8.37</td>\n",
       "      <td>21.05</td>\n",
       "      <td>17.43</td>\n",
       "      <td>99.57</td>\n",
       "      <td>196.9</td>\n",
       "      <td>735.7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7.94</td>\n",
       "      <td>29.86</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>32.86</td>\n",
       "      <td>8.35</td>\n",
       "      <td>26.32</td>\n",
       "      <td>5.13</td>\n",
       "      <td>95.33</td>\n",
       "      <td>654.8</td>\n",
       "      <td>1324.8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.93</td>\n",
       "      <td>13.98</td>\n",
       "      <td>11.86</td>\n",
       "      <td>13.83</td>\n",
       "      <td>8.32</td>\n",
       "      <td>21.20</td>\n",
       "      <td>14.38</td>\n",
       "      <td>99.43</td>\n",
       "      <td>179.4</td>\n",
       "      <td>129.4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>35.48</td>\n",
       "      <td>31.25</td>\n",
       "      <td>7.64</td>\n",
       "      <td>16.50</td>\n",
       "      <td>8.35</td>\n",
       "      <td>21.61</td>\n",
       "      <td>89.30</td>\n",
       "      <td>99.00</td>\n",
       "      <td>78.8</td>\n",
       "      <td>559.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     nb_entites_idem  nb_lieux_idem  nb_dates_idem  score_similarite_titres  \\\n",
       "100               10              0              7                    16.92   \n",
       "101                5              0              2                    15.11   \n",
       "102                0              0              4                     4.68   \n",
       "103               12              0              4                     3.08   \n",
       "104                2              2              7                     7.41   \n",
       "..               ...            ...            ...                      ...   \n",
       "186                0              0              2                     1.78   \n",
       "187                7              0              0                    25.11   \n",
       "188                3              0              6                     7.94   \n",
       "189                0              0              0                    13.93   \n",
       "190               22              0              2                    35.48   \n",
       "\n",
       "     score_similarite_resume1  score_similarite_resume2  score_classif1  \\\n",
       "100                     13.18                     12.12           55.78   \n",
       "101                     30.88                      9.29           32.23   \n",
       "102                      4.31                      5.74           10.86   \n",
       "103                      2.45                      3.82           10.04   \n",
       "104                     14.68                     22.20           13.71   \n",
       "..                        ...                       ...             ...   \n",
       "186                      4.27                     10.28            8.92   \n",
       "187                     28.36                     22.68           20.06   \n",
       "188                     29.86                     -1.50           32.86   \n",
       "189                     13.98                     11.86           13.83   \n",
       "190                     31.25                      7.64           16.50   \n",
       "\n",
       "     score_classif2  score_sentiment1  score_sentiment2  score_sentiment3  \\\n",
       "100            8.34             20.98              6.82             97.85   \n",
       "101            8.32             24.34             98.09             96.14   \n",
       "102            8.51             35.78             98.09             99.48   \n",
       "103            8.33             20.46             61.80             79.66   \n",
       "104            8.35             18.03              2.12             63.97   \n",
       "..              ...               ...               ...               ...   \n",
       "186            8.29             19.18             90.36              5.42   \n",
       "187            8.37             21.05             17.43             99.57   \n",
       "188            8.35             26.32              5.13             95.33   \n",
       "189            8.32             21.20             14.38             99.43   \n",
       "190            8.35             21.61             89.30             99.00   \n",
       "\n",
       "     meth1_similarites  meth2_similarites  Overall  RF  \n",
       "100              526.2              408.0        3   2  \n",
       "101              419.6              421.1        1   3  \n",
       "102              367.1              453.2        4   4  \n",
       "103              253.3              327.2        4   4  \n",
       "104              239.6              140.1        4   4  \n",
       "..                 ...                ...      ...  ..  \n",
       "186               51.9              185.6        4   4  \n",
       "187              196.9              735.7        1   2  \n",
       "188              654.8             1324.8        2   3  \n",
       "189              179.4              129.4        3   3  \n",
       "190               78.8              559.6        1   1  \n",
       "\n",
       "[91 rows x 15 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_rf = rf.predict(Xtrain[taille_train:])\n",
    "res_rf = pd.concat([Xtrain[taille_train:],ytrain[taille_train:],pd.DataFrame(res_rf,columns = ['RF'],index = range(taille_train,dernier_test))],axis=1)\n",
    "res_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  4,  1,  0],\n",
       "       [ 7,  3,  1,  3],\n",
       "       [ 0,  5,  7,  8],\n",
       "       [ 0,  3,  2, 39]], dtype=int64)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Résultats corrects : 57/91 - 27/91 : 1 d'écart et 7/91 : 2 écart \n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(res_rf.Overall,res_rf.RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11,  4,  1,  2,  0,  9,  7,  5,  8, 12, 10,  6,  3], dtype=int64)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "result = permutation_importance(rf, Xtrain, ytrain, n_repeats=10, random_state=42, n_jobs=2)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "sorted_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "tree_feature_importances = rf.feature_importances_\n",
    "sorted_idx = tree_feature_importances.argsort()\n",
    "\n",
    "y_ticks = np.arange(0, len(predicteurs))\n",
    "fig, ax = plt.subplots(figsize = (10,5))\n",
    "ax.barh(y_ticks, tree_feature_importances[sorted_idx])\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_yticklabels(np.array(predicteurs)[sorted_idx])\n",
    "ax.set_title(\"Random Forest Feature Importances (MDI)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall</th>\n",
       "      <th>RF</th>\n",
       "      <th>LDA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Overall  RF  LDA\n",
       "100        3   2    3\n",
       "101        1   3    3\n",
       "102        4   4    4\n",
       "103        4   4    4\n",
       "104        4   4    4\n",
       "..       ...  ..  ...\n",
       "186        4   4    4\n",
       "187        1   2    2\n",
       "188        2   3    2\n",
       "189        3   3    4\n",
       "190        1   1    1\n",
       "\n",
       "[91 rows x 3 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(Xtrain[:taille_train],ytrain[:taille_train])\n",
    "res_lda = lda.predict(Xtrain[taille_train:])\n",
    "res_final = pd.concat([res_rf[['Overall','RF']],pd.DataFrame(res_lda,columns = ['LDA'],index = range(taille_train,dernier_test))],axis=1)\n",
    "res_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.11,   0.57,  98.96,   0.36],\n",
       "       [  4.18,   8.16,  84.78,   2.88],\n",
       "       [  0.02,   0.42,  15.54,  84.02],\n",
       "       [  0.01,   0.1 ,   4.58,  95.31],\n",
       "       [  0.02,   4.06,  23.9 ,  72.02],\n",
       "       [  2.34,  24.43,  44.73,  28.5 ],\n",
       "       [  0.  ,   0.01,   2.2 ,  97.79],\n",
       "       [  0.54,   1.42,  97.61,   0.43],\n",
       "       [  0.01,   0.15,   8.92,  90.92],\n",
       "       [  0.01,   0.01,   6.39,  93.59],\n",
       "       [  0.69,   0.26,  68.56,  30.49],\n",
       "       [ 77.93,  21.91,   0.16,   0.  ],\n",
       "       [  0.  ,   0.  ,   1.68,  98.32],\n",
       "       [  0.63,   0.24,  67.81,  31.32],\n",
       "       [  0.34,   3.12,  51.63,  44.91],\n",
       "       [  0.01,   0.03,   5.9 ,  94.06],\n",
       "       [  0.  ,   0.02,  10.17,  89.81],\n",
       "       [  0.1 ,   9.47,  19.81,  70.63],\n",
       "       [  0.02,   0.19,  14.24,  85.55],\n",
       "       [ 99.08,   0.89,   0.03,   0.  ],\n",
       "       [ 92.72,   7.26,   0.02,   0.  ],\n",
       "       [ 82.17,  16.15,   1.65,   0.04],\n",
       "       [  0.16,   2.3 ,  27.68,  69.86],\n",
       "       [  0.  ,  99.99,   0.  ,   0.  ],\n",
       "       [  0.  ,   0.02,   7.63,  92.35],\n",
       "       [  0.07,   0.19,  15.8 ,  83.94],\n",
       "       [  0.06,   1.68,  21.94,  76.33],\n",
       "       [ 31.84,  41.88,  24.67,   1.61],\n",
       "       [  0.21,   7.64,  35.06,  57.09],\n",
       "       [  0.37,   0.82,  11.16,  87.65],\n",
       "       [ 32.9 ,   5.97,  61.12,   0.  ],\n",
       "       [  0.01,   0.13,  20.49,  79.37],\n",
       "       [ 65.89,  33.93,   0.17,   0.  ],\n",
       "       [  0.56,  66.26,  11.95,  21.24],\n",
       "       [  4.13,  84.36,  10.27,   1.24],\n",
       "       [  0.  , 100.  ,   0.  ,   0.  ],\n",
       "       [  0.  ,   0.09,   6.31,  93.59],\n",
       "       [  0.24,   0.28,  44.14,  55.33],\n",
       "       [  0.01,   0.1 ,   8.58,  91.31],\n",
       "       [  0.  ,   0.01,   1.54,  98.45],\n",
       "       [ 77.32,  22.67,   0.01,   0.  ],\n",
       "       [  0.  ,   0.01,   2.41,  97.59],\n",
       "       [  0.  ,   0.03,   4.25,  95.72],\n",
       "       [  0.16,   1.49,  20.11,  78.23],\n",
       "       [  0.  ,   0.01,   1.6 ,  98.39],\n",
       "       [  2.32,  94.37,   1.37,   1.94],\n",
       "       [  2.58,  71.3 ,  10.64,  15.49],\n",
       "       [ 61.25,  37.7 ,   0.86,   0.19],\n",
       "       [ 74.13,  11.1 ,  11.77,   3.  ],\n",
       "       [  0.91,  56.32,  27.1 ,  15.66],\n",
       "       [  0.82,   5.44,  19.86,  73.87],\n",
       "       [ 70.2 ,  20.04,   9.6 ,   0.16],\n",
       "       [  0.01,   0.04,  13.9 ,  86.06],\n",
       "       [  0.  ,   0.01,   2.99,  96.99],\n",
       "       [  0.  ,   0.03,   3.34,  96.63],\n",
       "       [  0.01,   0.17,  10.16,  89.66],\n",
       "       [ 28.2 ,   6.9 ,  46.23,  18.67],\n",
       "       [  0.  ,   0.01,   3.82,  96.17],\n",
       "       [  0.02,   0.67,  17.65,  81.66],\n",
       "       [  0.09,   3.14,  19.53,  77.24],\n",
       "       [ 76.69,  16.18,   6.95,   0.18],\n",
       "       [  0.  ,   0.  ,   0.22,  99.78],\n",
       "       [ 62.15,  37.04,   0.81,   0.  ],\n",
       "       [ 33.81,  64.97,   1.19,   0.04],\n",
       "       [  0.29,   0.91,  53.73,  45.07],\n",
       "       [  0.  ,   0.  ,   0.68,  99.32],\n",
       "       [  0.01,   0.48,  20.37,  79.14],\n",
       "       [  0.  ,   0.04,   5.25,  94.71],\n",
       "       [  0.66,   0.91,  31.35,  67.07],\n",
       "       [  0.  ,   0.01,   3.22,  96.77],\n",
       "       [  0.11,   0.08,  40.66,  59.14],\n",
       "       [  0.  ,   0.  ,   1.38,  98.62],\n",
       "       [ 98.68,   1.32,   0.  ,   0.  ],\n",
       "       [  0.  ,   0.03,   5.22,  94.75],\n",
       "       [  0.  ,   0.01,   6.21,  93.77],\n",
       "       [  1.96,  97.65,   0.28,   0.11],\n",
       "       [  0.02,   0.09,  10.63,  89.27],\n",
       "       [  0.01,   0.03,   5.24,  94.72],\n",
       "       [  1.25,  35.44,  54.84,   8.48],\n",
       "       [ 92.71,   7.29,   0.01,   0.  ],\n",
       "       [ 47.7 ,  39.58,  11.29,   1.43],\n",
       "       [  1.89,  83.13,  11.89,   3.1 ],\n",
       "       [ 91.08,   0.73,   8.14,   0.05],\n",
       "       [ 20.14,  35.19,  35.96,   8.71],\n",
       "       [  4.28,   4.73,  58.32,  32.67],\n",
       "       [  0.  ,   0.06,   9.62,  90.32],\n",
       "       [  0.  ,   0.02,   5.59,  94.39],\n",
       "       [ 27.39,  70.17,   2.18,   0.26],\n",
       "       [  0.09,  98.74,   1.12,   0.05],\n",
       "       [  0.32,   0.63,  36.39,  62.67],\n",
       "       [ 90.57,   9.29,   0.14,   0.01]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import around\n",
    "import numpy\n",
    "np.set_printoptions(suppress=True)  # supprime notation exp\n",
    "res_lda2 = around(lda.predict_proba(Xtrain[taille_train:])*100, decimals=2)\n",
    "res_lda2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.,  45.,  27.,  21.],\n",
       "       [ 23.,  26.,  42.,   9.],\n",
       "       [  6.,   6.,  13.,  75.],\n",
       "       [  5.,  11.,   9.,  75.],\n",
       "       [  0.,   6.,  17.,  77.],\n",
       "       [  6.,  41.,  36.,  17.],\n",
       "       [  1.,   0.,   5.,  94.],\n",
       "       [ 17.,  33.,  19.,  31.],\n",
       "       [  0.,   2.,  20.,  78.],\n",
       "       [  0.,   1.,   8.,  91.],\n",
       "       [ 10.,  11.,  39.,  40.],\n",
       "       [ 27.,  57.,  11.,   5.],\n",
       "       [  0.,   0.,   7.,  93.],\n",
       "       [ 20.,   6.,  31.,  43.],\n",
       "       [  3.,  29.,  31.,  37.],\n",
       "       [  7.,   1.,  29.,  63.],\n",
       "       [  2.,   1.,   3.,  94.],\n",
       "       [  6.,  11.,  16.,  67.],\n",
       "       [  2.,   0.,   8.,  90.],\n",
       "       [ 59.,  24.,   9.,   8.],\n",
       "       [ 67.,  24.,   7.,   2.],\n",
       "       [ 43.,  29.,  19.,   9.],\n",
       "       [  6.,  19.,  40.,  35.],\n",
       "       [ 27.,  40.,  21.,  12.],\n",
       "       [  1.,   0.,  13.,  86.],\n",
       "       [  1.,   3.,  27.,  69.],\n",
       "       [  3.,   7.,  24.,  66.],\n",
       "       [ 21.,  30.,  39.,  10.],\n",
       "       [ 14.,  15.,  28.,  43.],\n",
       "       [  4.,  10.,  18.,  68.],\n",
       "       [ 67.,  19.,  10.,   4.],\n",
       "       [  2.,   3.,  24.,  71.],\n",
       "       [ 53.,  36.,   8.,   3.],\n",
       "       [  4.,  38.,  32.,  26.],\n",
       "       [ 38.,  44.,  13.,   5.],\n",
       "       [ 31.,  59.,   9.,   1.],\n",
       "       [  0.,   6.,   7.,  87.],\n",
       "       [ 10.,  16.,  33.,  41.],\n",
       "       [  3.,   9.,  20.,  68.],\n",
       "       [  0.,   0.,   3.,  97.],\n",
       "       [ 58.,  24.,   9.,   9.],\n",
       "       [  1.,   1.,   1.,  97.],\n",
       "       [  3.,   6.,   8.,  83.],\n",
       "       [  7.,   8.,  24.,  61.],\n",
       "       [  0.,   0.,   0., 100.],\n",
       "       [ 17.,  34.,  43.,   6.],\n",
       "       [ 16.,  39.,  17.,  28.],\n",
       "       [ 40.,  35.,  11.,  14.],\n",
       "       [ 38.,  14.,  25.,  23.],\n",
       "       [ 14.,  38.,  31.,  17.],\n",
       "       [  4.,  21.,  24.,  51.],\n",
       "       [ 31.,  44.,  14.,  11.],\n",
       "       [  3.,   0.,  11.,  86.],\n",
       "       [  0.,   1.,  21.,  78.],\n",
       "       [  0.,   2.,   9.,  89.],\n",
       "       [  3.,   9.,  10.,  78.],\n",
       "       [ 19.,  34.,  17.,  30.],\n",
       "       [  0.,   0.,   5.,  95.],\n",
       "       [  5.,   3.,  26.,  66.],\n",
       "       [  1.,   9.,  11.,  79.],\n",
       "       [ 54.,  20.,  11.,  15.],\n",
       "       [  2.,   1.,  10.,  87.],\n",
       "       [ 65.,  29.,   4.,   2.],\n",
       "       [ 54.,  39.,   4.,   3.],\n",
       "       [  1.,  14.,  26.,  59.],\n",
       "       [  0.,   0.,   4.,  96.],\n",
       "       [  2.,   2.,  13.,  83.],\n",
       "       [  0.,   4.,   4.,  92.],\n",
       "       [  2.,  11.,  45.,  42.],\n",
       "       [  1.,   0.,   7.,  92.],\n",
       "       [ 12.,  10.,  45.,  33.],\n",
       "       [  1.,   0.,   4.,  95.],\n",
       "       [ 79.,  16.,   4.,   1.],\n",
       "       [  1.,   1.,  20.,  78.],\n",
       "       [  2.,   1.,   8.,  89.],\n",
       "       [ 29.,  22.,  21.,  28.],\n",
       "       [  2.,   2.,  24.,  72.],\n",
       "       [  2.,   1.,   4.,  93.],\n",
       "       [  7.,  33.,  34.,  26.],\n",
       "       [ 72.,  12.,   7.,   9.],\n",
       "       [ 37.,  50.,  12.,   1.],\n",
       "       [  7.,  27.,  50.,  16.],\n",
       "       [ 34.,   8.,  19.,  39.],\n",
       "       [ 20.,  48.,  23.,   9.],\n",
       "       [ 10.,  30.,  42.,  18.],\n",
       "       [  3.,   3.,  14.,  80.],\n",
       "       [  2.,   1.,   9.,  88.],\n",
       "       [ 28.,  48.,  20.,   4.],\n",
       "       [ 15.,  23.,  35.,  27.],\n",
       "       [  4.,   6.,  52.,  38.],\n",
       "       [ 68.,  16.,   8.,   8.]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_rf2 = around(rf.predict_proba(Xtrain[taille_train:])*100, decimals=2)\n",
    "res_rf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Remarquer que la classification ne tient pas compte du fait que c'est ordonné en classement 1-2-3-4 : ce qui est TRES important (ex : ligne 55% de 1 - 43% de 4) !! : il faudrait donc faire ressortir un score avec les probas plutot !!!_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nouveau programme basé sur les scores probas : si plus de 50% mettre catégorie obtenue sinon, faire la somme 1-2 et 3/4 \n",
    "# et prendre le plus gros score puis regarder si ce sore > 65% alors à ce moment là prendre le plus gros de la catégorie \n",
    "# sinon prendre 2 ou 3\n",
    "def choix_classes(score_prob):\n",
    "    classe_finale = []\n",
    "    for i in range(len(score_prob)):\n",
    "        res = list(score_prob[i,:])\n",
    "        max_res = max(res)\n",
    "        if max_res > 50:\n",
    "            classe_finale.append(res.index(max_res)+1)\n",
    "        else:\n",
    "            som1 = res[0]+res[1]\n",
    "            som2 = res[2]+res[3]\n",
    "            if som1 > som2:\n",
    "                if som1 >= 65:\n",
    "                    choix = 1 if res[0]>res[1] else 2\n",
    "                else:\n",
    "                    choix = 2\n",
    "            else:\n",
    "                if som2 >= 65:\n",
    "                    choix = 4 if res[3]>res[2] else 3\n",
    "                else:\n",
    "                    choix = 3\n",
    "            classe_finale.append(choix)\n",
    "    return classe_finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_rf = choix_classes(res_rf2)\n",
    "liste_lda = choix_classes(res_lda2)\n",
    "res_final = pd.concat([res_final,pd.DataFrame(liste_lda,columns = ['LDA_Prob'],index = range(taille_train,dernier_test)),\n",
    "                       pd.DataFrame(liste_rf,columns = ['RF_Prob'],index = range(taille_train,dernier_test))],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(Xtrain[:taille_train],ytrain[:taille_train])\n",
    "res_knn = knn.predict(Xtrain[taille_train:])\n",
    "liste_knn = choix_classes(around(knn.predict_proba(Xtrain[taille_train:])*100, decimals=2))\n",
    "res_final = pd.concat([res_final,pd.DataFrame(res_knn,columns = ['KNN'],index = range(taille_train,dernier_test)),\n",
    "                      pd.DataFrame(liste_knn,columns = ['KNN_Prob'],index = range(taille_train,dernier_test))],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(Xtrain[:taille_train],ytrain[:taille_train])\n",
    "res_logreg = logreg.predict(Xtrain[taille_train:])\n",
    "liste_logreg = choix_classes(around(logreg.predict_proba(Xtrain[taille_train:])*100, decimals=2))\n",
    "res_final = pd.concat([res_final,pd.DataFrame(res_logreg,columns = ['LOGR'],index = range(taille_train,dernier_test)),\n",
    "                      pd.DataFrame(liste_knn,columns = ['LOGR_Prob'],index = range(taille_train,dernier_test))],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada = AdaBoostClassifier()\n",
    "ada.fit(Xtrain[:taille_train],ytrain[:taille_train])\n",
    "res_ada = ada.predict(Xtrain[taille_train:])\n",
    "liste_ada = choix_classes(around(ada.predict_proba(Xtrain[taille_train:])*100, decimals=2))\n",
    "res_final = pd.concat([res_final,pd.DataFrame(res_knn,columns = ['ADA'],index = range(taille_train,dernier_test)),\n",
    "                      pd.DataFrame(liste_knn,columns = ['ADA_Prob'],index = range(taille_train,dernier_test))],axis=1)\n",
    "res_final = res_final [['Overall','RF','LDA','KNN','LOGR','ADA','RF_Prob','LDA_Prob','KNN_Prob','LOGR_Prob','ADA_Prob']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  4,  1,  0],\n",
       "       [ 7,  3,  1,  3],\n",
       "       [ 0,  5,  7,  8],\n",
       "       [ 0,  3,  2, 39]], dtype=int64)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(res_final.Overall,res_final.RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  4,  1,  0],\n",
       "       [ 5,  4,  2,  3],\n",
       "       [ 0,  5,  7,  8],\n",
       "       [ 0,  3,  3, 38]], dtype=int64)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(res_final.Overall,res_final.RF_Prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  2,  1,  0],\n",
       "       [ 4,  5,  5,  0],\n",
       "       [ 1,  4,  4, 11],\n",
       "       [ 1,  2,  3, 38]], dtype=int64)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(res_final.Overall,res_final.LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  2,  1,  0],\n",
       "       [ 4,  5,  5,  0],\n",
       "       [ 1,  4,  4, 11],\n",
       "       [ 1,  3,  2, 38]], dtype=int64)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(res_final.Overall,res_final.LDA_Prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  6,  1,  4],\n",
       "       [ 4,  6,  1,  3],\n",
       "       [ 5,  5,  0, 10],\n",
       "       [ 3,  6,  5, 30]], dtype=int64)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(res_final.Overall,res_final.KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  6,  2,  3],\n",
       "       [ 2,  8,  1,  3],\n",
       "       [ 1,  7,  3,  9],\n",
       "       [ 1,  7,  7, 29]], dtype=int64)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(res_final.Overall,res_final.KNN_Prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9,  3,  1,  0],\n",
       "       [ 3,  5,  3,  3],\n",
       "       [ 1,  3,  4, 12],\n",
       "       [ 1,  3,  1, 39]], dtype=int64)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(res_final.Overall,res_final.LOGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  6,  2,  3],\n",
       "       [ 2,  8,  1,  3],\n",
       "       [ 1,  7,  3,  9],\n",
       "       [ 1,  7,  7, 29]], dtype=int64)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention, ce n'est plus bon du tout ....\n",
    "confusion_matrix(res_final.Overall,res_final.LOGR_Prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  6,  1,  4],\n",
       "       [ 4,  6,  1,  3],\n",
       "       [ 5,  5,  0, 10],\n",
       "       [ 3,  6,  5, 30]], dtype=int64)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(res_final.Overall,res_final.ADA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  6,  2,  3],\n",
       "       [ 2,  8,  1,  3],\n",
       "       [ 1,  7,  3,  9],\n",
       "       [ 1,  7,  7, 29]], dtype=int64)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(res_final.Overall,res_final.ADA_Prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REGRESSION Sklearn Pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.regression import *\n",
    "from sklearn.linear_model import LinearRegression\n",
    "anglais = pd.read_csv('corpus_en_notes.csv',index_col=0)\n",
    "# anglais2 = pd.read_csv('corpus_en_de_notes.csv',index_col=0)\n",
    "# anglais = pd.concat([anglais,anglais2],axis=0).reset_index(drop=True)\n",
    "anglais = anglais[anglais.meth1_similarites!='Error']\n",
    "english_classif = setup(data = anglais[predicteurs + ['Overall']],  target = 'Overall', html=False, silent=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>RMSLE</th>\n",
       "      <th>MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7087</td>\n",
       "      <td>0.7993</td>\n",
       "      <td>0.8940</td>\n",
       "      <td>-0.1146</td>\n",
       "      <td>0.2608</td>\n",
       "      <td>0.2437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.3694</td>\n",
       "      <td>0.2012</td>\n",
       "      <td>0.4486</td>\n",
       "      <td>0.8481</td>\n",
       "      <td>0.1670</td>\n",
       "      <td>0.1662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6331</td>\n",
       "      <td>0.6331</td>\n",
       "      <td>0.7957</td>\n",
       "      <td>0.5163</td>\n",
       "      <td>0.2630</td>\n",
       "      <td>0.3875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6552</td>\n",
       "      <td>0.7115</td>\n",
       "      <td>0.8435</td>\n",
       "      <td>0.4268</td>\n",
       "      <td>0.3269</td>\n",
       "      <td>0.2895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.8844</td>\n",
       "      <td>1.2604</td>\n",
       "      <td>1.1227</td>\n",
       "      <td>-0.1704</td>\n",
       "      <td>0.3892</td>\n",
       "      <td>0.5134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.7635</td>\n",
       "      <td>0.8805</td>\n",
       "      <td>0.9383</td>\n",
       "      <td>0.2251</td>\n",
       "      <td>0.2882</td>\n",
       "      <td>0.4284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.7455</td>\n",
       "      <td>0.8614</td>\n",
       "      <td>0.9281</td>\n",
       "      <td>0.3479</td>\n",
       "      <td>0.2967</td>\n",
       "      <td>0.4467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.5425</td>\n",
       "      <td>0.5305</td>\n",
       "      <td>0.7283</td>\n",
       "      <td>0.3987</td>\n",
       "      <td>0.1933</td>\n",
       "      <td>0.2438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.7893</td>\n",
       "      <td>0.9168</td>\n",
       "      <td>0.9575</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.3309</td>\n",
       "      <td>0.3862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.5278</td>\n",
       "      <td>0.4370</td>\n",
       "      <td>0.6611</td>\n",
       "      <td>0.5251</td>\n",
       "      <td>0.1900</td>\n",
       "      <td>0.2311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.6619</td>\n",
       "      <td>0.7232</td>\n",
       "      <td>0.8318</td>\n",
       "      <td>0.3052</td>\n",
       "      <td>0.2706</td>\n",
       "      <td>0.3337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SD</th>\n",
       "      <td>0.1428</td>\n",
       "      <td>0.2785</td>\n",
       "      <td>0.1769</td>\n",
       "      <td>0.2979</td>\n",
       "      <td>0.0672</td>\n",
       "      <td>0.1079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         MAE     MSE    RMSE      R2   RMSLE    MAPE\n",
       "0     0.7087  0.7993  0.8940 -0.1146  0.2608  0.2437\n",
       "1     0.3694  0.2012  0.4486  0.8481  0.1670  0.1662\n",
       "2     0.6331  0.6331  0.7957  0.5163  0.2630  0.3875\n",
       "3     0.6552  0.7115  0.8435  0.4268  0.3269  0.2895\n",
       "4     0.8844  1.2604  1.1227 -0.1704  0.3892  0.5134\n",
       "5     0.7635  0.8805  0.9383  0.2251  0.2882  0.4284\n",
       "6     0.7455  0.8614  0.9281  0.3479  0.2967  0.4467\n",
       "7     0.5425  0.5305  0.7283  0.3987  0.1933  0.2438\n",
       "8     0.7893  0.9168  0.9575  0.0485  0.3309  0.3862\n",
       "9     0.5278  0.4370  0.6611  0.5251  0.1900  0.2311\n",
       "Mean  0.6619  0.7232  0.8318  0.3052  0.2706  0.3337\n",
       "SD    0.1428  0.2785  0.1769  0.2979  0.0672  0.1079"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>RMSLE</th>\n",
       "      <th>MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4118</td>\n",
       "      <td>0.4516</td>\n",
       "      <td>0.6720</td>\n",
       "      <td>0.3702</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.1413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.3412</td>\n",
       "      <td>0.1905</td>\n",
       "      <td>0.4365</td>\n",
       "      <td>0.8562</td>\n",
       "      <td>0.1221</td>\n",
       "      <td>0.1477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.4931</td>\n",
       "      <td>0.4754</td>\n",
       "      <td>0.6895</td>\n",
       "      <td>0.6368</td>\n",
       "      <td>0.2395</td>\n",
       "      <td>0.3353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6455</td>\n",
       "      <td>0.5976</td>\n",
       "      <td>0.7730</td>\n",
       "      <td>0.5186</td>\n",
       "      <td>0.2328</td>\n",
       "      <td>0.2995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4244</td>\n",
       "      <td>0.3826</td>\n",
       "      <td>0.6185</td>\n",
       "      <td>0.6447</td>\n",
       "      <td>0.2175</td>\n",
       "      <td>0.2782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.3267</td>\n",
       "      <td>0.1864</td>\n",
       "      <td>0.4317</td>\n",
       "      <td>0.8360</td>\n",
       "      <td>0.1302</td>\n",
       "      <td>0.1589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.6523</td>\n",
       "      <td>0.6580</td>\n",
       "      <td>0.8112</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.2501</td>\n",
       "      <td>0.3452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.3374</td>\n",
       "      <td>0.1746</td>\n",
       "      <td>0.4178</td>\n",
       "      <td>0.8021</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>0.1728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.4010</td>\n",
       "      <td>0.2556</td>\n",
       "      <td>0.5056</td>\n",
       "      <td>0.7347</td>\n",
       "      <td>0.1458</td>\n",
       "      <td>0.1850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.4590</td>\n",
       "      <td>0.3539</td>\n",
       "      <td>0.5949</td>\n",
       "      <td>0.6154</td>\n",
       "      <td>0.1763</td>\n",
       "      <td>0.2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.4492</td>\n",
       "      <td>0.3726</td>\n",
       "      <td>0.5951</td>\n",
       "      <td>0.6517</td>\n",
       "      <td>0.1830</td>\n",
       "      <td>0.2266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SD</th>\n",
       "      <td>0.1120</td>\n",
       "      <td>0.1643</td>\n",
       "      <td>0.1360</td>\n",
       "      <td>0.1504</td>\n",
       "      <td>0.0465</td>\n",
       "      <td>0.0756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         MAE     MSE    RMSE      R2   RMSLE    MAPE\n",
       "0     0.4118  0.4516  0.6720  0.3702  0.1809  0.1413\n",
       "1     0.3412  0.1905  0.4365  0.8562  0.1221  0.1477\n",
       "2     0.4931  0.4754  0.6895  0.6368  0.2395  0.3353\n",
       "3     0.6455  0.5976  0.7730  0.5186  0.2328  0.2995\n",
       "4     0.4244  0.3826  0.6185  0.6447  0.2175  0.2782\n",
       "5     0.3267  0.1864  0.4317  0.8360  0.1302  0.1589\n",
       "6     0.6523  0.6580  0.8112  0.5019  0.2501  0.3452\n",
       "7     0.3374  0.1746  0.4178  0.8021  0.1343  0.1728\n",
       "8     0.4010  0.2556  0.5056  0.7347  0.1458  0.1850\n",
       "9     0.4590  0.3539  0.5949  0.6154  0.1763  0.2016\n",
       "Mean  0.4492  0.3726  0.5951  0.6517  0.1830  0.2266\n",
       "SD    0.1120  0.1643  0.1360  0.1504  0.0465  0.0756"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>RMSLE</th>\n",
       "      <th>MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.3365</td>\n",
       "      <td>0.5679</td>\n",
       "      <td>0.7536</td>\n",
       "      <td>0.2081</td>\n",
       "      <td>0.2127</td>\n",
       "      <td>0.0981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.3208</td>\n",
       "      <td>0.1657</td>\n",
       "      <td>0.4070</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.1169</td>\n",
       "      <td>0.1467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5038</td>\n",
       "      <td>0.4618</td>\n",
       "      <td>0.6795</td>\n",
       "      <td>0.6472</td>\n",
       "      <td>0.2314</td>\n",
       "      <td>0.3098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5326</td>\n",
       "      <td>0.6217</td>\n",
       "      <td>0.7885</td>\n",
       "      <td>0.4992</td>\n",
       "      <td>0.2271</td>\n",
       "      <td>0.2250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4197</td>\n",
       "      <td>0.3187</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>0.7041</td>\n",
       "      <td>0.1994</td>\n",
       "      <td>0.2627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.4023</td>\n",
       "      <td>0.2728</td>\n",
       "      <td>0.5223</td>\n",
       "      <td>0.7599</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.2109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.5296</td>\n",
       "      <td>0.4962</td>\n",
       "      <td>0.7044</td>\n",
       "      <td>0.6244</td>\n",
       "      <td>0.2025</td>\n",
       "      <td>0.2273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.4350</td>\n",
       "      <td>0.2756</td>\n",
       "      <td>0.5250</td>\n",
       "      <td>0.6876</td>\n",
       "      <td>0.1519</td>\n",
       "      <td>0.2035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.4065</td>\n",
       "      <td>0.3171</td>\n",
       "      <td>0.5631</td>\n",
       "      <td>0.6709</td>\n",
       "      <td>0.1452</td>\n",
       "      <td>0.1573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.4225</td>\n",
       "      <td>0.3024</td>\n",
       "      <td>0.5499</td>\n",
       "      <td>0.6714</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>0.1860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.4309</td>\n",
       "      <td>0.3800</td>\n",
       "      <td>0.6058</td>\n",
       "      <td>0.6348</td>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.2028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SD</th>\n",
       "      <td>0.0693</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.1686</td>\n",
       "      <td>0.0365</td>\n",
       "      <td>0.0571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         MAE     MSE    RMSE      R2   RMSLE    MAPE\n",
       "0     0.3365  0.5679  0.7536  0.2081  0.2127  0.0981\n",
       "1     0.3208  0.1657  0.4070  0.8750  0.1169  0.1467\n",
       "2     0.5038  0.4618  0.6795  0.6472  0.2314  0.3098\n",
       "3     0.5326  0.6217  0.7885  0.4992  0.2271  0.2250\n",
       "4     0.4197  0.3187  0.5645  0.7041  0.1994  0.2627\n",
       "5     0.4023  0.2728  0.5223  0.7599  0.1601  0.2109\n",
       "6     0.5296  0.4962  0.7044  0.6244  0.2025  0.2273\n",
       "7     0.4350  0.2756  0.5250  0.6876  0.1519  0.2035\n",
       "8     0.4065  0.3171  0.5631  0.6709  0.1452  0.1573\n",
       "9     0.4225  0.3024  0.5499  0.6714  0.1674  0.1860\n",
       "Mean  0.4309  0.3800  0.6058  0.6348  0.1815  0.2028\n",
       "SD    0.0693  0.1400  0.1140  0.1686  0.0365  0.0571"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>RMSLE</th>\n",
       "      <th>MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.3450</td>\n",
       "      <td>0.4261</td>\n",
       "      <td>0.6528</td>\n",
       "      <td>0.4058</td>\n",
       "      <td>0.1767</td>\n",
       "      <td>0.1114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.3326</td>\n",
       "      <td>0.1886</td>\n",
       "      <td>0.4343</td>\n",
       "      <td>0.8576</td>\n",
       "      <td>0.1245</td>\n",
       "      <td>0.1496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3911</td>\n",
       "      <td>0.3296</td>\n",
       "      <td>0.5741</td>\n",
       "      <td>0.7482</td>\n",
       "      <td>0.2081</td>\n",
       "      <td>0.2789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6158</td>\n",
       "      <td>0.6415</td>\n",
       "      <td>0.8009</td>\n",
       "      <td>0.4832</td>\n",
       "      <td>0.2386</td>\n",
       "      <td>0.2882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4528</td>\n",
       "      <td>0.4603</td>\n",
       "      <td>0.6785</td>\n",
       "      <td>0.5726</td>\n",
       "      <td>0.2355</td>\n",
       "      <td>0.3078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.3099</td>\n",
       "      <td>0.1880</td>\n",
       "      <td>0.4336</td>\n",
       "      <td>0.8346</td>\n",
       "      <td>0.1312</td>\n",
       "      <td>0.1570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.6224</td>\n",
       "      <td>0.5469</td>\n",
       "      <td>0.7395</td>\n",
       "      <td>0.5860</td>\n",
       "      <td>0.2286</td>\n",
       "      <td>0.3165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.3996</td>\n",
       "      <td>0.2395</td>\n",
       "      <td>0.4894</td>\n",
       "      <td>0.7286</td>\n",
       "      <td>0.1600</td>\n",
       "      <td>0.2094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.4851</td>\n",
       "      <td>0.3560</td>\n",
       "      <td>0.5966</td>\n",
       "      <td>0.6306</td>\n",
       "      <td>0.1645</td>\n",
       "      <td>0.2082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.4324</td>\n",
       "      <td>0.3907</td>\n",
       "      <td>0.6251</td>\n",
       "      <td>0.5754</td>\n",
       "      <td>0.1863</td>\n",
       "      <td>0.1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.4387</td>\n",
       "      <td>0.3767</td>\n",
       "      <td>0.6025</td>\n",
       "      <td>0.6423</td>\n",
       "      <td>0.1854</td>\n",
       "      <td>0.2225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SD</th>\n",
       "      <td>0.1039</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.1172</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         MAE     MSE    RMSE      R2   RMSLE    MAPE\n",
       "0     0.3450  0.4261  0.6528  0.4058  0.1767  0.1114\n",
       "1     0.3326  0.1886  0.4343  0.8576  0.1245  0.1496\n",
       "2     0.3911  0.3296  0.5741  0.7482  0.2081  0.2789\n",
       "3     0.6158  0.6415  0.8009  0.4832  0.2386  0.2882\n",
       "4     0.4528  0.4603  0.6785  0.5726  0.2355  0.3078\n",
       "5     0.3099  0.1880  0.4336  0.8346  0.1312  0.1570\n",
       "6     0.6224  0.5469  0.7395  0.5860  0.2286  0.3165\n",
       "7     0.3996  0.2395  0.4894  0.7286  0.1600  0.2094\n",
       "8     0.4851  0.3560  0.5966  0.6306  0.1645  0.2082\n",
       "9     0.4324  0.3907  0.6251  0.5754  0.1863  0.1981\n",
       "Mean  0.4387  0.3767  0.6025  0.6423  0.1854  0.2225\n",
       "SD    0.1039  0.1418  0.1172  0.1400  0.0394  0.0682"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>RMSLE</th>\n",
       "      <th>MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4187</td>\n",
       "      <td>0.5372</td>\n",
       "      <td>0.7329</td>\n",
       "      <td>0.2509</td>\n",
       "      <td>0.1991</td>\n",
       "      <td>0.1183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.4248</td>\n",
       "      <td>0.2544</td>\n",
       "      <td>0.5043</td>\n",
       "      <td>0.8080</td>\n",
       "      <td>0.1426</td>\n",
       "      <td>0.1896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5788</td>\n",
       "      <td>0.5569</td>\n",
       "      <td>0.7463</td>\n",
       "      <td>0.5745</td>\n",
       "      <td>0.2469</td>\n",
       "      <td>0.3417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6192</td>\n",
       "      <td>0.7310</td>\n",
       "      <td>0.8550</td>\n",
       "      <td>0.4111</td>\n",
       "      <td>0.2495</td>\n",
       "      <td>0.2692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4663</td>\n",
       "      <td>0.4230</td>\n",
       "      <td>0.6504</td>\n",
       "      <td>0.6072</td>\n",
       "      <td>0.2267</td>\n",
       "      <td>0.2990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.3907</td>\n",
       "      <td>0.2649</td>\n",
       "      <td>0.5147</td>\n",
       "      <td>0.7669</td>\n",
       "      <td>0.1545</td>\n",
       "      <td>0.1962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.7240</td>\n",
       "      <td>0.8660</td>\n",
       "      <td>0.9306</td>\n",
       "      <td>0.3444</td>\n",
       "      <td>0.2747</td>\n",
       "      <td>0.3185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.3898</td>\n",
       "      <td>0.1825</td>\n",
       "      <td>0.4272</td>\n",
       "      <td>0.7931</td>\n",
       "      <td>0.1277</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.4137</td>\n",
       "      <td>0.2975</td>\n",
       "      <td>0.5454</td>\n",
       "      <td>0.6913</td>\n",
       "      <td>0.1455</td>\n",
       "      <td>0.1647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.5473</td>\n",
       "      <td>0.4408</td>\n",
       "      <td>0.6639</td>\n",
       "      <td>0.5211</td>\n",
       "      <td>0.1940</td>\n",
       "      <td>0.2336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.4973</td>\n",
       "      <td>0.4554</td>\n",
       "      <td>0.6571</td>\n",
       "      <td>0.5768</td>\n",
       "      <td>0.1961</td>\n",
       "      <td>0.2308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SD</th>\n",
       "      <td>0.1085</td>\n",
       "      <td>0.2095</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.0494</td>\n",
       "      <td>0.0699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         MAE     MSE    RMSE      R2   RMSLE    MAPE\n",
       "0     0.4187  0.5372  0.7329  0.2509  0.1991  0.1183\n",
       "1     0.4248  0.2544  0.5043  0.8080  0.1426  0.1896\n",
       "2     0.5788  0.5569  0.7463  0.5745  0.2469  0.3417\n",
       "3     0.6192  0.7310  0.8550  0.4111  0.2495  0.2692\n",
       "4     0.4663  0.4230  0.6504  0.6072  0.2267  0.2990\n",
       "5     0.3907  0.2649  0.5147  0.7669  0.1545  0.1962\n",
       "6     0.7240  0.8660  0.9306  0.3444  0.2747  0.3185\n",
       "7     0.3898  0.1825  0.4272  0.7931  0.1277  0.1774\n",
       "8     0.4137  0.2975  0.5454  0.6913  0.1455  0.1647\n",
       "9     0.5473  0.4408  0.6639  0.5211  0.1940  0.2336\n",
       "Mean  0.4973  0.4554  0.6571  0.5768  0.1961  0.2308\n",
       "SD    0.1085  0.2095  0.1539  0.1847  0.0494  0.0699"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>RMSLE</th>\n",
       "      <th>MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7842</td>\n",
       "      <td>1.0789</td>\n",
       "      <td>1.0387</td>\n",
       "      <td>-0.5046</td>\n",
       "      <td>0.2901</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5428</td>\n",
       "      <td>0.4740</td>\n",
       "      <td>0.6885</td>\n",
       "      <td>0.6422</td>\n",
       "      <td>0.2006</td>\n",
       "      <td>0.2496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6078</td>\n",
       "      <td>0.7052</td>\n",
       "      <td>0.8398</td>\n",
       "      <td>0.4613</td>\n",
       "      <td>0.2515</td>\n",
       "      <td>0.3006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.8579</td>\n",
       "      <td>1.2575</td>\n",
       "      <td>1.1214</td>\n",
       "      <td>-0.0131</td>\n",
       "      <td>0.3469</td>\n",
       "      <td>0.5123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4564</td>\n",
       "      <td>0.2942</td>\n",
       "      <td>0.5424</td>\n",
       "      <td>0.7268</td>\n",
       "      <td>0.1770</td>\n",
       "      <td>0.2271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5534</td>\n",
       "      <td>0.3729</td>\n",
       "      <td>0.6107</td>\n",
       "      <td>0.6718</td>\n",
       "      <td>0.1767</td>\n",
       "      <td>0.2659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.7702</td>\n",
       "      <td>0.9070</td>\n",
       "      <td>0.9524</td>\n",
       "      <td>0.3134</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.4486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.4655</td>\n",
       "      <td>0.3375</td>\n",
       "      <td>0.5809</td>\n",
       "      <td>0.6175</td>\n",
       "      <td>0.1997</td>\n",
       "      <td>0.2476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0293</td>\n",
       "      <td>1.5964</td>\n",
       "      <td>1.2635</td>\n",
       "      <td>-0.6568</td>\n",
       "      <td>0.3252</td>\n",
       "      <td>0.4653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.5850</td>\n",
       "      <td>0.5592</td>\n",
       "      <td>0.7478</td>\n",
       "      <td>0.3923</td>\n",
       "      <td>0.2007</td>\n",
       "      <td>0.2273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.6653</td>\n",
       "      <td>0.7583</td>\n",
       "      <td>0.8386</td>\n",
       "      <td>0.2651</td>\n",
       "      <td>0.2464</td>\n",
       "      <td>0.3184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SD</th>\n",
       "      <td>0.1776</td>\n",
       "      <td>0.4172</td>\n",
       "      <td>0.2346</td>\n",
       "      <td>0.4713</td>\n",
       "      <td>0.0606</td>\n",
       "      <td>0.1057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         MAE     MSE    RMSE      R2   RMSLE    MAPE\n",
       "0     0.7842  1.0789  1.0387 -0.5046  0.2901  0.2401\n",
       "1     0.5428  0.4740  0.6885  0.6422  0.2006  0.2496\n",
       "2     0.6078  0.7052  0.8398  0.4613  0.2515  0.3006\n",
       "3     0.8579  1.2575  1.1214 -0.0131  0.3469  0.5123\n",
       "4     0.4564  0.2942  0.5424  0.7268  0.1770  0.2271\n",
       "5     0.5534  0.3729  0.6107  0.6718  0.1767  0.2659\n",
       "6     0.7702  0.9070  0.9524  0.3134  0.2957  0.4486\n",
       "7     0.4655  0.3375  0.5809  0.6175  0.1997  0.2476\n",
       "8     1.0293  1.5964  1.2635 -0.6568  0.3252  0.4653\n",
       "9     0.5850  0.5592  0.7478  0.3923  0.2007  0.2273\n",
       "Mean  0.6653  0.7583  0.8386  0.2651  0.2464  0.3184\n",
       "SD    0.1776  0.4172  0.2346  0.4713  0.0606  0.1057"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = create_model('lr')\n",
    "rr = create_model('lasso')\n",
    "etr = create_model('et')\n",
    "svr = create_model('svm')\n",
    "adar = create_model('ada')\n",
    "mlpr = create_model('mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression simple sur scikit learn\n",
    "essai_classif = anglais[['Geography','Entities', 'Time', 'Narrative', 'Overall', 'Style', 'Tone','nb_entites_idem', \n",
    "    'nb_lieux_idem', 'nb_dates_idem', 'score_similarite_titres', 'score_similarite_resume1','score_similarite_resume2', \n",
    "    'score_classif1', 'score_classif2','score_sentiment1', 'score_sentiment2', 'score_sentiment3','meth1_similarites','meth2_similarites']]\n",
    "lr = LinearRegression()\n",
    "Xtrain = essai_classif[predicteurs].reset_index(drop=True)\n",
    "ytrain = essai_classif['Overall'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_entites_idem</th>\n",
       "      <th>nb_lieux_idem</th>\n",
       "      <th>nb_dates_idem</th>\n",
       "      <th>score_similarite_titres</th>\n",
       "      <th>score_similarite_resume1</th>\n",
       "      <th>score_similarite_resume2</th>\n",
       "      <th>score_classif1</th>\n",
       "      <th>score_classif2</th>\n",
       "      <th>score_sentiment1</th>\n",
       "      <th>score_sentiment2</th>\n",
       "      <th>score_sentiment3</th>\n",
       "      <th>meth1_similarites</th>\n",
       "      <th>meth2_similarites</th>\n",
       "      <th>Overall</th>\n",
       "      <th>LR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>16.92</td>\n",
       "      <td>13.18</td>\n",
       "      <td>12.12</td>\n",
       "      <td>55.78</td>\n",
       "      <td>8.34</td>\n",
       "      <td>20.98</td>\n",
       "      <td>6.82</td>\n",
       "      <td>97.85</td>\n",
       "      <td>526.2</td>\n",
       "      <td>408.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.975029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>15.11</td>\n",
       "      <td>30.88</td>\n",
       "      <td>9.29</td>\n",
       "      <td>32.23</td>\n",
       "      <td>8.32</td>\n",
       "      <td>24.34</td>\n",
       "      <td>98.09</td>\n",
       "      <td>96.14</td>\n",
       "      <td>419.6</td>\n",
       "      <td>421.1</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>2.655848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.68</td>\n",
       "      <td>4.31</td>\n",
       "      <td>5.74</td>\n",
       "      <td>10.86</td>\n",
       "      <td>8.51</td>\n",
       "      <td>35.78</td>\n",
       "      <td>98.09</td>\n",
       "      <td>99.48</td>\n",
       "      <td>367.1</td>\n",
       "      <td>453.2</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>3.587408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.08</td>\n",
       "      <td>2.45</td>\n",
       "      <td>3.82</td>\n",
       "      <td>10.04</td>\n",
       "      <td>8.33</td>\n",
       "      <td>20.46</td>\n",
       "      <td>61.80</td>\n",
       "      <td>79.66</td>\n",
       "      <td>253.3</td>\n",
       "      <td>327.2</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>3.704653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7.41</td>\n",
       "      <td>14.68</td>\n",
       "      <td>22.20</td>\n",
       "      <td>13.71</td>\n",
       "      <td>8.35</td>\n",
       "      <td>18.03</td>\n",
       "      <td>2.12</td>\n",
       "      <td>63.97</td>\n",
       "      <td>239.6</td>\n",
       "      <td>140.1</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>3.439632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.78</td>\n",
       "      <td>4.27</td>\n",
       "      <td>10.28</td>\n",
       "      <td>8.92</td>\n",
       "      <td>8.29</td>\n",
       "      <td>19.18</td>\n",
       "      <td>90.36</td>\n",
       "      <td>5.42</td>\n",
       "      <td>51.9</td>\n",
       "      <td>185.6</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.920088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.11</td>\n",
       "      <td>28.36</td>\n",
       "      <td>22.68</td>\n",
       "      <td>20.06</td>\n",
       "      <td>8.37</td>\n",
       "      <td>21.05</td>\n",
       "      <td>17.43</td>\n",
       "      <td>99.57</td>\n",
       "      <td>196.9</td>\n",
       "      <td>735.7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.996764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7.94</td>\n",
       "      <td>29.86</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>32.86</td>\n",
       "      <td>8.35</td>\n",
       "      <td>26.32</td>\n",
       "      <td>5.13</td>\n",
       "      <td>95.33</td>\n",
       "      <td>654.8</td>\n",
       "      <td>1324.8</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>2.447097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.93</td>\n",
       "      <td>13.98</td>\n",
       "      <td>11.86</td>\n",
       "      <td>13.83</td>\n",
       "      <td>8.32</td>\n",
       "      <td>21.20</td>\n",
       "      <td>14.38</td>\n",
       "      <td>99.43</td>\n",
       "      <td>179.4</td>\n",
       "      <td>129.4</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.289070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>35.48</td>\n",
       "      <td>31.25</td>\n",
       "      <td>7.64</td>\n",
       "      <td>16.50</td>\n",
       "      <td>8.35</td>\n",
       "      <td>21.61</td>\n",
       "      <td>89.30</td>\n",
       "      <td>99.00</td>\n",
       "      <td>78.8</td>\n",
       "      <td>559.6</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.454010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     nb_entites_idem  nb_lieux_idem  nb_dates_idem  score_similarite_titres  \\\n",
       "100               10              0              7                    16.92   \n",
       "101                5              0              2                    15.11   \n",
       "102                0              0              4                     4.68   \n",
       "103               12              0              4                     3.08   \n",
       "104                2              2              7                     7.41   \n",
       "..               ...            ...            ...                      ...   \n",
       "186                0              0              2                     1.78   \n",
       "187                7              0              0                    25.11   \n",
       "188                3              0              6                     7.94   \n",
       "189                0              0              0                    13.93   \n",
       "190               22              0              2                    35.48   \n",
       "\n",
       "     score_similarite_resume1  score_similarite_resume2  score_classif1  \\\n",
       "100                     13.18                     12.12           55.78   \n",
       "101                     30.88                      9.29           32.23   \n",
       "102                      4.31                      5.74           10.86   \n",
       "103                      2.45                      3.82           10.04   \n",
       "104                     14.68                     22.20           13.71   \n",
       "..                        ...                       ...             ...   \n",
       "186                      4.27                     10.28            8.92   \n",
       "187                     28.36                     22.68           20.06   \n",
       "188                     29.86                     -1.50           32.86   \n",
       "189                     13.98                     11.86           13.83   \n",
       "190                     31.25                      7.64           16.50   \n",
       "\n",
       "     score_classif2  score_sentiment1  score_sentiment2  score_sentiment3  \\\n",
       "100            8.34             20.98              6.82             97.85   \n",
       "101            8.32             24.34             98.09             96.14   \n",
       "102            8.51             35.78             98.09             99.48   \n",
       "103            8.33             20.46             61.80             79.66   \n",
       "104            8.35             18.03              2.12             63.97   \n",
       "..              ...               ...               ...               ...   \n",
       "186            8.29             19.18             90.36              5.42   \n",
       "187            8.37             21.05             17.43             99.57   \n",
       "188            8.35             26.32              5.13             95.33   \n",
       "189            8.32             21.20             14.38             99.43   \n",
       "190            8.35             21.61             89.30             99.00   \n",
       "\n",
       "    meth1_similarites meth2_similarites   Overall        LR  \n",
       "100             526.2             408.0  3.000000  2.975029  \n",
       "101             419.6             421.1  1.333333  2.655848  \n",
       "102             367.1             453.2  3.666667  3.587408  \n",
       "103             253.3             327.2  3.666667  3.704653  \n",
       "104             239.6             140.1  3.666667  3.439632  \n",
       "..                ...               ...       ...       ...  \n",
       "186              51.9             185.6  4.000000  3.920088  \n",
       "187             196.9             735.7  1.000000  1.996764  \n",
       "188             654.8            1324.8  2.333333  2.447097  \n",
       "189             179.4             129.4  2.666667  3.289070  \n",
       "190              78.8             559.6  1.333333  1.454010  \n",
       "\n",
       "[91 rows x 15 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(Xtrain[:taille_train],ytrain[:taille_train])\n",
    "res_lr = lr.predict(Xtrain[taille_train:])\n",
    "res_lr = pd.concat([Xtrain[taille_train:],ytrain[taille_train:],pd.DataFrame(res_lr,columns = ['LR'],index = range(taille_train,dernier_test))],axis=1)\n",
    "res_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_final = pd.concat([res_final,res_lr['LR']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_entites_idem</th>\n",
       "      <th>nb_lieux_idem</th>\n",
       "      <th>nb_dates_idem</th>\n",
       "      <th>score_similarite_titres</th>\n",
       "      <th>score_similarite_resume1</th>\n",
       "      <th>score_similarite_resume2</th>\n",
       "      <th>score_classif1</th>\n",
       "      <th>score_classif2</th>\n",
       "      <th>score_sentiment1</th>\n",
       "      <th>score_sentiment2</th>\n",
       "      <th>score_sentiment3</th>\n",
       "      <th>meth1_similarites</th>\n",
       "      <th>meth2_similarites</th>\n",
       "      <th>Overall</th>\n",
       "      <th>LR</th>\n",
       "      <th>PLS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>16.92</td>\n",
       "      <td>13.18</td>\n",
       "      <td>12.12</td>\n",
       "      <td>55.78</td>\n",
       "      <td>8.34</td>\n",
       "      <td>20.98</td>\n",
       "      <td>6.82</td>\n",
       "      <td>97.85</td>\n",
       "      <td>526.2</td>\n",
       "      <td>408.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.975029</td>\n",
       "      <td>2.236149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>15.11</td>\n",
       "      <td>30.88</td>\n",
       "      <td>9.29</td>\n",
       "      <td>32.23</td>\n",
       "      <td>8.32</td>\n",
       "      <td>24.34</td>\n",
       "      <td>98.09</td>\n",
       "      <td>96.14</td>\n",
       "      <td>419.6</td>\n",
       "      <td>421.1</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>2.655848</td>\n",
       "      <td>2.305216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.68</td>\n",
       "      <td>4.31</td>\n",
       "      <td>5.74</td>\n",
       "      <td>10.86</td>\n",
       "      <td>8.51</td>\n",
       "      <td>35.78</td>\n",
       "      <td>98.09</td>\n",
       "      <td>99.48</td>\n",
       "      <td>367.1</td>\n",
       "      <td>453.2</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>3.587408</td>\n",
       "      <td>3.555531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.08</td>\n",
       "      <td>2.45</td>\n",
       "      <td>3.82</td>\n",
       "      <td>10.04</td>\n",
       "      <td>8.33</td>\n",
       "      <td>20.46</td>\n",
       "      <td>61.80</td>\n",
       "      <td>79.66</td>\n",
       "      <td>253.3</td>\n",
       "      <td>327.2</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>3.704653</td>\n",
       "      <td>3.539568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7.41</td>\n",
       "      <td>14.68</td>\n",
       "      <td>22.20</td>\n",
       "      <td>13.71</td>\n",
       "      <td>8.35</td>\n",
       "      <td>18.03</td>\n",
       "      <td>2.12</td>\n",
       "      <td>63.97</td>\n",
       "      <td>239.6</td>\n",
       "      <td>140.1</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>3.439632</td>\n",
       "      <td>3.312935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.78</td>\n",
       "      <td>4.27</td>\n",
       "      <td>10.28</td>\n",
       "      <td>8.92</td>\n",
       "      <td>8.29</td>\n",
       "      <td>19.18</td>\n",
       "      <td>90.36</td>\n",
       "      <td>5.42</td>\n",
       "      <td>51.9</td>\n",
       "      <td>185.6</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.920088</td>\n",
       "      <td>3.815560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.11</td>\n",
       "      <td>28.36</td>\n",
       "      <td>22.68</td>\n",
       "      <td>20.06</td>\n",
       "      <td>8.37</td>\n",
       "      <td>21.05</td>\n",
       "      <td>17.43</td>\n",
       "      <td>99.57</td>\n",
       "      <td>196.9</td>\n",
       "      <td>735.7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.996764</td>\n",
       "      <td>2.014655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7.94</td>\n",
       "      <td>29.86</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>32.86</td>\n",
       "      <td>8.35</td>\n",
       "      <td>26.32</td>\n",
       "      <td>5.13</td>\n",
       "      <td>95.33</td>\n",
       "      <td>654.8</td>\n",
       "      <td>1324.8</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>2.447097</td>\n",
       "      <td>2.151264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.93</td>\n",
       "      <td>13.98</td>\n",
       "      <td>11.86</td>\n",
       "      <td>13.83</td>\n",
       "      <td>8.32</td>\n",
       "      <td>21.20</td>\n",
       "      <td>14.38</td>\n",
       "      <td>99.43</td>\n",
       "      <td>179.4</td>\n",
       "      <td>129.4</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.289070</td>\n",
       "      <td>3.340806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>35.48</td>\n",
       "      <td>31.25</td>\n",
       "      <td>7.64</td>\n",
       "      <td>16.50</td>\n",
       "      <td>8.35</td>\n",
       "      <td>21.61</td>\n",
       "      <td>89.30</td>\n",
       "      <td>99.00</td>\n",
       "      <td>78.8</td>\n",
       "      <td>559.6</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.454010</td>\n",
       "      <td>1.467984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     nb_entites_idem  nb_lieux_idem  nb_dates_idem  score_similarite_titres  \\\n",
       "100               10              0              7                    16.92   \n",
       "101                5              0              2                    15.11   \n",
       "102                0              0              4                     4.68   \n",
       "103               12              0              4                     3.08   \n",
       "104                2              2              7                     7.41   \n",
       "..               ...            ...            ...                      ...   \n",
       "186                0              0              2                     1.78   \n",
       "187                7              0              0                    25.11   \n",
       "188                3              0              6                     7.94   \n",
       "189                0              0              0                    13.93   \n",
       "190               22              0              2                    35.48   \n",
       "\n",
       "     score_similarite_resume1  score_similarite_resume2  score_classif1  \\\n",
       "100                     13.18                     12.12           55.78   \n",
       "101                     30.88                      9.29           32.23   \n",
       "102                      4.31                      5.74           10.86   \n",
       "103                      2.45                      3.82           10.04   \n",
       "104                     14.68                     22.20           13.71   \n",
       "..                        ...                       ...             ...   \n",
       "186                      4.27                     10.28            8.92   \n",
       "187                     28.36                     22.68           20.06   \n",
       "188                     29.86                     -1.50           32.86   \n",
       "189                     13.98                     11.86           13.83   \n",
       "190                     31.25                      7.64           16.50   \n",
       "\n",
       "     score_classif2  score_sentiment1  score_sentiment2  score_sentiment3  \\\n",
       "100            8.34             20.98              6.82             97.85   \n",
       "101            8.32             24.34             98.09             96.14   \n",
       "102            8.51             35.78             98.09             99.48   \n",
       "103            8.33             20.46             61.80             79.66   \n",
       "104            8.35             18.03              2.12             63.97   \n",
       "..              ...               ...               ...               ...   \n",
       "186            8.29             19.18             90.36              5.42   \n",
       "187            8.37             21.05             17.43             99.57   \n",
       "188            8.35             26.32              5.13             95.33   \n",
       "189            8.32             21.20             14.38             99.43   \n",
       "190            8.35             21.61             89.30             99.00   \n",
       "\n",
       "    meth1_similarites meth2_similarites   Overall        LR       PLS  \n",
       "100             526.2             408.0  3.000000  2.975029  2.236149  \n",
       "101             419.6             421.1  1.333333  2.655848  2.305216  \n",
       "102             367.1             453.2  3.666667  3.587408  3.555531  \n",
       "103             253.3             327.2  3.666667  3.704653  3.539568  \n",
       "104             239.6             140.1  3.666667  3.439632  3.312935  \n",
       "..                ...               ...       ...       ...       ...  \n",
       "186              51.9             185.6  4.000000  3.920088  3.815560  \n",
       "187             196.9             735.7  1.000000  1.996764  2.014655  \n",
       "188             654.8            1324.8  2.333333  2.447097  2.151264  \n",
       "189             179.4             129.4  2.666667  3.289070  3.340806  \n",
       "190              78.8             559.6  1.333333  1.454010  1.467984  \n",
       "\n",
       "[91 rows x 16 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "pls = PLSRegression()\n",
    "pls.fit(Xtrain[:taille_train],ytrain[:taille_train])\n",
    "res_pls = list(pls.predict(Xtrain[taille_train:]).flatten())\n",
    "res_pls = pd.concat([res_lr,pd.DataFrame(res_pls,columns = ['PLS'],index = range(taille_train,dernier_test))],axis=1)\n",
    "res_pls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_final = pd.concat([res_final,res_pls['PLS']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall</th>\n",
       "      <th>RF</th>\n",
       "      <th>LDA</th>\n",
       "      <th>KNN</th>\n",
       "      <th>LOGR</th>\n",
       "      <th>ADA</th>\n",
       "      <th>RF_Prob</th>\n",
       "      <th>LDA_Prob</th>\n",
       "      <th>KNN_Prob</th>\n",
       "      <th>LOGR_Prob</th>\n",
       "      <th>ADA_Prob</th>\n",
       "      <th>LR</th>\n",
       "      <th>PLS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.975029</td>\n",
       "      <td>2.236149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2.655848</td>\n",
       "      <td>2.305216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3.587408</td>\n",
       "      <td>3.555531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.704653</td>\n",
       "      <td>3.539568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.439632</td>\n",
       "      <td>3.312935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.920088</td>\n",
       "      <td>3.815560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.996764</td>\n",
       "      <td>2.014655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.447097</td>\n",
       "      <td>2.151264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.289070</td>\n",
       "      <td>3.340806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.454010</td>\n",
       "      <td>1.467984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Overall  RF  LDA  KNN  LOGR  ADA  RF_Prob  LDA_Prob  KNN_Prob  LOGR_Prob  \\\n",
       "100        3   2    3    2     3    2        2         3         3          3   \n",
       "101        1   3    3    4     3    4        3         3         4          4   \n",
       "102        4   4    4    2     4    2        4         4         2          2   \n",
       "103        4   4    4    2     4    2        4         4         3          3   \n",
       "104        4   4    4    4     4    4        4         4         4          4   \n",
       "..       ...  ..  ...  ...   ...  ...      ...       ...       ...        ...   \n",
       "186        4   4    4    4     4    4        4         4         4          4   \n",
       "187        1   2    2    2     2    2        2         2         2          2   \n",
       "188        2   3    2    2     2    2        3         2         2          2   \n",
       "189        3   3    4    4     3    4        3         4         4          4   \n",
       "190        1   1    1    4     1    4        1         1         3          3   \n",
       "\n",
       "     ADA_Prob        LR       PLS  \n",
       "100         3  2.975029  2.236149  \n",
       "101         4  2.655848  2.305216  \n",
       "102         2  3.587408  3.555531  \n",
       "103         3  3.704653  3.539568  \n",
       "104         4  3.439632  3.312935  \n",
       "..        ...       ...       ...  \n",
       "186         4  3.920088  3.815560  \n",
       "187         2  1.996764  2.014655  \n",
       "188         2  2.447097  2.151264  \n",
       "189         4  3.289070  3.340806  \n",
       "190         3  1.454010  1.467984  \n",
       "\n",
       "[91 rows x 13 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pour le classement final utiliser aussi la stade de base des notations : si plus de 4 ....** <br/>\n",
    "**Pour le seuils : Le mieux est d'utiliser un algo qui regrade les erreurs des autres ... NB / LR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on ne garde finalement que LDA - RF et LR en classif et sans les probas qui changent peu finalement\n",
    "def choix_final(df):\n",
    "    res = pd.DataFrame(columns = ['Overall','Classif_value','Regression_value','Classif','Regression','Final','Final_aide'])\n",
    "    for i in range(df.index[0],df.index[len(df)-1]):\n",
    "        my_dic = {}\n",
    "        my_dic['Overall'] = df.Overall[i]                   \n",
    "        my_dic['Classif_value'] = round((df.RF[i] + df.LDA[i] + df.LOGR[i])/3,2)\n",
    "        my_dic['Regression_value'] = round((df.LR[i] + df.PLS[i])/2,2)\n",
    "        if my_dic['Classif_value'] < 1.7:\n",
    "            my_dic['Classif'] = 1\n",
    "        elif my_dic['Classif_value'] < 2.5:\n",
    "            my_dic['Classif'] = 2\n",
    "        elif my_dic['Classif_value'] < 3.25:\n",
    "            my_dic['Classif'] = 3\n",
    "        else:\n",
    "            my_dic['Classif'] = 4\n",
    "        if my_dic['Regression_value'] <= 1.6:\n",
    "            my_dic['Regression'] = 1\n",
    "        elif my_dic['Regression_value'] < 2.5:\n",
    "            my_dic['Regression'] = 2\n",
    "        elif my_dic['Regression_value'] < 3.5:\n",
    "            my_dic['Regression'] = 3\n",
    "        else:\n",
    "            my_dic['Regression'] = 4\n",
    "        diff = 0.01 if my_dic['Classif'] != my_dic['Regression'] else 0\n",
    "        diff += 0.005 if abs(my_dic['Classif']-my_dic['Regression'])>=2 else 0\n",
    "        my_dic['Final'] = round((my_dic['Classif']+my_dic['Regression'])/2,0)\n",
    "        my_dic['Final_aide'] = round((my_dic['Classif']+my_dic['Regression'])/2,0)+diff\n",
    "        res.loc[len(res)] = my_dic\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall</th>\n",
       "      <th>Classif_value</th>\n",
       "      <th>Regression_value</th>\n",
       "      <th>Classif</th>\n",
       "      <th>Regression</th>\n",
       "      <th>Final</th>\n",
       "      <th>Final_aide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.67</td>\n",
       "      <td>2.61</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.48</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.57</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.62</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.38</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.85</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.87</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.33</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.33</td>\n",
       "      <td>3.31</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Overall  Classif_value  Regression_value  Classif  Regression  Final  \\\n",
       "0       3.0           2.67              2.61      3.0         3.0    3.0   \n",
       "1       1.0           3.00              2.48      3.0         2.0    2.0   \n",
       "2       4.0           4.00              3.57      4.0         4.0    4.0   \n",
       "3       4.0           4.00              3.62      4.0         4.0    4.0   \n",
       "4       4.0           4.00              3.38      4.0         3.0    4.0   \n",
       "..      ...            ...               ...      ...         ...    ...   \n",
       "85      4.0           4.00              3.85      4.0         4.0    4.0   \n",
       "86      4.0           4.00              3.87      4.0         4.0    4.0   \n",
       "87      1.0           2.00              2.01      2.0         2.0    2.0   \n",
       "88      2.0           2.33              2.30      2.0         2.0    2.0   \n",
       "89      3.0           3.33              3.31      4.0         3.0    4.0   \n",
       "\n",
       "    Final_aide  \n",
       "0         3.00  \n",
       "1         2.01  \n",
       "2         4.00  \n",
       "3         4.00  \n",
       "4         4.01  \n",
       "..         ...  \n",
       "85        4.00  \n",
       "86        4.00  \n",
       "87        2.00  \n",
       "88        2.00  \n",
       "89        4.01  \n",
       "\n",
       "[90 rows x 7 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vote = choix_final(res_final)\n",
    "vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7,  5,  0,  0],\n",
       "       [ 5,  4,  5,  0],\n",
       "       [ 0,  4, 10,  6],\n",
       "       [ 0,  2, 12, 30]], dtype=int64)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plutôt moins bon que la classif : peut etre question de seuil ...\n",
    "confusion_matrix(vote.Overall,vote.Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7,  5,  0,  0],\n",
       "       [ 3,  6,  2,  3],\n",
       "       [ 0,  4,  3, 13],\n",
       "       [ 0,  4,  1, 39]], dtype=int64)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ne sert à rien pratiquement, mais seuils à voir\n",
    "confusion_matrix(vote.Overall,vote.Final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMARQUE GENERALE : IL Y A BCP DE 4 ce qui aide beaucoup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base de classification en Allemand\n",
    "\n",
    "    - Transformers : Summarization : 2 modèles --> 2 Résumés / Puis score de similarités de ces 2 résumés\n",
    "    Noter que l'on peut faire aussi la similarité des textes (autre note ?) et non du résumé\n",
    "    - Text classification sur une base de catégories \"Presse\" \n",
    "    - Sentiment analysis : voir si le ton du texte est de même type \n",
    "    - Les 2 derniers classifier seronts utilisés en produit scalaire : Par Catégorie : texte1: note1 - texte2 : note2\n",
    "    et donc sum(notes_par_catégorie) = sum(note1*note2) * 100 au bout (note sur 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stg-sdu\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\stg-sdu\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.QVLO2T66WEPI7JZ63PS3HMOHFEY472BC.gfortran-win_amd64.dll\n",
      "C:\\Users\\stg-sdu\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pke\n",
    "import spacy\n",
    "import torch\n",
    "import stanza\n",
    "import spacy_stanza\n",
    "import warnings\n",
    "import string\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import enchant    # Pour correction orthographique de synonymes\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "tqdm.pandas()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selection des modèles NLP : ici ALLEMAND**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement pour l'utilisation de Spacy  - Français\n",
    "nlp_de = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_spacy = {'de':nlp_de}   # 'en':nlp_en,'de':nlp_de,'es':nlp_es,'pl':nlp_pl  - POUR MEMOIRE\n",
    "langues = ['en','fr','es','de','pl','ar','tr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du modèle Word2Vec pour utilisation de synonymes\n",
    "from gensim.models import Word2Vec\n",
    "model_gensim = gensim.models.KeyedVectors.load_word2vec_format(\"D:/Users/STG-SDU/Documents/NLP/german.model\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords Français NLTK + Spacy \n",
    "stopWords = list(nlp_de.Defaults.stop_words)\n",
    "stopwords_de = list(stopwords.words('german'))  \n",
    "stopwords_de = list(set(stopwords_de + stopWords))\n",
    "stopwds_lg = {'de':stopwords_de}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correcteur orthographique pour validation des synonymes OPTIONNEL CAR NON NECESSAIRE\n",
    "# d = enchant.Dict(\"de\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sélection des modèles Transformers : Summary - Text Classification - Sentiment Analysis - Similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèles Transformers de Résumé (NB : Ne pas oublier d'ajouter la truncation pour tous les modèles, peut être source d'erreur)\n",
    "summarizer1 = pipeline(\"summarization\", model=\"ml6team/mt5-small-german-finetune-mlsum\", truncation = \"only_first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n"
     ]
    }
   ],
   "source": [
    "# 2e résumé\n",
    "from transformers import BertTokenizerFast, EncoderDecoderModel\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ckpt = 'mrm8488/bert2bert_shared-german-finetuned-summarization'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(ckpt)\n",
    "model = EncoderDecoderModel.from_pretrained(ckpt).to(device)\n",
    "def summarizer2(text):\n",
    "    inputs = tokenizer([text], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "    output = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes : politics, economy, entertainment, environment,sport,health\n",
    "text_clf1 = pipeline(\"zero-shot-classification\", model = \"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\", truncation = \"only_first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero shot classification (permet de chosir nos propres thèmes)\n",
    "text_clf2 = pipeline('zero-shot-classification', model=\"Sahajtomar/German_Zeroshot\",truncation = \"only_first\")\n",
    "# ce modèle est un zero shot classification : catégories possibles choisies par mes soins (dans la presse)\n",
    "candidate_labels = ['Wissenschaft','Politik','Bildung','Nachrichten','Gesundheit','Technologie','Gesellschaft','Sport','Wirtschaft','Kultur','International','Umwelt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "sentiment1 = pipeline(\"text-classification\", model = 'oliverguhr/german-sentiment-bert', truncation = \"only_first\")\n",
    "# ATTENTION CE MODELE n°2 SE DEFINIT SUR 5 niveaux\n",
    "sentiment2 = pipeline(\"text-classification\", model = 'nlptown/bert-base-multilingual-uncased-sentiment', truncation = \"only_first\")\n",
    "# on prend un  3e niveau de classification\n",
    "sentiment3 = pipeline(\"text-classification\", model = 'symanto/xlm-roberta-base-snli-mnli-anli-xnli', truncation = \"only_first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODAGE AVEC SENTENCE TRANSFORMER\n",
    "from sentence_transformers import SentenceTransformer,util\n",
    "encoder = SentenceTransformer(\"Sahajtomar/German-semantic\")\n",
    "encoder2 = SentenceTransformer(\"symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli\")\n",
    "def score_similarite(sentence1,sentence2):\n",
    "    # attention, pour que torch fonctionne en dimension sentence1 (et 2) est une liste simple\n",
    "    embed1 = encoder.encode(sentence1, convert_to_tensor=True)\n",
    "    embed2 = encoder.encode(sentence2, convert_to_tensor=True)\n",
    "    embed3 = encoder2.encode(sentence1, convert_to_tensor=True)\n",
    "    embed4 = encoder2.encode(sentence2, convert_to_tensor=True)\n",
    "    return round(float(util.pytorch_cos_sim(embed1,embed2))+float(util.pytorch_cos_sim(embed3,embed4))*100/2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selection Data par langues**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('eval_data_prep_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id</th>\n",
       "      <th>pair_lang</th>\n",
       "      <th>source_url_1</th>\n",
       "      <th>publish_date_1</th>\n",
       "      <th>source_url_2</th>\n",
       "      <th>publish_date_2</th>\n",
       "      <th>title_1</th>\n",
       "      <th>text_1</th>\n",
       "      <th>meta_description_1</th>\n",
       "      <th>meta_keywords_1</th>\n",
       "      <th>title_2</th>\n",
       "      <th>text_2</th>\n",
       "      <th>meta_description_2</th>\n",
       "      <th>meta_keywords_2</th>\n",
       "      <th>ligne</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1484189203_1484121193</td>\n",
       "      <td>en_en</td>\n",
       "      <td>https://wsvn.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://wsvn.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police: 2 men stole tools from Lowe’s in Davie</td>\n",
       "      <td>DAVIE, FLA. (WSVN) - Police need help catching...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>No-swim advisory lifted for Deerfield Beach Pier</td>\n",
       "      <td>DEERFIELD BEACH, FLA. (WSVN) - A no-swim advis...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1484011097_1484011106</td>\n",
       "      <td>en_en</td>\n",
       "      <td>https://www.zdnet.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://securityboulevard.com</td>\n",
       "      <td>Fri Oct 25 11:10:18 2019</td>\n",
       "      <td>Open database leaked 179GB in customer, US gov...</td>\n",
       "      <td>Govt officials confirm Trump can block US comp...</td>\n",
       "      <td>The US Department of Homeland Security has bec...</td>\n",
       "      <td>['']</td>\n",
       "      <td>Best Western’s Massive Data Leak: 179GB Amazon...</td>\n",
       "      <td>The latest huge unsecured cloud storage find i...</td>\n",
       "      <td>The latest huge unsecured cloud storage find i...</td>\n",
       "      <td>['']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1484039488_1484261803</td>\n",
       "      <td>en_en</td>\n",
       "      <td>https://www.presstelegram.com</td>\n",
       "      <td>Tue Dec 31 00:00:00 2019</td>\n",
       "      <td>https://boingboing.net</td>\n",
       "      <td>Wed Jan  1 00:00:00 2020</td>\n",
       "      <td>Ducks are own worst enemies in sloppy loss in ...</td>\n",
       "      <td>Ducks defenseman Erik Gudbranson, left, knocks...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>Woody Guthrie's 1943 New Year's Resolutions ar...</td>\n",
       "      <td>Woody Guthrie's 1943 New Year's Resolutions ar...</td>\n",
       "      <td>I'd seen this before, but I was reminded of it...</td>\n",
       "      <td>['']</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1484332324_1484796748</td>\n",
       "      <td>en_en</td>\n",
       "      <td>https://www.financialexpress.com</td>\n",
       "      <td>Thu Jan  2 08:28:22 2020</td>\n",
       "      <td>https://www.news18.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Another Bengal vs Centre tussle? Govt rejects ...</td>\n",
       "      <td>The West Bengal government’s proposal was reje...</td>\n",
       "      <td>The West Bengal government's proposal was reje...</td>\n",
       "      <td>['republic day', 'west bengal tableau', 'benga...</td>\n",
       "      <td>'Congress Rejected 7 Times': BJP's Reminder as...</td>\n",
       "      <td>Mumbai: The NCP and Shiv Sena on Thursday targ...</td>\n",
       "      <td>BJP ally and Union minister Ramdas Athawale sa...</td>\n",
       "      <td>['BJP', 'congress', 'Mamata Banerjee', 'NCP', ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1484012256_1484419682</td>\n",
       "      <td>en_en</td>\n",
       "      <td>https://www.birminghammail.co.uk</td>\n",
       "      <td>Wed Jan  1 15:03:04 2020</td>\n",
       "      <td>http://m.fightbacknews.org</td>\n",
       "      <td>Wed Jan  1 00:00:00 2020</td>\n",
       "      <td>Bars and clubs you loved and lost this decade ...</td>\n",
       "      <td>The video will start in 8 Cancel\\n\\nSign up to...</td>\n",
       "      <td>Nightclubs and bars that have closed in the pa...</td>\n",
       "      <td>['Birmingham City Centre', 'Digbeth', 'Things ...</td>\n",
       "      <td>Top 20 films of the 2010s</td>\n",
       "      <td>Jacksonville, FL - I'm not sure how we'll look...</td>\n",
       "      <td>Jacksonville, FL - I'm not sure how we'll look...</td>\n",
       "      <td>['organizing', 'activism', 'socialism', \"Peopl...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4948</th>\n",
       "      <td>1553907621_1553488848</td>\n",
       "      <td>es_it</td>\n",
       "      <td>https://www.diariolibre.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.basketuniverso.it</td>\n",
       "      <td>Thu Mar 19 23:45:49 2020</td>\n",
       "      <td>Denver Nuggets reporta que “un miembro de la o...</td>\n",
       "      <td>Los Denver Nuggets de la NBA reportaron este j...</td>\n",
       "      <td>Los Denver Nuggets de la NBA reportaron este j...</td>\n",
       "      <td>['NBA']</td>\n",
       "      <td>Coronavirus, un caso anche fra i Denver Nuggets</td>\n",
       "      <td>Nato ad Alatri (Fr) nel ’93 e qui diplomato al...</td>\n",
       "      <td>Un altro caso di Coronavirus nella NBA, stavol...</td>\n",
       "      <td>['']</td>\n",
       "      <td>4948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4949</th>\n",
       "      <td>1646957948_1643667075</td>\n",
       "      <td>es_it</td>\n",
       "      <td>https://diario16.com</td>\n",
       "      <td>Sun Jun 28 04:20:00 2020</td>\n",
       "      <td>https://www.laregione.ch</td>\n",
       "      <td>Wed Jun 24 16:41:00 2020</td>\n",
       "      <td>Vivir en España es más barato que en la media ...</td>\n",
       "      <td>El estudio realizado por Eurostat muestra que ...</td>\n",
       "      <td>El estudio realizado por Eurostat muestra que ...</td>\n",
       "      <td>['']</td>\n",
       "      <td>Coronavirus, in Europa 140mila morti in più in...</td>\n",
       "      <td>Nei mesi di marzo e aprile 2020, dalla decima ...</td>\n",
       "      <td>Il picco di morti aggiuntivi rispetto alla med...</td>\n",
       "      <td>['']</td>\n",
       "      <td>4949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4950</th>\n",
       "      <td>1504063453_1502866628</td>\n",
       "      <td>es_it</td>\n",
       "      <td>https://elaragueno.com.ve</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.greenme.it</td>\n",
       "      <td>Thu Jan 23 12:46:02 2020</td>\n",
       "      <td>Activan sistema de vigilancia epidemiológica e...</td>\n",
       "      <td>Foto: Archivo Foto: Archivo\\n\\nEl sistema de v...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>Coronavirus in Cina: consumo di serpenti e zup...</td>\n",
       "      <td>La diffusione del mortale Coronavirus potrebbe...</td>\n",
       "      <td>L'infezione da coronavirus potrebbe aver avuto...</td>\n",
       "      <td>['cina', 'coronavirus', 'pipistrelli', 'serpen...</td>\n",
       "      <td>4950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4951</th>\n",
       "      <td>1647862428_1647712939</td>\n",
       "      <td>es_it</td>\n",
       "      <td>http://www.am.com.mx</td>\n",
       "      <td>Mon Jun 29 00:00:00 2020</td>\n",
       "      <td>https://it.sputniknews.com</td>\n",
       "      <td>Mon Jun 29 13:46:00 2020</td>\n",
       "      <td>Emite Irán orden de arresto contra Tump por as...</td>\n",
       "      <td>CDMX.- Anunció Irán este lunes que ha emitido ...</td>\n",
       "      <td>Ha emitido Irán una orden de arresto, que ha s...</td>\n",
       "      <td>['DONALD TRUMP', 'ESTADOS UNIDOS', 'IRÁN']</td>\n",
       "      <td>Iran emette mandato di arresto per Trump per l...</td>\n",
       "      <td>Il procuratore di Teheran Ali Alqasi Mehr in u...</td>\n",
       "      <td>Il procuratore di Teheran, Ali Alqasi Mehr, ha...</td>\n",
       "      <td>['mondo', 'qasem soleimani', \"le tensioni tra ...</td>\n",
       "      <td>4951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4952</th>\n",
       "      <td>1636570015_1637492652</td>\n",
       "      <td>es_it</td>\n",
       "      <td>https://elnuevodiario.com.do</td>\n",
       "      <td>Tue Jun 16 18:49:57 2020</td>\n",
       "      <td>https://it.notizie.yahoo.com</td>\n",
       "      <td>Wed Jun 17 16:44:25 2020</td>\n",
       "      <td>Biden, con más de 10 puntos que Trump en estad...</td>\n",
       "      <td>Comparte esta noticia\\n\\nEL NUEVO DIARIO,MIAMI...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>Sondaggio Usa 2020: Biden in vantaggio nei 6 S...</td>\n",
       "      <td>New York, 17 giu. (askanews) - L'ex vicepresid...</td>\n",
       "      <td>Per la prima volta, Trump  in ritardo in tutti...</td>\n",
       "      <td>['']</td>\n",
       "      <td>4952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4953 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    pair_id pair_lang                      source_url_1  \\\n",
       "0     1484189203_1484121193     en_en                  https://wsvn.com   \n",
       "1     1484011097_1484011106     en_en             https://www.zdnet.com   \n",
       "2     1484039488_1484261803     en_en     https://www.presstelegram.com   \n",
       "3     1484332324_1484796748     en_en  https://www.financialexpress.com   \n",
       "4     1484012256_1484419682     en_en  https://www.birminghammail.co.uk   \n",
       "...                     ...       ...                               ...   \n",
       "4948  1553907621_1553488848     es_it       https://www.diariolibre.com   \n",
       "4949  1646957948_1643667075     es_it              https://diario16.com   \n",
       "4950  1504063453_1502866628     es_it         https://elaragueno.com.ve   \n",
       "4951  1647862428_1647712939     es_it              http://www.am.com.mx   \n",
       "4952  1636570015_1637492652     es_it      https://elnuevodiario.com.do   \n",
       "\n",
       "                publish_date_1                   source_url_2  \\\n",
       "0                          NaN               https://wsvn.com   \n",
       "1                          NaN  https://securityboulevard.com   \n",
       "2     Tue Dec 31 00:00:00 2019         https://boingboing.net   \n",
       "3     Thu Jan  2 08:28:22 2020         https://www.news18.com   \n",
       "4     Wed Jan  1 15:03:04 2020     http://m.fightbacknews.org   \n",
       "...                        ...                            ...   \n",
       "4948                       NaN  https://www.basketuniverso.it   \n",
       "4949  Sun Jun 28 04:20:00 2020       https://www.laregione.ch   \n",
       "4950                       NaN         https://www.greenme.it   \n",
       "4951  Mon Jun 29 00:00:00 2020     https://it.sputniknews.com   \n",
       "4952  Tue Jun 16 18:49:57 2020   https://it.notizie.yahoo.com   \n",
       "\n",
       "                publish_date_2  \\\n",
       "0                          NaN   \n",
       "1     Fri Oct 25 11:10:18 2019   \n",
       "2     Wed Jan  1 00:00:00 2020   \n",
       "3                          NaN   \n",
       "4     Wed Jan  1 00:00:00 2020   \n",
       "...                        ...   \n",
       "4948  Thu Mar 19 23:45:49 2020   \n",
       "4949  Wed Jun 24 16:41:00 2020   \n",
       "4950  Thu Jan 23 12:46:02 2020   \n",
       "4951  Mon Jun 29 13:46:00 2020   \n",
       "4952  Wed Jun 17 16:44:25 2020   \n",
       "\n",
       "                                                title_1  \\\n",
       "0        Police: 2 men stole tools from Lowe’s in Davie   \n",
       "1     Open database leaked 179GB in customer, US gov...   \n",
       "2     Ducks are own worst enemies in sloppy loss in ...   \n",
       "3     Another Bengal vs Centre tussle? Govt rejects ...   \n",
       "4     Bars and clubs you loved and lost this decade ...   \n",
       "...                                                 ...   \n",
       "4948  Denver Nuggets reporta que “un miembro de la o...   \n",
       "4949  Vivir en España es más barato que en la media ...   \n",
       "4950  Activan sistema de vigilancia epidemiológica e...   \n",
       "4951  Emite Irán orden de arresto contra Tump por as...   \n",
       "4952  Biden, con más de 10 puntos que Trump en estad...   \n",
       "\n",
       "                                                 text_1  \\\n",
       "0     DAVIE, FLA. (WSVN) - Police need help catching...   \n",
       "1     Govt officials confirm Trump can block US comp...   \n",
       "2     Ducks defenseman Erik Gudbranson, left, knocks...   \n",
       "3     The West Bengal government’s proposal was reje...   \n",
       "4     The video will start in 8 Cancel\\n\\nSign up to...   \n",
       "...                                                 ...   \n",
       "4948  Los Denver Nuggets de la NBA reportaron este j...   \n",
       "4949  El estudio realizado por Eurostat muestra que ...   \n",
       "4950  Foto: Archivo Foto: Archivo\\n\\nEl sistema de v...   \n",
       "4951  CDMX.- Anunció Irán este lunes que ha emitido ...   \n",
       "4952  Comparte esta noticia\\n\\nEL NUEVO DIARIO,MIAMI...   \n",
       "\n",
       "                                     meta_description_1  \\\n",
       "0                                                   NaN   \n",
       "1     The US Department of Homeland Security has bec...   \n",
       "2                                                   NaN   \n",
       "3     The West Bengal government's proposal was reje...   \n",
       "4     Nightclubs and bars that have closed in the pa...   \n",
       "...                                                 ...   \n",
       "4948  Los Denver Nuggets de la NBA reportaron este j...   \n",
       "4949  El estudio realizado por Eurostat muestra que ...   \n",
       "4950                                                NaN   \n",
       "4951  Ha emitido Irán una orden de arresto, que ha s...   \n",
       "4952                                                NaN   \n",
       "\n",
       "                                        meta_keywords_1  \\\n",
       "0                                                  ['']   \n",
       "1                                                  ['']   \n",
       "2                                                  ['']   \n",
       "3     ['republic day', 'west bengal tableau', 'benga...   \n",
       "4     ['Birmingham City Centre', 'Digbeth', 'Things ...   \n",
       "...                                                 ...   \n",
       "4948                                            ['NBA']   \n",
       "4949                                               ['']   \n",
       "4950                                               ['']   \n",
       "4951         ['DONALD TRUMP', 'ESTADOS UNIDOS', 'IRÁN']   \n",
       "4952                                               ['']   \n",
       "\n",
       "                                                title_2  \\\n",
       "0      No-swim advisory lifted for Deerfield Beach Pier   \n",
       "1     Best Western’s Massive Data Leak: 179GB Amazon...   \n",
       "2     Woody Guthrie's 1943 New Year's Resolutions ar...   \n",
       "3     'Congress Rejected 7 Times': BJP's Reminder as...   \n",
       "4                             Top 20 films of the 2010s   \n",
       "...                                                 ...   \n",
       "4948    Coronavirus, un caso anche fra i Denver Nuggets   \n",
       "4949  Coronavirus, in Europa 140mila morti in più in...   \n",
       "4950  Coronavirus in Cina: consumo di serpenti e zup...   \n",
       "4951  Iran emette mandato di arresto per Trump per l...   \n",
       "4952  Sondaggio Usa 2020: Biden in vantaggio nei 6 S...   \n",
       "\n",
       "                                                 text_2  \\\n",
       "0     DEERFIELD BEACH, FLA. (WSVN) - A no-swim advis...   \n",
       "1     The latest huge unsecured cloud storage find i...   \n",
       "2     Woody Guthrie's 1943 New Year's Resolutions ar...   \n",
       "3     Mumbai: The NCP and Shiv Sena on Thursday targ...   \n",
       "4     Jacksonville, FL - I'm not sure how we'll look...   \n",
       "...                                                 ...   \n",
       "4948  Nato ad Alatri (Fr) nel ’93 e qui diplomato al...   \n",
       "4949  Nei mesi di marzo e aprile 2020, dalla decima ...   \n",
       "4950  La diffusione del mortale Coronavirus potrebbe...   \n",
       "4951  Il procuratore di Teheran Ali Alqasi Mehr in u...   \n",
       "4952  New York, 17 giu. (askanews) - L'ex vicepresid...   \n",
       "\n",
       "                                     meta_description_2  \\\n",
       "0                                                   NaN   \n",
       "1     The latest huge unsecured cloud storage find i...   \n",
       "2     I'd seen this before, but I was reminded of it...   \n",
       "3     BJP ally and Union minister Ramdas Athawale sa...   \n",
       "4     Jacksonville, FL - I'm not sure how we'll look...   \n",
       "...                                                 ...   \n",
       "4948  Un altro caso di Coronavirus nella NBA, stavol...   \n",
       "4949  Il picco di morti aggiuntivi rispetto alla med...   \n",
       "4950  L'infezione da coronavirus potrebbe aver avuto...   \n",
       "4951  Il procuratore di Teheran, Ali Alqasi Mehr, ha...   \n",
       "4952  Per la prima volta, Trump  in ritardo in tutti...   \n",
       "\n",
       "                                        meta_keywords_2  ligne  \n",
       "0                                                  ['']      0  \n",
       "1                                                  ['']      1  \n",
       "2                                                  ['']      2  \n",
       "3     ['BJP', 'congress', 'Mamata Banerjee', 'NCP', ...      3  \n",
       "4     ['organizing', 'activism', 'socialism', \"Peopl...      4  \n",
       "...                                                 ...    ...  \n",
       "4948                                               ['']   4948  \n",
       "4949                                               ['']   4949  \n",
       "4950  ['cina', 'coronavirus', 'pipistrelli', 'serpen...   4950  \n",
       "4951  ['mondo', 'qasem soleimani', \"le tensioni tra ...   4951  \n",
       "4952                                               ['']   4952  \n",
       "\n",
       "[4953 rows x 15 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Corrections**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/stg-sdu/Notebooks/NLP/SemEval-2022/Data/'\n",
    "eval_data_location = path + \"semeval-2022_task8_eval_data_202201.csv\"\n",
    "data_eval_location = pd.read_csv(eval_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste_indexes = []\n",
    "# liste_lignes= []\n",
    "# for i in range(len(allemand)):\n",
    "#     b = False\n",
    "#     if len(allemand.title_1[i]) == 0:\n",
    "#         print('index ',i,' titre 1 : ligne ',allemand.ligne[i],data_eval_location.ia_link1[allemand.ligne[i]])\n",
    "#         b=True\n",
    "#     if len(allemand.text_1[i]) == 0:\n",
    "#         print('index ',i,' texte 1 : ligne ',allemand.ligne[i],data_eval_location.ia_link1[allemand.ligne[i]])  \n",
    "#         liste_indexes.append(i);liste_lignes.append(i)\n",
    "#         b=True\n",
    "#     if len(allemand.title_2[i]) == 0:\n",
    "#         print('index ',i,' titre 2 : ligne ',allemand.ligne[i],data_eval_location.ia_link2[allemand.ligne[i]])\n",
    "#         b=True\n",
    "#     if len(allemand.text_2[i]) == 0:\n",
    "#         print('index ',i,' texte 2 : ligne ',allemand.ligne[i],data_eval_location.ia_link2[allemand.ligne[i]])\n",
    "#         b = True\n",
    "#     if b == True:\n",
    "#         liste_indexes.append(i);liste_lignes.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programme**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remémorer numéro de ligne - compléter les Nan\n",
    "data['ligne'] = data.index\n",
    "data = data.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# séparation des datasets, le dernier étant à traduire en plus\n",
    "allemand = data.loc[data.pair_lang == 'de_de',['ligne','title_1','title_2','text_1','text_2']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de calcul du score (produit scalaire) pour résultats de classifaction\n",
    "def fonction_produit_dotcom(liste_categor, dico_scores1,dico_scores2):\n",
    "    \"\"\"\"dico scores sont les résultats obtenus pour chaque catégorie des textes 1 et 2\"\"\"\n",
    "    result = 0.0\n",
    "    for cat in liste_categor:\n",
    "        result += round(dico_scores1[cat] * dico_scores2[cat],4)\n",
    "    return result * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation des résultats du transformer type1\n",
    "def transform_text_clf1(liste_dico):\n",
    "    res = {}\n",
    "    for dic in liste_dico:\n",
    "        res[dic['label']] = dic['score']\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation des résultats du transformer type2\n",
    "def transform_text_clf2(liste_cat,liste_sc):\n",
    "    res = {}\n",
    "    for i in range(len(liste_cat)):\n",
    "        res[liste_cat[i]] = liste_sc[i]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sur sentiment3\n",
    "liste_categories = ['ENTAILMENT','NEUTRAL','CONTRADICTION']\n",
    "labels = ['politics', 'economy', 'entertainment', 'environment','sport','health']\n",
    "liste_labels = ['positive','negative','neutral']\n",
    "liste_sentiments = ['1 star','2 stars','3 stars','4 stars','5 stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions de summarization \n",
    "def summarization(texte):\n",
    "    return summarizer1(texte)[0]['summary_text'], summarizer2(texte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_classifiers = {'text_clf1': 'score_classif1','text_clf2':'score_classif2','sentiment1':'score_sentiment1',\n",
    "                    'sentiment2': 'score_sentiment2','sentiment3': 'score_sentiment3'}\n",
    "dico_categories = {'text_clf1': labels,'text_clf2':candidate_labels,'sentiment1':liste_labels,\n",
    "                    'sentiment2': liste_sentiments,'sentiment3':liste_categories}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions de classification et sentiment analysis\n",
    "def classification(texte,clf):\n",
    "    # assume nms des claasifiers et methode de transformation\n",
    "    if clf == \"text_clf1\":\n",
    "        try:\n",
    "            classes = text_clf1(texte,dico_categories['text_clf1'])\n",
    "        except:\n",
    "            return 'error'\n",
    "        else:\n",
    "            return transform_text_clf2(classes['labels'],classes['scores'])\n",
    "    elif clf == \"text_clf2\":                                 \n",
    "        try:\n",
    "            classes = text_clf2(texte,dico_categories['text_clf2'])\n",
    "        except:\n",
    "            return 'error'\n",
    "        else:\n",
    "            return transform_text_clf2(classes['labels'],classes['scores'])                          \n",
    "    elif clf == \"sentiment1\":\n",
    "        try:\n",
    "            scores = transform_text_clf1(sentiment1(texte,return_all_scores=True)[0])\n",
    "        except:\n",
    "            return 'error'\n",
    "        else:\n",
    "            return scores\n",
    "    elif clf == \"sentiment2\":\n",
    "        try:\n",
    "            scores = transform_text_clf1(sentiment2(texte,return_all_scores=True)[0])\n",
    "        except:\n",
    "            return 'error'\n",
    "        else:\n",
    "            return scores\n",
    "    elif clf == \"sentiment3\":\n",
    "        try:\n",
    "            scores = transform_text_clf1(sentiment3(texte,return_all_scores=True)[0])\n",
    "        except:\n",
    "            return 'error'\n",
    "        else:\n",
    "            return scores\n",
    "    else:\n",
    "        return 'error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraitement NLP pour PKE : suppression des mots de moins de 2 lettres non numériques\n",
    "def supp_moins_2_lettres_stopwords(phrase,stopwd):\n",
    "    temp = phrase.split(' ')\n",
    "    res = ''\n",
    "    for mot in temp:\n",
    "        if mot not in stopwd and (len(mot)>2 or (len(mot)>0 and mot[0] in ['0','1','2','3','4','5','6','7','8','9'])):\n",
    "            res += mot + ' '\n",
    "    return res[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraitement NLP pour PKE : suppression des traits d'union(regroupe)/ des apostrophes / ponctuations\n",
    "def modif(texte,stopmots):\n",
    "    # modifications simples des textes : ponctuations, petits mots, stopwords (à faire pour entités et pke textes)\n",
    "    texte=re.sub('\\'',' ',texte)   # suppression apostrophe\n",
    "    texte=re.sub('-','',texte)    # suppression trait union\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation)) # suppression de toutes les ponctuations\n",
    "    texte=regex.sub(' ',texte)\n",
    "    texte = supp_moins_2_lettres_stopwords(texte,stopmots)\n",
    "    return texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout des synonymes (existants en orthographe) à la suite de l'analyse pke\n",
    "def ajout_synonymes(mot, correct_ortho = False):\n",
    "    # on ajoute les 10 premiers synonymes existants, on vérifie orthographe (optionnel)\n",
    "    syns = model_gensim.most_similar(mot,topn = 20)\n",
    "    if correct_ortho == True:\n",
    "        res = []\n",
    "        for m in syns:\n",
    "            if d.check(m[0]):   #  il y a le mot et son pourcentage d'importance\n",
    "                res.append(m)\n",
    "        syns = res\n",
    "    return syns[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choix des paramètres de la méthode : A revoir ?\n",
    "methode1 = {\"NOUN\", \"PROPN\", \"ADJ\",\"VERB\"}\n",
    "methode2 = {\"NOUN\", \"PROPN\", \"ADJ\"}\n",
    "nb_mots = {'meth1': 30, 'meth2':50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PKE : Analyse des termes principaux dans les textes et titres \n",
    "# Problème \n",
    "def transformation_pke_results(res1,res2, correct_ortho = False):\n",
    "    \"\"\"\n",
    "    Transformation des resultats de PKE : Pb bigramme peuvent ne pas être ds les 2 textes mais 1 mot seulement\n",
    "    liste de clés et dictionnaires de valeurs, bigrammes jouera ainsi de maniere coefficientée \n",
    "    Exemple : fuite eau:0.05 --> 3 mots au final : fuite, eau, fuite eau : 0.05\n",
    "    De plus on ajoute les synonymes issus de gensim en les coefficiant et vérifiant que cela \"\"\"\n",
    "    \n",
    "    liste1 = []; liste2 = [] ; dico1 = {}; dico2 = {}\n",
    "    for elt in res1:\n",
    "        liste1.append(elt[0])\n",
    "        dico1[elt[0]] = round(elt[1],3)\n",
    "        if ' ' in elt[0]:    # bigramme dans ce cas, ajout des 2 mots\n",
    "            liste = elt[0].split(' ')\n",
    "            for mot in liste:\n",
    "                liste1.append(mot)\n",
    "                dico1[mot] = round(elt[1],3)\n",
    "                try:\n",
    "                    synonyms = ajout_synonymes(mot,correct_ortho = correct_ortho)\n",
    "                except:\n",
    "                    pass\n",
    "                else:\n",
    "                    for syn in synonyms:\n",
    "                        liste1.append(syn[0])   # Ajout du mot \n",
    "                        dico1[syn[0]] = round(elt[1] * syn[1], 3)  # poids considéré\n",
    "                    \n",
    "    for elt in res2:\n",
    "        liste2.append(elt[0])\n",
    "        dico2[elt[0]] = round(elt[1],3)\n",
    "        if ' ' in elt[0]:\n",
    "            liste = elt[0].split(' ')\n",
    "            for mot in liste:\n",
    "                liste2.append(mot)\n",
    "                dico2[mot] = round(elt[1],3)\n",
    "                try:\n",
    "                    synonyms = ajout_synonymes(mot,correct_ortho = correct_ortho)\n",
    "                except:\n",
    "                    pass\n",
    "                else:\n",
    "                    for syn in synonyms:\n",
    "                        liste2.append(syn[0])   # Ajout du mot \n",
    "                        dico2[syn[0]] = round(elt[1] * syn[1], 3)  # poids considéré\n",
    "    \n",
    "    # similarites entre les 2 listes issus de pke avec poids\n",
    "    sim = 0\n",
    "    for elt in liste1:\n",
    "        if elt in liste2:\n",
    "            sim += (dico1[elt] + dico2[elt])/2\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entites_communes(nlp,text1,text2):\n",
    "    \"\"\"\"\n",
    "    Cette première fonction ne regarde que les entités communes : personnes, dates, groupe, localisations\n",
    "    Elle sera appliquée aux textes et aux titres et cumulé : si cumul en titre et texte : compte double !\"\"\"\n",
    "    \n",
    "    doc1 = nlp(text1)\n",
    "    doc2 = nlp(text2)\n",
    "    nb_commun_ent = 0; liste_commun_ent = []\n",
    "    nb_commun_geo = 0; liste_commun_geo = []\n",
    "    nb_commun_dat = 0; liste_commun_dat = []\n",
    "    \n",
    "    if len(doc1.ents)>0 and len(doc2.ents)>0:\n",
    "        liste1 = []; dico1 = {}\n",
    "        for elt in doc1.ents:\n",
    "            if elt.label_ in ['PERSON','PER'] and ' ' in elt.text:\n",
    "                mots = elt.text.split(' ')\n",
    "                for mot in mots:\n",
    "                    if mot not in liste1:\n",
    "                        liste1.append(mot)\n",
    "                        dico1[mot] = elt.label_\n",
    "            elif elt.label_ in ['LOC','ORG','GPE','DATE','TIME']:\n",
    "                if elt.text not in liste1:\n",
    "                    liste1.append(elt.text)\n",
    "                    dico1[elt.text] = elt.label_\n",
    "        liste2 = []\n",
    "        for elt in doc2.ents:\n",
    "            if elt.label_ in ['PERSON','PER'] and ' ' in elt.text:\n",
    "                mots = elt.text.split(' ')\n",
    "                for mot in mots:\n",
    "                    if mot not in liste2:\n",
    "                        liste2.append(mot)\n",
    "            elif elt.label_ in ['LOC','ORG','GPE','DATE','TIME']:\n",
    "                if elt.text not in liste2:\n",
    "                    liste2.append(elt.text)\n",
    "        \n",
    "        # points communs des listes        \n",
    "        for elt in liste1:\n",
    "            if elt in liste2:\n",
    "                if dico1[elt] == 'LOC':\n",
    "                    nb_commun_geo += 1\n",
    "                    liste_commun_geo.append(elt)\n",
    "                elif dico1[elt] in ['DATE','TIME']:\n",
    "                    nb_commun_dat += 1\n",
    "                    liste_commun_dat.append(elt)\n",
    "                else:\n",
    "                    nb_commun_ent += 1\n",
    "                    liste_commun_ent.append(elt)\n",
    "                    \n",
    "    return nb_commun_ent, liste_commun_ent,nb_commun_geo, liste_commun_geo,nb_commun_dat, liste_commun_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Creation_features_comparaison(df,langue, test_position = [methode1,methode2]):\n",
    "    \"\"\"Création des notes pour classification ensuite\"\"\"\n",
    "    \n",
    "    resultats = pd.DataFrame(columns = ['summary1_text1','summary2_text1','summary1_text2','summary2_text2',\n",
    "            'nb_entites_idem','nb_lieux_idem', 'nb_dates_idem','entites_idem','lieux_idem','dates_idem',\n",
    "            'score_similarite_titres','score_similarite_resume1','score_similarite_resume2','score_classif1','score_classif2',\n",
    "            'score_sentiment1','score_sentiment2','score_sentiment3','meth1_similarites','meth2_similarites'])\n",
    "    \n",
    "    # initialisation de la langue stanza\n",
    "    stanza.download(langue)\n",
    "    nlp_stanza = spacy_stanza.load_pipeline(langue)\n",
    "    stopmts = stopwds_lg[langue]\n",
    "    if langue in dico_spacy.keys():\n",
    "        nlp_spacy = dico_spacy[langue]\n",
    "    else:\n",
    "        nlp_spacy = None\n",
    "    print(nlp_spacy != None)   \n",
    "    for i in tqdm(range(len(df))):\n",
    "        dico_res = {}\n",
    "        \n",
    "        # Summary et comparatifs \n",
    "        dico_res['summary1_text1'],dico_res['summary2_text1'] = summarization(df.text_1[i])\n",
    "        dico_res['summary1_text2'],dico_res['summary2_text2'] = summarization(df.text_2[i])\n",
    "        dico_res['score_similarite_titres'] = score_similarite([df.title_1[i]],[df.title_2[i]])\n",
    "        dico_res['score_similarite_resume1'] = score_similarite([dico_res['summary1_text1']],[dico_res['summary1_text2']])\n",
    "        dico_res['score_similarite_resume2'] = score_similarite([dico_res['summary2_text1']],[dico_res['summary2_text2']])\n",
    "        \n",
    "        # analyse de textes classification et de sentiments\n",
    "        texte1 = df.title_1[i] + ' ' + df.text_1[i]\n",
    "        texte2 = df.title_2[i] + ' ' + df.text_2[i]\n",
    "        if len(texte1)>0 and len(texte2)>0:\n",
    "            for classifier in dico_classifiers.keys():\n",
    "                scores1 = classification(texte1,classifier)\n",
    "                scores2 = classification(texte2,classifier)\n",
    "                if scores1 != 'error' and scores2 != 'error':\n",
    "                    dico_res[dico_classifiers[classifier]] = fonction_produit_dotcom(dico_categories[classifier], scores1,scores2)\n",
    "                else:\n",
    "                    scores1 = classification(df.title_1[i],classifier)\n",
    "                    scores2 = classification(df.title_2[i],classifier)\n",
    "                    if scores1 != 'error' and scores2 != 'error':\n",
    "                        dico_res[dico_classifiers[classifier]] = fonction_produit_dotcom(dico_categories[classifier], scores1,scores2)\n",
    "                    else:\n",
    "                        dico_res[dico_classifiers[classifier]] = None\n",
    "                \n",
    "        # pré traitement des textes pour PKE\n",
    "        texte1 = modif(texte1, stopmts)\n",
    "        texte2 = modif(texte2, stopmts)\n",
    "        \n",
    "        # ENTITES COMMUNES : on tient compte des bigrammes Noms qui posent erreurs ex: Joe Biden et Biden \n",
    "        # Ici, on considère mieux le CUMUl titres et Textes avec une pondération double pour le titre \n",
    "        # Il faut aussi enlever les petits mots donc pré-traitement en texte\n",
    "        \n",
    "        nb_ent1,list_ent1,nb_geo1,list_geo1,nb_dat1,list_dat1 = entites_communes(nlp_stanza,df.title_1[i],df.title_2[i])\n",
    "        nb_ent2,list_ent2,nb_geo2,list_geo2,nb_dat2,list_dat2 = entites_communes(nlp_stanza,df.text_1[i],df.text_2[i])\n",
    "        if nlp_spacy != None:\n",
    "            nb_ent3,list_ent3,nb_geo3,list_geo3,nb_dat3,list_dat3 = entites_communes(nlp_spacy,df.title_1[i],df.title_2[i])\n",
    "            nb_ent4,list_ent4,nb_geo4,list_geo4,nb_dat4,list_dat4 = entites_communes(nlp_spacy,df.text_1[i],df.text_2[i])\n",
    "        else:\n",
    "            nb_ent3,list_ent3,nb_geo3,list_geo3,nb_dat3,list_dat3 = (0,[],0,[],0,[])\n",
    "            nb_ent4,list_ent4,nb_geo4,list_geo4,nb_dat4,list_dat4 = (0,[],0,[],0,[])\n",
    "        dico_res['nb_entites_idem'] = nb_ent1 * 2 + nb_ent2 + nb_ent3 * 2 + nb_ent4\n",
    "        dico_res['nb_lieux_idem'] = nb_geo1  * 2 + nb_geo2 + nb_geo3  * 2 + nb_geo4\n",
    "        dico_res['nb_dates_idem'] = nb_dat1 * 2 + nb_dat2 + nb_dat3 * 2 + nb_dat4\n",
    "        # fusion des listes en supprimant les doublons\n",
    "        dico_res['entites_idem'] = list(set(list_ent1+list_ent2+ list_ent3+list_ent4))\n",
    "        dico_res['lieux_idem'] = list(set(list_geo1+list_geo2+list_geo3+list_geo4))\n",
    "        dico_res['dates_idem'] = list(set(list_dat1+list_dat2+list_dat3+list_dat4))\n",
    "        \n",
    "        for j,meth in enumerate(test_position):\n",
    "            nom ='meth'+str(j+1)\n",
    "            nb_mots_meth = nb_mots[nom]\n",
    "            if len(texte1)>0 and len(texte2)>0:\n",
    "                extractor = pke.unsupervised.TopicRank()\n",
    "                extractor.load_document(input=texte1,language=langue,normalization=\"stemming\")\n",
    "                extractor.candidate_selection(pos=meth)\n",
    "                extractor.candidate_weighting()\n",
    "                keyphrases3 = extractor.get_n_best(n=nb_mots_meth)\n",
    "                extractor = pke.unsupervised.TopicRank()\n",
    "                extractor.load_document(input=texte2,language=langue,normalization=\"stemming\")\n",
    "                extractor.candidate_selection(pos=meth)\n",
    "                extractor.candidate_weighting()\n",
    "                keyphrases4 = extractor.get_n_best(n=nb_mots_meth)\n",
    "                dico_res[nom+'_similarites'] = round(100*transformation_pke_results(keyphrases3,keyphrases4),1)\n",
    "            else:\n",
    "                dico_res[nom+'_similarites'] = 'Error'\n",
    "\n",
    "        resultats.loc[len(resultats)] = dico_res\n",
    "        \n",
    "    newdf = pd.concat([df,resultats],axis=1)\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c2221aca264576b7478635872aab1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-14 23:14:22 INFO: Downloading default packages for language: de (German)...\n",
      "2022-01-14 23:14:24 INFO: File exists: C:\\Users\\stg-sdu\\stanza_resources\\de\\default.zip.\n",
      "2022-01-14 23:14:28 INFO: Finished downloading models and saved to C:\\Users\\stg-sdu\\stanza_resources.\n",
      "2022-01-14 23:14:28 INFO: Loading these models for language: de (German):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "| sentiment | sb10k   |\n",
      "| ner       | conll03 |\n",
      "=======================\n",
      "\n",
      "2022-01-14 23:14:28 INFO: Use device: cpu\n",
      "2022-01-14 23:14:28 INFO: Loading: tokenize\n",
      "2022-01-14 23:14:28 INFO: Loading: mwt\n",
      "2022-01-14 23:14:28 INFO: Loading: pos\n",
      "2022-01-14 23:14:29 INFO: Loading: lemma\n",
      "2022-01-14 23:14:29 INFO: Loading: depparse\n",
      "2022-01-14 23:14:29 INFO: Loading: sentiment\n",
      "2022-01-14 23:14:29 INFO: Loading: ner\n",
      "2022-01-14 23:14:30 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0d2e5662f94e39acf5f2bc2f186a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "#similarites = Creation_features_comparaison(allemand[:300].reset_index(drop=True),'de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarites.to_csv('eval_de_notes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarites = Creation_features_comparaison(allemand[300:].reset_index(drop=True),'de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precedent = pd.read_csv('eval_de_notes.csv',index_col=0)\n",
    "# similarites2 = pd.concat([precedent,similarites], axis=0)\n",
    "# similarites2 = similarites2.reset_index(drop=True)\n",
    "# similarites2.to_csv('eval_de_notes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compl_allemand = pd.read_csv('eval_compl_allemand.csv')\n",
    "len(compl_allemand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9141572d08ed435bb62c096ef5e2ef20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 17:52:16 INFO: Downloading default packages for language: de (German)...\n",
      "2022-01-17 17:52:18 INFO: File exists: C:\\Users\\stg-sdu\\stanza_resources\\de\\default.zip.\n",
      "2022-01-17 17:52:23 INFO: Finished downloading models and saved to C:\\Users\\stg-sdu\\stanza_resources.\n",
      "2022-01-17 17:52:23 INFO: Loading these models for language: de (German):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "| sentiment | sb10k   |\n",
      "| ner       | conll03 |\n",
      "=======================\n",
      "\n",
      "2022-01-17 17:52:23 INFO: Use device: cpu\n",
      "2022-01-17 17:52:23 INFO: Loading: tokenize\n",
      "2022-01-17 17:52:23 INFO: Loading: mwt\n",
      "2022-01-17 17:52:23 INFO: Loading: pos\n",
      "2022-01-17 17:52:23 INFO: Loading: lemma\n",
      "2022-01-17 17:52:24 INFO: Loading: depparse\n",
      "2022-01-17 17:52:24 INFO: Loading: sentiment\n",
      "2022-01-17 17:52:25 INFO: Loading: ner\n",
      "2022-01-17 17:52:26 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a016d716a8af41f58a24cb6351fed9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Your max_length is set to 20, but you input_length is only 14. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 20, but you input_length is only 19. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 20, but you input_length is only 9. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    }
   ],
   "source": [
    "similarites = Creation_features_comparaison(compl_allemand.reset_index(drop=True),'de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarites.to_csv('eval_compl_de_notes.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
